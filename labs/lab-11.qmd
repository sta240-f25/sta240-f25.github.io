---
title: "Lab 11"
format: html
filters:
  - shinylive
---

Let's say you work at an insurance company. Your boss asks you "how many insurance claims do we typically receive in a month?" So you pull up some data. Here's a preview of what it might look like:

::: {.center-table}
| Month       | Year | Number of claims |
|----------------|------------|------------|
| ...           | ...        | ... |
| March   | 2005        | 80 |
| April   | 2005        | 61 |
| May   | 2005        | 98 |
| June   | 2005        | 73 |
| ...           | ...        | ... |
:::

To answer your boss's question, you could take the numbers and just compute the average. That's probably a pretty good guess. But imagine your boss follows up with questions like "how certain are we of that guess?" or "how reliable is that estimate?" Now we're talking about statistics.

As soon as we start worrying about uncertainty quantification, we enter the subtle realm of modeling assumptions and interpretive philosophies. It's not like probability theory where we were just proving theorems and doing calculations. Now, the theory of statistics is in some ways just another *branch* of probability theory, and so in truth were are still proving theorems and doing calculations. But we wring our hands about the interpretation of the results in ways that we didn't necessarily do before.

Anyway, I'm rambling. Data don't speak for themselves, and so we require an interpretive lens to make sense of them. The strongest form of this is a concrete statistical model, so let's pose one. The number of insurance claims in a month is going to be a nonnegative integer, and so we could choose for our model any parametric family of probability distributions supported on $\mathbb{N}$. Let's go with the simplest one:

$$
X_1,\,X_2,\,...,\,X_n\overset{\text{iid}}{\sim}\text{Poisson}(\theta).
$$

That's our model for the claims data. The rate parameter $\theta>0$ is unknown, and we want to use the data to learn what it is. Note that this model may or may not be a good choice, and entire books have been written on model building and model selection and model diagnostics and model criticism. If you keep studying statistics, you'll learn about all that, but this class is about one thing and one thing only: can you do the math or not? So we will just take the model as given and use it as an opportunity to test our technical skills.

::: callout-note
## Write-up instructions

You don't have to type up all the work. The final answer is fine. Frankly, I don't really care what you turn in. This is just extra practice with stuff you are guaranteed to see on the final.
:::

# Task 0

Compute the likelihood function and the log-likelihood function for the Poisson distribution. You will need both in what follows.

# Task 1

Compute the maximum likelihood estimator for $\theta>0$ in the Poisson distribution.

# Task 2

Compute the mean, bias, variance, and MSE of the estimator and comment on its statistical properties.

# Task 3

The rate parameter $\theta>0$ is a positive real number, so if we want to perform a Bayesian analysis, we need a continuous probability distribution on positive reals. The gamma family is a convenient choice for this, so consider the Bayesian model:

$$
\begin{aligned}
\theta&\sim\text{Gamma}(a_0,\,b_0)
\\
X_1,\,X_2,\,...,\,X_n\mid\theta&\overset{\text{iid}}{\sim}\text{Poisson}(\theta).
\end{aligned}
$$

Derive the posterior distribution.


# Task 4

Based on your answer from the previous task, show that the posterior mean is a convex combination of the prior mean and the maximum likelihood estimator that you derived earlier.

# Task 5

Ignoring that we ever did a Bayesian analysis, treat the posterior mean formula from the previous task as a new *classical* estimator for $\theta$. It's just a function of the data at the end of the day. 

What are the mean, bias, and variance of the posterior mean? How do these compare to the pure MLE? Are there circumstances under which this new estimator would be preferred to the pure MLE?


# Play around!

Here's a cute lil' app you can play with to get a sense of how these statistical procedures behave. Start by keeping all the default setting the same but increasing the sample size. What happens to the posterior distribution as you accumulate more and more data?


```{shinylive-r}
#| standalone: true
#| viewerHeight: 700
library(shiny)

# true value
# prior hyperparameter
# sample size
# show the data
# show the summary statistics

ui <- fluidPage(
  titlePanel("Inference for the Poisson distribution"),
  
  sidebarLayout(
    sidebarPanel(
      sliderInput("lambda", "True value of θ:",
                   min = 0.1, max = 10, value = 1, step = 0.1),
      sliderInput("a", "Prior shape (a₀):",
                  min = 0.1, max = 100, value = 70, step = 0.1),
      sliderInput("b", "Prior rate (b₀):",
                  min = 0.1, max = 100, value = 10, step = 0.1),
      sliderInput("n", "Sample size (n):",
                  min = 1, max = 300, value = 1, step = 1),
      hr(),
      verbatimTextOutput("moments")
    ),
    
    mainPanel(
      plotOutput("betaPlot", height = "600px")
    )
  )
)

server <- function(input, output, session) {
  
  output$moments <- renderText({
    set.seed(1234)
  
    n <- input$n
    lambda <- input$lambda
    a <- input$a
    b  <- input$b
    X <- rpois(n, lambda)
    
    xbar <- mean(X)
    a_new <- a + sum(X)
    b_new <- b + n
    paste0("Prior mean: ", round(a / b, 3),
           "\nMLE: ", round(xbar, 3),
           "\nPosterior mean: ", round(a_new / b_new, 3))
  })
  
  output$betaPlot <- renderPlot({
    set.seed(1234)
  
    n <- input$n
    lambda <- input$lambda
    a <- input$a
    b  <- input$b
    X <- rpois(n, lambda)
    
    xbar <- mean(X)
    a_new <- a + sum(X)
    b_new <- b + n
    
    curve(dgamma(x, shape = a, rate = b), from = 0, to = 10, n = 1000, col = "blue",
          ylim = c(0, 5), lwd = 2, xlab = "θ",
          ylab = "density")
    curve(dgamma(x, shape = a_new, rate = b_new), from = 0, to = 10, n = 1000, add = TRUE,
          col = "purple", lwd = 2)
    abline(v = lambda, lwd = 2, col = "red")
      legend("topright", c("prior", "posterior", "true value"), 
         lty = 1, lwd = 2,
         col = c("blue", "purple", "red"), bty = "n")
  })
}

shinyApp(ui, server)

```






