[
  {
    "objectID": "distributions/discrete-binomial.html",
    "href": "distributions/discrete-binomial.html",
    "title": "Binomial distribution",
    "section": "",
    "text": "The binomial distribution is our model for the total number of successes in \\(n\\) independent binary trials, each with identical probability of success \\(p\\in[0,\\,1]\\).",
    "crumbs": [
      "Distribution Families",
      "Binomial"
    ]
  },
  {
    "objectID": "distributions/discrete-binomial.html#basic-properties",
    "href": "distributions/discrete-binomial.html#basic-properties",
    "title": "Binomial distribution",
    "section": "Basic properties",
    "text": "Basic properties\n\n\nNotation\n\\(X\\sim\\text{Binom}(n,\\,p)\\)\n\n\nRange\n\\(\\{0,\\,1,\\,2,\\,...,\\,n-1,\\,n\\}\\)\n\n\nParameter space\n\\(p\\in[0,\\,1]\\)\n\n\nPMF\n\\(P(X=k)=\\binom{n}{k}p^k(1-p)^{n-k}\\)\n\n\nExpectation\n\\(np\\)\n\n\nVariance\n\\(np(1-p)\\)",
    "crumbs": [
      "Distribution Families",
      "Binomial"
    ]
  },
  {
    "objectID": "distributions/discrete-binomial.html#r-commands",
    "href": "distributions/discrete-binomial.html#r-commands",
    "title": "Binomial distribution",
    "section": "\nR commands",
    "text": "R commands\nHere is the documentation for the suite of commands that let you work with the binomial distribution in R:\n\ndbinom(x, size, prob) # PMF: P(X = x)\npbinom(q, size, prob) # CDF: P(X &lt;= q)\nqbinom(p, size, prob) # quantile function (inverse CDF)\nrbinom(n, size, prob) # random numbers",
    "crumbs": [
      "Distribution Families",
      "Binomial"
    ]
  },
  {
    "objectID": "distributions/discrete-binomial.html#play-around",
    "href": "distributions/discrete-binomial.html#play-around",
    "title": "Binomial distribution",
    "section": "Play around!",
    "text": "Play around!\nNotice that, for all values of \\(p\\in(0,\\,1)\\), the shape of the PMF becomes more like the bell curve as \\(n\\) grows:\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 500\n\nlibrary(shiny)\n\ndiscrete_pmf &lt;- function(x, p, xlim = c(min(x) - 1, max(x) + 1), label = \"\", add_mean = FALSE){\n  plot(x, p,\n       pch = 19,\n       cex = 1,\n       xlab = \"\",\n       ylab = \"\",\n       main = \"\",\n       ylim = c(0, 1.05 * max(p)),\n       yaxs = \"i\",\n       yaxt = \"n\",\n       xlim = xlim,\n       xaxt = \"n\",\n       bty = \"n\"\n  )\n  segments(x,\n           rep(0, length(x) + 1),\n           x1 = x,\n           y1 = p,\n           lwd = 3\n  )\n  axis(1, at = floor(xlim[1]):ceiling(xlim[2]), cex.axis = 1)\n  axis(2, at = seq(0, 1, length.out = 11), las = 1, cex.axis = 1.5)\n  legend(\"topright\", label, bty = \"n\", cex = 3)\n  if(add_mean == TRUE){\n    mtext(\"E(X)\", side = 1, at = sum(x * p), col = \"red\", line = 2)\n  }\n}\n\ndiscrete_cdf &lt;- function(x, p, xlim = c(min(x) - 1, max(x) + 1), label = \"\"){\n  closeddot = cumsum(p)\n  opencircle = c(0, closeddot[1:length(x)-1])\n  plot(x, closeddot, pch = 19, cex = 1,\n       ylim = c(0, 1),\n       ylab = \"\", main = \"\", xlab = \"\",\n       yaxt = \"n\",\n       xlim = xlim,\n       xaxt = \"n\",\n       #yaxs = \"i\", \n       bty = \"n\")\n  points(x, opencircle, cex = 1)\n  segments(c(xlim[1], x), c(0, closeddot), c(x, xlim[2]), c(0, closeddot), lwd = 1)\n  axis(1, at = floor(xlim[1]):ceiling(xlim[2]), cex.axis = 1)\n  axis(2, at = seq(0, 1, length.out = 11), las = 1, cex.axis = 1.5)\n  legend(\"bottomright\", label, bty = \"n\", cex = 3)\n}\n\n# Define UI for application that draws a histogram\nui &lt;- fluidPage(\n  \n  # Application title\n  titlePanel(\"Binomial distribution CDF and PMF\"),\n  \n  # Sidebar with a slider input for number of bins \n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"p\",\n                  \"Probability of success (p):\",\n                  min = 0,\n                  max = 1,\n                  value = 0.5,\n                  step = 0.01),\n      sliderInput(\"n\",\n                  \"Number of trials (n):\",\n                  min = 1,\n                  max = 100,\n                  value = 5,\n                  step = 1)\n    ),\n    \n    # Show a plot of the generated distribution\n    mainPanel(\n      plotOutput(\"distPlot\")\n    )\n  )\n)\n\n# Define server logic required to draw a histogram\nserver &lt;- function(input, output) {\n  \n  output$distPlot &lt;- renderPlot({\n    \n    p &lt;- input$p\n    n &lt;- input$n\n\n    par(mfrow = c(2, 1), mar = c(4, 4, 2, 2))\n    \n    discrete_cdf(0:n, dbinom(0:n, n, p))\n    discrete_pmf(0:n, dbinom(0:n, n, p), add_mean = TRUE)\n    \n  })\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)",
    "crumbs": [
      "Distribution Families",
      "Binomial"
    ]
  },
  {
    "objectID": "distributions/discrete-geometric.html",
    "href": "distributions/discrete-geometric.html",
    "title": "Geometric distribution",
    "section": "",
    "text": "The geometric distribution is based on a (possibly infinite) sequence of independent and identically distributed binary trials, each with probability of success \\(p\\). There are two versions:",
    "crumbs": [
      "Distribution Families",
      "Geometric"
    ]
  },
  {
    "objectID": "distributions/discrete-geometric.html#basic-properties",
    "href": "distributions/discrete-geometric.html#basic-properties",
    "title": "Geometric distribution",
    "section": "Basic properties",
    "text": "Basic properties\nFor the first version of the distribution that we saw in lecture, the properties are:\n\n\nNotation\n\\(X\\sim\\text{Geom}(p)\\)\n\n\nRange\n\\(\\{1,\\,2,\\,3,\\,4,\\,...\\}\\)\n\n\nParameter space\n\\(p\\in[0,\\,1]\\)\n\n\nPMF\n\\(P(X=k)=(1-p)^{k-1}p\\)\n\n\nExpectation\n\\(1/p\\)\n\n\nVariance\n\\(\\frac{1-p}{p^2}\\)",
    "crumbs": [
      "Distribution Families",
      "Geometric"
    ]
  },
  {
    "objectID": "distributions/discrete-geometric.html#r-commands",
    "href": "distributions/discrete-geometric.html#r-commands",
    "title": "Geometric distribution",
    "section": "\nR commands",
    "text": "R commands\nFor the second version of the distribution, R provides:\n\ndgeom(x, prob) # PMF: P(X = x)\npgeom(q, prob) # CDF: P(X &lt;= q)\nqgeom(p, prob) # quantile function (inverse CDF)\nrgeom(n, prob) # random numbers",
    "crumbs": [
      "Distribution Families",
      "Geometric"
    ]
  },
  {
    "objectID": "distributions/discrete-geometric.html#play-around",
    "href": "distributions/discrete-geometric.html#play-around",
    "title": "Geometric distribution",
    "section": "Play around!",
    "text": "Play around!\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 500\n\nlibrary(shiny)\n\ndiscrete_pmf &lt;- function(x, p, xlim = c(min(x) - 1, max(x) + 1), label = \"\", add_mean = FALSE){\n  plot(x, p,\n       pch = 19,\n       cex = 0.5,\n       xlab = \"\",\n       ylab = \"\",\n       main = \"\",\n       ylim = c(0, 0.4),\n       yaxs = \"i\",\n       yaxt = \"n\",\n       xlim = xlim,\n       xaxt = \"n\",\n       bty = \"n\"\n  )\n  segments(x,\n           rep(0, length(x) + 1),\n           x1 = x,\n           y1 = p,\n           lwd = 3\n  )\n  axis(1, at = floor(xlim[1]):ceiling(xlim[2]), cex.axis = 1)\n  axis(2, at = seq(0, 1, length.out = 11), las = 1, cex.axis = 1.5)\n  legend(\"topright\", label, bty = \"n\", cex = 3)\n  if(add_mean == TRUE){\n    mtext(\"E(X)\", side = 1, at = sum(x * p), col = \"red\", line = 2)\n  }\n}\n\ndiscrete_cdf &lt;- function(x, p, xlim = c(min(x) - 1, max(x) + 1), label = \"\"){\n  closeddot = cumsum(p)\n  opencircle = c(0, closeddot[1:length(x)-1])\n  plot(x, closeddot, pch = 19, cex = 0.5,\n       ylim = c(0, 1),\n       ylab = \"\", main = \"\", xlab = \"\",\n       yaxt = \"n\",\n       xlim = xlim,\n       xaxt = \"n\",\n       #yaxs = \"i\", \n       bty = \"n\")\n  points(x, opencircle, cex = 0.5)\n  segments(c(xlim[1], x), c(0, closeddot), c(x, xlim[2]), c(0, closeddot), lwd = 1)\n  axis(1, at = floor(xlim[1]):ceiling(xlim[2]), cex.axis = 1)\n  axis(2, at = seq(0, 1, length.out = 11), las = 1, cex.axis = 1.5)\n  legend(\"bottomright\", label, bty = \"n\", cex = 3)\n}\n\n# Define UI for application that draws a histogram\nui &lt;- fluidPage(\n  \n  # Application title\n  titlePanel(\"Geometric distribution CDF and PMF\"),\n  \n  # Sidebar with a slider input for number of bins \n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"p\",\n                  \"Probability of success (p):\",\n                  min = 0,\n                  max = 1,\n                  value = 0.5,\n                  step = 0.01)\n    ),\n    \n    # Show a plot of the generated distribution\n    mainPanel(\n      plotOutput(\"distPlot\")\n    )\n  )\n)\n\n# Define server logic required to draw a histogram\nserver &lt;- function(input, output) {\n  \n  output$distPlot &lt;- renderPlot({\n    \n    p &lt;- input$p\n    n &lt;- 30\n\n    par(mfrow = c(2, 1), mar = c(4, 4, 2, 2))\n    \n    discrete_cdf(1:n, dgeom(0:(n-1), p))\n    plot(1:n, dgeom(0:(n-1), p), type = \"h\",\n         #ylim = c(0, 1),\n        yaxs = \"i\",\n       yaxt = \"n\",\n       ylab = \"\",\n       xlim = c(0, n + 1),\n       xlab = \"\",\n       xaxt = \"n\",\n       bty = \"n\")\n    axis(1, at = 0:(n+1), cex.axis = 1)\n    axis(2, at = seq(0, 1, length.out = 11), las = 1, cex.axis = 1.5)\n    mtext(\"E(X)\", side = 1, at = 1 / p, col = \"red\", line = 2)\n  })\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)",
    "crumbs": [
      "Distribution Families",
      "Geometric"
    ]
  },
  {
    "objectID": "distributions/discrete-poisson.html",
    "href": "distributions/discrete-poisson.html",
    "title": "Poisson distribution",
    "section": "",
    "text": "The Poisson distribution can be used to model the number of random arrivals in a given window of time:\nThe distribution is governed by one parameter \\(\\lambda&gt;0\\) which is called the rate.",
    "crumbs": [
      "Distribution Families",
      "Poisson"
    ]
  },
  {
    "objectID": "distributions/discrete-poisson.html#basic-properties",
    "href": "distributions/discrete-poisson.html#basic-properties",
    "title": "Poisson distribution",
    "section": "Basic properties",
    "text": "Basic properties\n\n\nNotation\n\\(X\\sim\\text{Poisson}(\\lambda)\\)\n\n\nRange\n\\(\\mathbb{N}=\\{0,\\,1,\\,2,\\,3,\\,4,\\,...\\}\\)\n\n\nParameter space\n\\(\\lambda&gt;0\\)\n\n\nPMF\n\\(P(X=k)=e^{-\\lambda}\\frac{\\lambda^k}{k!}\\)\n\n\nMGF\n\n\\(M(t)=\\exp(\\lambda(e^t-1))\\), \\(t\\in\\mathbb{R}\\)\n\n\n\nExpectation\n\\(\\lambda\\)\n\n\nVariance\n\\(\\lambda\\)",
    "crumbs": [
      "Distribution Families",
      "Poisson"
    ]
  },
  {
    "objectID": "distributions/discrete-poisson.html#r-commands",
    "href": "distributions/discrete-poisson.html#r-commands",
    "title": "Poisson distribution",
    "section": "\nR commands",
    "text": "R commands\nHere is the documentation for the suite of commands that let you work with the Poisson distribution in R:\n\ndpois(x, lambda) # PMF: P(X = x)\nppois(q, lambda) # CDF: F(q) = P(X &lt;= q)\nqpois(p, lambda) # quantile function (inverse CDF)\nrpois(n, lambda) # random numbers",
    "crumbs": [
      "Distribution Families",
      "Poisson"
    ]
  },
  {
    "objectID": "distributions/discrete-poisson.html#play-around",
    "href": "distributions/discrete-poisson.html#play-around",
    "title": "Poisson distribution",
    "section": "Play around!",
    "text": "Play around!\nAs \\(\\lambda\\) grows, the distribution shifts to the right and gets wider, which makes sense since \\(\\lambda\\) acts as both the mean and variance. Furthermore, the shape of the PMF becomes more bell-like as \\(\\lambda\\) gets big. This is not an accident.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 500\n\nlibrary(shiny)\n\ndiscrete_pmf &lt;- function(x, p, xlim = c(min(x) - 1, max(x) + 1), label = \"\", add_mean = FALSE){\n  plot(x, p,\n       pch = 19,\n       cex = 0.5,\n       xlab = \"\",\n       ylab = \"\",\n       main = \"\",\n       ylim = c(0, 0.4),\n       yaxs = \"i\",\n       yaxt = \"n\",\n       xlim = xlim,\n       xaxt = \"n\",\n       bty = \"n\"\n  )\n  segments(x,\n           rep(0, length(x) + 1),\n           x1 = x,\n           y1 = p,\n           lwd = 3\n  )\n  axis(1, at = floor(xlim[1]):ceiling(xlim[2]), cex.axis = 1)\n  axis(2, at = seq(0, 1, length.out = 11), las = 1, cex.axis = 1.5)\n  legend(\"topright\", label, bty = \"n\", cex = 3)\n  if(add_mean == TRUE){\n    mtext(\"E(X)\", side = 1, at = sum(x * p), col = \"red\", line = 2)\n  }\n}\n\ndiscrete_cdf &lt;- function(x, p, xlim = c(min(x) - 1, max(x) + 1), label = \"\"){\n  closeddot = cumsum(p)\n  opencircle = c(0, closeddot[1:length(x)-1])\n  plot(x, closeddot, pch = 19, cex = 0.5,\n       ylim = c(0, 1),\n       ylab = \"\", main = \"\", xlab = \"\",\n       yaxt = \"n\",\n       xlim = xlim,\n       xaxt = \"n\",\n       #yaxs = \"i\", \n       bty = \"n\")\n  points(x, opencircle, cex = 0.5)\n  segments(c(xlim[1], x), c(0, closeddot), c(x, xlim[2]), c(0, closeddot), lwd = 1)\n  axis(1, at = floor(xlim[1]):ceiling(xlim[2]), cex.axis = 1)\n  axis(2, at = seq(0, 1, length.out = 11), las = 1, cex.axis = 1.5)\n  legend(\"bottomright\", label, bty = \"n\", cex = 3)\n}\n\n# Define UI for application that draws a histogram\nui &lt;- fluidPage(\n  \n  # Application title\n  titlePanel(\"Poisson distribution CDF and PMF\"),\n  \n  # Sidebar with a slider input for number of bins \n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"lambda\",\n                  \"Rate (λ):\",\n                  min = 0,\n                  max = 100,\n                  value = 1,\n                  step = 0.1)\n    ),\n    \n    # Show a plot of the generated distribution\n    mainPanel(\n      plotOutput(\"distPlot\")\n    )\n  )\n)\n\n# Define server logic required to draw a histogram\nserver &lt;- function(input, output) {\n  \n  output$distPlot &lt;- renderPlot({\n    \n    lambda &lt;- input$lambda\n    n &lt;- 100\n\n    par(mfrow = c(2, 1), mar = c(4, 4, 2, 2))\n    \n    discrete_cdf(0:n, dpois(0:n, lambda))\n    discrete_pmf(0:n, dpois(0:n, lambda), add_mean = FALSE)\n    mtext(\"E(X)\", side = 1, at = lambda, col = \"red\", line = 2)\n  })\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)",
    "crumbs": [
      "Distribution Families",
      "Poisson"
    ]
  },
  {
    "objectID": "distributions/discrete-poisson.html#derivations",
    "href": "distributions/discrete-poisson.html#derivations",
    "title": "Poisson distribution",
    "section": "Derivations",
    "text": "Derivations\n\n\n\n\n\n\nThe mean (via the definition)\n\n\n\n\n\nMassage and squint until you recognize the Taylor series of \\(e^x\\):\n\\[\n    \\begin{align*}\n    E(X)\n    &=\n\\sum\\limits_{n=0}^\\infty nP(X=n)\n    \\\\\n    &=\n    \\sum\\limits_{n=0}^\\infty\n    n\n    \\frac{\\lambda^n}{n!}e^{-\\lambda}\n    \\\\\n    &=\n    e^{-\\lambda}\n    \\sum\\limits_{n=0}^\\infty\n    n\n    \\frac{\\lambda^n}{n!}\n    &&\n    \\text{Pull out constant}\n    \\\\\n    &=\n    e^{-\\lambda}\n    \\sum\\limits_{n=1}^\\infty\n    n\n    \\frac{\\lambda^n}{n!}\n    &&\n    n=0\\text{ term is equal to 0}\n    \\\\\n    &=\n    e^{-\\lambda}\n    \\sum\\limits_{n=1}^\\infty\n    \\frac{\\lambda^n}{(n-1)!}\n    &&\n    n\\text{ cancels}\n    \\\\\n    &=\n    \\lambda e^{-\\lambda}\n    \\sum\\limits_{n=1}^\\infty\n    \\frac{\\lambda^{n-1}}{(n-1)!}\n    &&\n    \\text{Pull out }\\lambda\n    \\\\\n    &=\n    \\lambda e^{-\\lambda}\n    \\sum\\limits_{j=0}^\\infty\n    \\frac{\\lambda^{j}}{j!}\n    &&\n    \\text{Reindex}\n    \\\\\n    &=\n    \\lambda e^{-\\lambda}\n    e^\\lambda\n    &&\n    \\text{Recall Taylor series: } e^x=\\sum\\limits_{k=0}^\\infty \\frac{x^k}{k!}\\,\\forall\\,x\\in\\mathbb{R}\n    \\\\\n    &=\n    \\lambda\n    .\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n\nThe moment-generating function\n\n\n\n\n\nMassage and squint until you recognize the Taylor series of \\(e^x\\):\n\\[\n\\begin{aligned}\nM(t)&=E[e^{tX}]\\\\\n&=\\sum\\limits_{k=0}^\\infty e^{tk}\\frac{\\lambda^k}{k!}e^{-\\lambda}\\\\\n&=\\sum\\limits_{k=0}^\\infty \\left(e^{t}\\right)^k\\frac{\\lambda^k}{k!}e^{-\\lambda}\\\\\n&=\\sum\\limits_{k=0}^\\infty \\frac{\\left(\\lambda e^{t}\\right)^k}{k!}e^{-\\lambda}\\\\\n&=e^{-\\lambda}\\sum\\limits_{k=0}^\\infty \\frac{\\left(\\lambda e^{t}\\right)^k}{k!}\\\\\n&=e^{-\\lambda}e^{\\lambda e^t}\\\\\n&=e^{\\lambda e^t-\\lambda}.\n\\end{aligned}\n\\]\nThis works for any \\(t\\in\\mathbb{R}\\).\n\n\n\n\n\n\n\n\n\nThe mean and variance (via the MGF)\n\n\n\n\n\nUse the chain rule, product rule, etc to compute the first two derivatives of the MGF, and then plug in zero:\n\\[\n\\begin{aligned}\nM'(t)&=\\lambda e^t\\exp(\\lambda e^t-\\lambda)\\\\\nM''(t)&=(\\lambda e^t)^2\\exp(\\lambda e^t-\\lambda)+\\lambda e^t\\exp(\\lambda e^t-\\lambda)\\\\\n\\\\\nE(X)=M'(0)&=\\lambda\\\\\nE(X^2)=M''(0)&=\\lambda^2+\\lambda\n\\end{aligned}\n\\]\nSo \\(E(X) = \\lambda\\) and \\(\\text{var}(X) = E(X^2) - E(X)^2 = \\lambda^2 + \\lambda - \\lambda^2 = \\lambda\\).",
    "crumbs": [
      "Distribution Families",
      "Poisson"
    ]
  },
  {
    "objectID": "distributions/continuous-inverse-gamma.html",
    "href": "distributions/continuous-inverse-gamma.html",
    "title": "Inverse gamma distribution",
    "section": "",
    "text": "If \\(X\\sim\\text{Gamma}(\\alpha,\\,\\beta)\\), then the new random variable \\(Y=1/X\\) has the inverse gamma distribution. We denote this \\(Y\\sim\\text{IG}(\\alpha,\\,\\beta)\\). You derived this on Problem Set 6 #6d.",
    "crumbs": [
      "Distribution Families",
      "Inverse gamma"
    ]
  },
  {
    "objectID": "distributions/continuous-inverse-gamma.html#basic-properties",
    "href": "distributions/continuous-inverse-gamma.html#basic-properties",
    "title": "Inverse gamma distribution",
    "section": "Basic properties",
    "text": "Basic properties\n\n\n\n\n\n\n\nNotation\n\\(X\\sim\\text{IG}(\\alpha,\\,\\beta)\\)\n\n\nRange\n\\((0,\\,\\infty)\\)\n\n\nParameter space\n\\(\\alpha,\\,\\beta&gt;0\\)\n\n\nPDF\n\\(f(x)=\\begin{cases}\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}x^{-\\alpha-1}e^{-\\beta /x} &x&gt; 0\\\\0 & x\\leq0.\\end{cases}\\)\n\n\nExpectation\n\\(\\frac{\\beta}{\\alpha-1}\\) if \\(\\alpha&gt;1\\)\n\n\nVariance\n\\(\\frac{\\beta^2}{(\\alpha-1)^2(\\alpha-2)}\\) if \\(\\alpha&gt;2\\)",
    "crumbs": [
      "Distribution Families",
      "Inverse gamma"
    ]
  },
  {
    "objectID": "distributions/continuous-inverse-gamma.html#derivations",
    "href": "distributions/continuous-inverse-gamma.html#derivations",
    "title": "Inverse gamma distribution",
    "section": "Derivations",
    "text": "Derivations\n\n\n\n\n\n\nDeriving the density from a change-of-variables\n\n\n\n\n\nLet \\(X\\sim\\text{Gamma}(\\alpha,\\,\\beta)\\) and consider \\(Y=1/X\\). So the forward transformation is \\(g(x) = 1/ x\\), and the inverse is \\(g^{-1}(y) = 1/ y\\). These are strictly decreasing, and they map positive numbers to positive numbers. So \\(\\text{Range}(Y)=(0,\\, \\infty)\\). Fix \\(y&gt;0\\). The change-of-variables formula says:\n\\[\n\\begin{aligned}\nf_Y(y) &= f_X \\left( g^{-1}(y) \\right) \\left| \\frac{d}{dy} g^{-1}(y) \\right| \\\\[6pt]\n&= \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\left( \\frac{1}{y} \\right)^{\\alpha-1} \\exp \\left(  \\frac{-\\beta}{y}  \\right)  \\left|-\\frac{1}{y^2}\\right| \\\\[6pt]\n&= \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\left( \\frac{1}{y} \\right)^{\\alpha-1} \\exp \\left(  \\frac{-\\beta}{y}  \\right)  \\frac{1}{y^2} \\\\[6pt]\n&= \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\left( \\frac{1}{y} \\right)^{\\alpha+1} \\exp \\left(  \\frac{-\\beta}{y}  \\right)   \\\\[6pt]\n&= \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} y^{-\\alpha-1} \\exp \\left(  \\frac{-\\beta}{y}  \\right),&& y&gt;0.\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\nGeneral formula for the raw moments\n\n\n\n\n\nLet \\(X\\sim\\text{Gamma}(\\alpha,\\,\\beta)\\) and \\(Y=1/X\\sim\\text{IG}(\\alpha,\\,\\beta)\\). Furthermore, assume \\(\\alpha &gt; k\\) for some \\(k\\in\\mathbb{N}\\). Then the \\(k\\)th raw moment of the inverse gamma variate \\(Y\\) is\n\\[\n\\begin{aligned}\nE(Y^k)\n&=\nE\\left(\\frac{1}{X^k}\\right)\n\\\\\n&=\n\\int_0^\\infty\n\\frac{1}{x^k}\nf_X(x)\n\\,\\text{d}x\n\\\\\n&=\n\\int_0^\\infty\n\\frac{1}{x^k}\n\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}x^{\\alpha-1}e^{-\\beta x}\n\\,\\text{d}x\n\\\\\n&=\n\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\n\\int_0^\\infty\n\\underbrace{x^{\\alpha-k-1}e^{-\\beta x}}_{\\text{kernel of Gamma}(\\alpha-k,\\,\\beta)}\n\\,\\text{d}x\n\\\\\n&=\n\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\n\\frac{\\Gamma(\\alpha-k)}{\\beta^{\\alpha-k}}\n\\\\\n&=\n\\beta^k\\frac{\\Gamma(\\alpha-k)}{\\Gamma(\\alpha)}.\n\\end{aligned}\n\\]\nSo\n\\[\n\\begin{aligned}\nE(Y)\n&=\n\\beta\\frac{\\Gamma(\\alpha-1)}{\\Gamma(\\alpha)}=\\beta\\frac{\\Gamma(\\alpha-1)}{\\Gamma(\\alpha-1+1)}=\\beta\\frac{\\Gamma(\\alpha-1)}{(\\alpha-1)\\Gamma(\\alpha-1)}=\\frac{\\beta}{\\alpha-1}\n\\\\\n\\\\\nE(Y^2)\n&=\n\\beta^2\\frac{\\Gamma(\\alpha-2)}{\\Gamma(\\alpha)}\n\\\\\n&=\n\\beta^2\\frac{\\Gamma(\\alpha-2)}{\\Gamma(\\alpha-1+1)}\n\\\\\n&=\n\\beta^2\\frac{\\Gamma(\\alpha-2)}{(\\alpha-1)\\Gamma(\\alpha-1)}\n\\\\\n&=\n\\beta^2\\frac{\\Gamma(\\alpha-2)}{(\\alpha-1)\\Gamma(\\alpha-2+1)}\n\\\\\n&=\n\\beta^2\\frac{\\Gamma(\\alpha-2)}{(\\alpha-1)(\\alpha-2)\\Gamma(\\alpha-2)}\n\\\\\n&=\n\\frac{\\beta^2}{(\\alpha-1)(\\alpha-2)}\n\\\\\n\\\\\n\\text{var}(Y)\n&=\nE(Y^2)-E(Y)^2\n\\\\\n&=\n\\frac{\\beta^2}{(\\alpha-1)(\\alpha-2)}\n-\n\\frac{\\beta^2}{(\\alpha-1)^2}\n\\\\\n&=\n\\beta^2\n\\left[\n\\frac{1}{(\\alpha-1)(\\alpha-2)}\n-\n\\frac{1}{(\\alpha-1)^2}\n\\right]\n\\\\\n&=\n\\beta^2\n\\frac{\\alpha-1-(\\alpha-2)}{(\\alpha-1)^2(\\alpha-2)}\n\\\\\n&=\n\\beta^2\n\\frac{\\alpha-1-\\alpha+2}{(\\alpha-1)^2(\\alpha-2)}\n\\\\\n&=\n\\frac{\\beta^2}{(\\alpha-1)^2(\\alpha-2)}\n.\n\\end{aligned}\n\\]",
    "crumbs": [
      "Distribution Families",
      "Inverse gamma"
    ]
  },
  {
    "objectID": "distributions/continuous-gamma.html",
    "href": "distributions/continuous-gamma.html",
    "title": "Gamma distribution",
    "section": "",
    "text": "The gamma distribution is a common choice for modeling continuous quantities that are non-negative: dollar amounts, waiting times, etc. It is governed by two positive parameters: the shape \\(\\alpha&gt;0\\) and the rate \\(\\beta&gt;0\\). Don’t worry about interpreting what those words mean. They’re just labels.",
    "crumbs": [
      "Distribution Families",
      "Gamma"
    ]
  },
  {
    "objectID": "distributions/continuous-gamma.html#basic-properties",
    "href": "distributions/continuous-gamma.html#basic-properties",
    "title": "Gamma distribution",
    "section": "Basic properties",
    "text": "Basic properties\nThe gamma distribution gets its name because it is related to the so-called gamma function, which appears in the normalizing constant:\n\\[\n\\Gamma(\\alpha) = \\int_0^\\infty x^{\\alpha-1}e^{-x}\\,\\text{d}x.\n\\]\nWe have a lil’ math explainer where you can read more about it.\n\n\n\n\n\n\n\nNotation\n\\(X\\sim\\text{Gamma}(\\alpha,\\,\\beta)\\)\n\n\nRange\n\\([0,\\,\\infty)\\)\n\n\nParameter space\n\\(\\alpha,\\,\\beta&gt;0\\)\n\n\nPDF\n\\(f(x)=\\begin{cases}\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}x^{\\alpha-1}e^{-\\beta x} &x\\geq 0\\\\0 & x&lt;0.\\end{cases}\\)\n\n\nMGF\n\n\\(M(t)=\\left(\\frac{\\beta}{\\beta-t}\\right)^\\alpha\\), \\(t&lt;\\beta\\)\n\n\n\nExpectation\n\\(\\alpha/\\beta\\)\n\n\nVariance\n\\(\\alpha/\\beta^2\\)",
    "crumbs": [
      "Distribution Families",
      "Gamma"
    ]
  },
  {
    "objectID": "distributions/continuous-gamma.html#special-cases",
    "href": "distributions/continuous-gamma.html#special-cases",
    "title": "Gamma distribution",
    "section": "Special cases",
    "text": "Special cases\nParticular special cases of the gamma distribution get special nicknames:\n\nIf \\(X\\sim\\text{Gamma}(1,\\,\\beta)\\), then \\(X\\) has the exponential distribution, denoted \\(X\\sim\\text{Exp}(\\beta)\\) This special case is nice because we get closed-form expressions for the CDF and the quantile function:\n\n\\[\n\\begin{aligned}\nf(x)&=\\beta e^{-\\beta x}\\\\\nF(x)&=1-e^{-\\beta x}\\\\\nF^{-1}(p)&=-\\frac{\\ln(1-p)}{\\beta}.\n\\end{aligned}\n\\]\n\nIf \\(X\\sim\\text{Gamma}(\\nu/2,\\, 1/2)\\), then \\(X\\) has the chi-squared distribution, denoted \\(X\\sim\\chi^2_\\nu\\). In this context we call the parameter \\(\\nu&gt;0\\) the degrees of freedom of the chi-squared distribution.\nIf \\(X\\sim\\text{Gamma}(k,\\,\\beta)\\) for some integer \\(k\\), then \\(X\\) has the Erlang distribution, denoted \\(X\\sim\\text{Erlang}(k,\\, \\beta)\\)",
    "crumbs": [
      "Distribution Families",
      "Gamma"
    ]
  },
  {
    "objectID": "distributions/continuous-gamma.html#r-commands",
    "href": "distributions/continuous-gamma.html#r-commands",
    "title": "Gamma distribution",
    "section": "\nR commands",
    "text": "R commands\nHere is the documentation for the suite of commands that let you work with the gamma distribution in R:\n\ndgamma(x, shape, rate = 1) # PDF\npgamma(q, shape, rate = 1) # CDF: P(X &lt;= q)\nqgamma(p, shape, rate = 1) # quantile function (inverse CDF)\nrgamma(n, shape, rate = 1) # random numbers",
    "crumbs": [
      "Distribution Families",
      "Gamma"
    ]
  },
  {
    "objectID": "distributions/continuous-gamma.html#play-around",
    "href": "distributions/continuous-gamma.html#play-around",
    "title": "Gamma distribution",
    "section": "Play around!",
    "text": "Play around!\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  titlePanel(\"Gamma distribution CDF and PDF\"),\n  \n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"alpha\", \"Shape (α):\",\n                  min = 0.5, max = 5, value = 2, step = 0.1),\n      sliderInput(\"beta\", \"Rate (β):\",\n                  min = 0.2, max = 2, value = 0.5, step = 0.05),\n      hr(),\n      verbatimTextOutput(\"moments\")\n    ),\n    \n    mainPanel(\n      plotOutput(\"gammaPlot\", height = \"600px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  output$moments &lt;- renderText({\n    alpha &lt;- input$alpha\n    beta  &lt;- input$beta\n    mu &lt;- alpha / beta\n    sd &lt;- sqrt(alpha) / beta\n    paste0(\"E(X) = \", round(mu, 3),\n           \"\\nsd(X) = \", round(sd, 3))\n  })\n  \n  output$gammaPlot &lt;- renderPlot({\n    alpha &lt;- input$alpha\n    beta  &lt;- input$beta\n    \n    # Fixed plotting range\n    x_min &lt;- -2\n    x_max &lt;- 25\n    x &lt;- seq(x_min, x_max, length.out = 2000)\n    \n    # PDF and CDF\n    pdf_vals &lt;- ifelse(x &gt;= 0, dgamma(x, shape = alpha, rate = beta), 0)\n    cdf_vals &lt;- ifelse(x &gt;= 0, pgamma(x, shape = alpha, rate = beta), 0)\n    \n    mu &lt;- alpha / beta\n    \n    # Fixed y-limits\n    pdf_ylim &lt;- c(0, 0.8)\n    cdf_ylim &lt;- c(0, 1)\n    \n    par(mfrow = c(2, 1), mar = c(4, 4, 2, 1))\n    \n    # --- CDF ---\n    plot(x, cdf_vals, type = \"l\", lwd = 2, col = \"blue\",\n         xlim = c(x_min, x_max), ylim = cdf_ylim,\n         main = \"Cumulative Distribution Function (CDF)\",\n         xlab = \"\", ylab = \"F(x)\")\n    abline(h = c(0, 1), col = \"gray80\", lty = 2)\n    #abline(v = mu, col = \"gray60\", lty = 3)\n    \n    # --- PDF ---\n    plot(x, pdf_vals, type = \"l\", lwd = 2, col = \"darkred\",\n         xlim = c(x_min, x_max), ylim = pdf_ylim,\n         main = \"Probability Density Function (PDF)\",\n         xlab = \"x\", ylab = \"f(x)\")\n    #abline(v = mu, col = \"gray60\", lty = 3)\n    \n    # Label E(X) with mtext along x-axis\n    if (mu &gt;= x_min && mu &lt;= x_max) {\n      mtext(\"E(X)\",\n            side = 1, line = 2.2, at = mu, cex = 1.0)\n    }\n  })\n}\n\nshinyApp(ui = ui, server = server)",
    "crumbs": [
      "Distribution Families",
      "Gamma"
    ]
  },
  {
    "objectID": "distributions/continuous-gamma.html#derivations",
    "href": "distributions/continuous-gamma.html#derivations",
    "title": "Gamma distribution",
    "section": "Derivations",
    "text": "Derivations\n\n\n\n\n\n\nCDF, quantile function, and median of Gamma(1, \\(\\beta\\))\n\n\n\n\n\nLet \\(X\\sim\\text{Exp}(\\beta)\\), and recall that this means \\(X\\sim\\text{Gamma}(\\alpha=1,\\, \\beta)\\). So the pdf is \\(f_X(x)=\\beta\\exp(-\\beta x)\\) for \\(x&gt;0\\), and the cdf is\n\\[\n\\begin{aligned}\nF_X(x)\n&=\n\\int_0^x\\beta e^{-\\beta t}\\,\\text{d} t\n=\n[\n-e^{\\beta t}\n]_0^x\n=\n1-e^{-\\beta x},\\quad x&gt;0.\n\\end{aligned}\n\\]\nFor some \\(y\\in(0,\\, 1)\\), we see that\n\\[\n\\begin{align*}\ny&=1-e^{-\\beta x}\\\\\ne^{-y}&=1-y\\\\\n-\\beta x&=\\ln(1-y)\\\\\nx&=\\frac{-\\ln(1-y)}{\\beta},\n\\end{align*}\n\\]\nand so the quantile function is\n\\[\nF_X^{-1}(y)=-\\frac{\\ln(1-y)}{\\beta},\n\\]\nand the median of the exponential distribution is\n\\[\nF_X^{-1}(0.5)=-\\frac{\\ln(1-0.5)}{\\beta}=-\\frac{\\ln(0.5)}{\\beta}=\\frac{\\ln 2}{\\beta}.\n\\]\n\n\n\n\n\n\n\n\n\nMoment-generating function\n\n\n\n\n\nThis is classic “massage and squint.” Let \\(t\\in(-\\infty,\\,\\beta)\\). Then\n\\[\n      \\begin{align*}\n          M_X(t)&=E\\left(e^{tX}\\right)\\\\\n          &=\\int_0^\\infty e^{tx}\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}x^{\\alpha-1}e^{-\\beta x}\\,\\text{d} x && \\text{LOTUS}\\\\\n          &=\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\int_0^\\infty x^{\\alpha-1}e^{tx}e^{-\\beta x}\\,\\text{d} x && \\text{pull out constant}\\\\\n          &=\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\int_0^\\infty x^{\\alpha-1}e^{tx-\\beta x}\\,\\text{d} x&& \\text{combine base-e terms}\\\\\n`         &=\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\int_0^\\infty x^{\\alpha-1}e^{(t-\\beta) x}\\,\\text{d} x&& \\text{factor out }x\\\\\n`         &=\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\int_0^\\infty x^{\\alpha-1}e^{-(\\beta-t) x}\\,\\text{d} x&& \\text{factor out -1}\\\\\n          &=\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\int_0^\\infty \\underbrace{x^{\\alpha-1}e^{-(\\beta-t)x}}_{\\text{kernel of Gamma}(\\alpha,\\,\\beta-t)}\\,\\text{d} x && \\text{recognize}\\\\\n          &=\\frac{\\beta^\\alpha}{{\\Gamma(\\alpha)}}\\frac{{\\Gamma(\\alpha)}}{(\\beta-t)^\\alpha}\\\\\n          &=\\left(\\frac{\\beta}{\\beta-t}\\right)^\\alpha.\n      \\end{align*}\n\\]\nIn order to invoke the gamma density kernel, we require that \\(t&lt;\\beta\\) so that the rate parameter of the implied gamma is positive.",
    "crumbs": [
      "Distribution Families",
      "Gamma"
    ]
  },
  {
    "objectID": "apps/disease-testing.html",
    "href": "apps/disease-testing.html",
    "title": "You tested positive. Now what?",
    "section": "",
    "text": "Testing positive for a disease is not exactly good news, but how alarmed you should be ultimately depends on a few factors: the accuracy of the test and the overall rarity of the disease (the so-called base rate). Before you walk into the doctor’s office, the probability that you are sick is \\(P(D=+)\\), the prevalence. This is the unconditional (or marginal) probability that a random member of the population (ie you in that moment) is sick. After you test positive, the revised or conditional probability that you have the disease is given by Bayes’ theorem:\n\\[\n\\begin{aligned}\nP(D = +\\,|\\,T=+)\n&=\n\\frac{{\\color{blue}{P(T=+\\,|\\,D=+)}}{\\color{red}{P(D=+)}}}{P(T=+)}\n\\\\\n&=\n\\frac{{\\color{blue}{P(T=+\\,|\\,D=+)}}{\\color{red}{P(D=+)}}}{{\\color{blue}{P(T=+\\,|\\,D=+)}}{\\color{red}{P(D=+)}}+[1-{\\color{green}{P(T=-\\,|\\,D=-)}}][1-{\\color{red}{P(D=+)}}]}\n\\\\\n&=\n\\frac{{\\color{blue}{\\text{TPR}}}\\times{\\color{red}{\\text{PREV}}}}{{\\color{blue}{\\text{TPR}}}\\times{\\color{red}{\\text{PREV}}}+(1-{\\color{green}{\\text{TNR}}})(1-{\\color{red}{\\text{PREV}}})}.\n\\end{aligned}\n\\]\n\\(P(T=+\\,|\\,D=+)\\) is the true positive rate or sensitivity: the probability that a sick person is correctly diagnosed. \\(P(T=-\\,|\\,D=-)\\) is the true negative rate or specificity: the probability that a well person is correctly diagnosed. It is the nonlinear interaction of three numbers (prevalence, TPR, and TNR) that determines the revised likelihood that you have the disease. Two numbers interacting linearly is about all a regular human can handle off the dome, hence people are generally bad at reasoning about this stuff.\nThe application below visualizes the sample space for our simple model of disease testing. We have \\(D\\in\\{-,\\,+\\}\\) and \\(T\\in\\{-,\\,+\\}\\), and so there are four individual outcomes:\n\n\\(P(T = +\\cap D=+)=P(T=+\\,|\\,D=+)P(D=+)=\\text{TPR}\\times P(D=+)\\);\n\\(P(T = +\\cap D=-)=P(T=+\\,|\\,D=-)P(D=-)=\\text{FPR}\\times P(D=-)\\);\n\\(P(T = -\\cap D=-)=P(T=-\\,|\\,D=-)P(D=-)=\\text{TNR}\\times P(D=-)\\);\n\\(P(T = -\\cap D=+)=P(T=-\\,|\\,D=+)P(D=+)=\\text{FNR}\\times P(D=+)\\).\n\nThe application plots the \\([0,\\,1]\\times[0,\\,1]\\) unit square, partitioned into four rectangles representing the outcomes listed above. The area of each rectangle is equal to the probability of that outcome.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 650\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  \n  # Application title\n  titlePanel(\"The sample space of a simple diagnostic test\"),\n  \n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"TPR\",\n                  \"Sensitivity (TPR):\",\n                  min = 0,\n                  max = 1,\n                  value = 0.5),\n      sliderInput(\"TNR\",\n                  \"Specificity (TNR):\",\n                  min = 0,\n                  max = 1,\n                  value = 0.5),\n      sliderInput(\"p\",\n                  \"Prevalence:\",\n                  min = 0,\n                  max = 1,\n                  value = 0.5)\n    ),\n    \n    mainPanel(\n      plotOutput(\"distPlot\"),\n      \n      # ---- Legend goes here ----\n      tags$div(\n        style = \"margin-top:20px;\",\n        tags$h4(\"Legend\"),\n        tags$div(style=\"display:flex; align-items:center; margin-bottom:5px;\",\n                 tags$div(style=\"width:20px; height:20px; background-color:rgba(255,0,0,0.25); margin-right:10px;\"),\n                 \"True Positive (T=+, D=+)\"\n        ),\n        tags$div(style=\"display:flex; align-items:center; margin-bottom:5px;\",\n                 tags$div(style=\"width:20px; height:20px; background-color:rgba(0,255,0,0.25); margin-right:10px;\"),\n                 \"False Negative (T=-, D=+)\"\n        ),\n        tags$div(style=\"display:flex; align-items:center; margin-bottom:5px;\",\n                 tags$div(style=\"width:20px; height:20px; background-color:rgba(0,0,255,0.25); margin-right:10px;\"),\n                 \"True Negative (T=-, D=-)\"\n        ),\n        tags$div(style=\"display:flex; align-items:center; margin-bottom:5px;\",\n                 tags$div(style=\"width:20px; height:20px; background-color:rgba(255,165,0,0.25); margin-right:10px;\"),\n                 \"False Positive (T=+, D=-)\"\n        )\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  \n  output$distPlot &lt;- renderPlot({\n    \n    p &lt;- input$p\n    TNR &lt;-input$TNR\n    TPR &lt;- input$TPR\n    \n    FNR &lt;- 1 - TPR\n    FPR &lt;- 1 - TNR\n    \n    par(mar = c(8, 8, 1, 8))\n    \n    plot(0, col = \"white\", xlim = c(0, 1), ylim = c(0, 1),\n         xaxt = \"n\", yaxt = \"n\", bty = \"n\", xlab = \"\", ylab = \"\")\n    \n    polygon(c(0, p, p, 0), c(FNR, FNR, 1.0, 1.0), col = rgb(1, 0, 0, alpha = 0.25), border = NA)\n    polygon(c(0, p, p, 0), c(0, 0, FNR, FNR), col = rgb(0, 1, 0, alpha = 0.25), border = NA)\n    polygon(c(p, 1, 1, p), c(0, 0, TNR, TNR), col = rgb(0, 0, 1, alpha = 0.25), border = NA)\n    polygon(c(p, 1, 1, p), c(TNR, TNR, 1, 1), col = rgb(1, 0.647, 0, alpha = 0.25), border = NA)\n    \n    mtext(\"TNR\", side = 4, at = TNR / 2, las = 1)\n    mtext(\"FPR\", side = 4, at = (1 + TNR) / 2, las = 1)\n    \n    mtext(\"TPR\", side = 2, at = (1 + FNR) / 2, las = 1, line = 2)\n    mtext(\"FNR\", side = 2, at = FNR / 2, las = 1, line = 2)\n    \n    mtext(\"p(D = +)\", side = 1, at = p / 2, las = 1, line = 2)\n    mtext(\"p(D = -)\", side = 1, at = (1 + p) / 2, las = 1, line = 2)\n    \n    mtext(c(\"0\", \"1\"), side = 1, at = c(0, 1), las = 1)\n    mtext(c(\"0\", \"1\"), side = 2, at = c(0, 1), las = 1)\n  })\n}\n\nshinyApp(ui = ui, server = server)\n\n\nThe application below displays how testing positive shifts the probability of disease from \\(P(D=+)\\) to \\(P(D = +\\,|\\,T=+)\\). The size of the revision ultimately depends on how big \\(P(D=+)\\) was to begin with, and how good the test is (TPR and TNR). Play around with different values and see what happens:\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 500\n\nlibrary(shiny)\n\n# Define UI for application that draws a histogram\nui &lt;- fluidPage(\n\n    # Application title\n    titlePanel(\"What can a diagnostic test teach you?\"),\n\n    # Sidebar with a slider input for number of bins \n    sidebarLayout(\n        sidebarPanel(\n            sliderInput(\"TPR\",\n                        \"Sensitivity (TPR):\",\n                        min = 0,\n                        max = 1,\n                        value = 0.5),\n            sliderInput(\"TNR\",\n                        \"Specificity (TNR):\",\n                        min = 0,\n                        max = 1,\n                        value = 0.5)\n        ),\n\n        # Show a plot of the generated distribution\n        mainPanel(\n           plotOutput(\"distPlot\")\n        )\n    )\n)\n\n# Define server logic required to draw a histogram\nserver &lt;- function(input, output) {\n\n    output$distPlot &lt;- renderPlot({\n      TPR &lt;- input$TPR\n      TNR &lt;- input$TNR\n      P &lt;- seq(0, 1, length.out = 1000)\n      B &lt;- TPR * P / (TPR * P + (1 - TNR) * (1 - P))\n\n      # draw the histogram with the specified number of bins\n      par(mar = c(5, 5, 5, 0.5))\n      plot(P, B, type = \"l\", xlim = c(0, 1), ylim = c(0, 1),\n           xlab = \"P(D = +)\", ylab = \"P(D = + | T = +)\",\n           lwd = 2, cex.lab = 2,\n           panel.first = abline(a = 0, b = 1, lty = 2, lwd = 2, col = \"grey\"))\n    })\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "explainers/sample.html",
    "href": "explainers/sample.html",
    "title": "The sample command",
    "section": "",
    "text": "The function sample() lets us pick random values. This is very useful in probability, because it allows us to simulate experiments like coin flips, die rolls, or drawing cards.\nThe pattern is:\nsample(x, size = n, replace = FALSE)",
    "crumbs": [
      "`R` Helpers",
      "`sample`"
    ]
  },
  {
    "objectID": "explainers/sample.html#example-coin-flip",
    "href": "explainers/sample.html#example-coin-flip",
    "title": "The sample command",
    "section": "Example: coin flip",
    "text": "Example: coin flip\nThis chooses either heads (H) or tails (T) at random:\n\nsample(c(\"H\", \"T\"), size = 1)\n\n[1] \"H\"",
    "crumbs": [
      "`R` Helpers",
      "`sample`"
    ]
  },
  {
    "objectID": "explainers/sample.html#example-die-roll",
    "href": "explainers/sample.html#example-die-roll",
    "title": "The sample command",
    "section": "Example: die roll",
    "text": "Example: die roll\nThis rolls a fair die twenty times:\n\nsample(1:6, size = 20, replace = TRUE)\n\n [1] 6 3 2 4 3 3 2 1 4 1 2 6 5 1 1 4 5 4 4 5\n\n\nWe need replace = TRUE here, because after rolling a die, the next roll can be the same number again.",
    "crumbs": [
      "`R` Helpers",
      "`sample`"
    ]
  },
  {
    "objectID": "explainers/sample.html#example-spotify-shuffle",
    "href": "explainers/sample.html#example-spotify-shuffle",
    "title": "The sample command",
    "section": "Example: Spotify shuffle",
    "text": "Example: Spotify shuffle\nHere’s my playlist:\n\nplaylist &lt;- c(\"Ramble On\",\n              \"It's Raining Men\",\n              \"Tears\",\n              \"Tiptoe Through the Tulips\",\n              \"Gangsta's Paradise\",\n              \"Days of Wine and Roses\")\n\nThere are 6 songs, so if I generate a sample of size 6 without replacement, I will ultimately draw each song exactly once, but in a random order:\n\nsample(playlist, size = length(playlist), replace = FALSE)\n\n[1] \"Tears\"                     \"Tiptoe Through the Tulips\"\n[3] \"Gangsta's Paradise\"        \"Ramble On\"                \n[5] \"Days of Wine and Roses\"    \"It's Raining Men\"",
    "crumbs": [
      "`R` Helpers",
      "`sample`"
    ]
  },
  {
    "objectID": "explainers/setup.html",
    "href": "explainers/setup.html",
    "title": "Getting started",
    "section": "",
    "text": "The path of least resistance to using R in this class is to reserve a container from the Duke Container Manager. It’s hosted in the cloud, and it has all of the software you will ever need for this class.\n\nGo here: https://cmgr.oit.duke.edu/containers. You may have to log in with your NetID at some point;\n(The first time you do this, you look for STA240 under “Reservations available” on the righthand side, and click “reserve STA240”. After you do that once, STA240 will appear under “My reservations” on the lefthand side forever more)\nClick STA240 under “My reservations”;\nLogin;\nStart. It may take a while, but then RStudio should launch in your browser.",
    "crumbs": [
      "`R` Helpers",
      "Getting started"
    ]
  },
  {
    "objectID": "explainers/setup.html#option-1-reserve-a-container",
    "href": "explainers/setup.html#option-1-reserve-a-container",
    "title": "Getting started",
    "section": "",
    "text": "The path of least resistance to using R in this class is to reserve a container from the Duke Container Manager. It’s hosted in the cloud, and it has all of the software you will ever need for this class.\n\nGo here: https://cmgr.oit.duke.edu/containers. You may have to log in with your NetID at some point;\n(The first time you do this, you look for STA240 under “Reservations available” on the righthand side, and click “reserve STA240”. After you do that once, STA240 will appear under “My reservations” on the lefthand side forever more)\nClick STA240 under “My reservations”;\nLogin;\nStart. It may take a while, but then RStudio should launch in your browser.",
    "crumbs": [
      "`R` Helpers",
      "Getting started"
    ]
  },
  {
    "objectID": "explainers/setup.html#option-2-download-it-yourself",
    "href": "explainers/setup.html#option-2-download-it-yourself",
    "title": "Getting started",
    "section": "Option 2: download it yourself",
    "text": "Option 2: download it yourself\nOf course, you won’t have containers after you graduate, so maybe it’s a good idea to hike up your big girl pants and actually install something. It’s pretty easy:\n\nDownload R: https://cran.r-project.org\nDownload RStudio: https://posit.co/download/rstudio-desktop/\n(optional) Install tinytex: https://yihui.org/tinytex/",
    "crumbs": [
      "`R` Helpers",
      "Getting started"
    ]
  },
  {
    "objectID": "explainers/math.html",
    "href": "explainers/math.html",
    "title": "Basic math in R\n",
    "section": "",
    "text": "Using R for the calculator is like buying a plane ticket to get free peanuts. But I have done my taxes in R, so…\n\n2 + 2 # addition\n\n[1] 4\n\n5 - 2 # subtraction\n\n[1] 3\n\n0.5 * 6 # multiplication\n\n[1] 3\n\n3 / 4 # division\n\n[1] 0.75\n\nsqrt(2) # square root\n\n[1] 1.414214\n\n4 ^ 2 # exponent\n\n[1] 16\n\nexp(4.9) # base-e exponent\n\n[1] 134.2898\n\nlog(7) # natural log\n\n[1] 1.94591\n\nabs(-6) # absolute value\n\n[1] 6\n\nceiling(4.6) # round up\n\n[1] 5\n\nfloor(4.6) # round down\n\n[1] 4\n\nfactorial(4) # n! = n x (n-1) x ... x 2 x 1\n\n[1] 24\n\nchoose(8, 3) # binomial coefficient (we'll get there)\n\n[1] 56",
    "crumbs": [
      "`R` Helpers",
      "Basic math"
    ]
  },
  {
    "objectID": "explainers/vectors.html",
    "href": "explainers/vectors.html",
    "title": "Vectors",
    "section": "",
    "text": "A vector in R is just an ordered set of values that have the same type. The easiest way to create one is to manually list out the values, separated by commas, inside c():\n\nmyvec &lt;- c(pi, 5, 3.6, 2, 9, 6000) \nmyvec\n\n[1]    3.141593    5.000000    3.600000    2.000000    9.000000 6000.000000\n\n\nIf you want to list out all of the integers between some min and max, you can use this shortcut:\n\n5:15\n\n [1]  5  6  7  8  9 10 11 12 13 14 15\n\n\nMore generally, if you want a vector of evenly spaced numbers between a min and a max, do this:\n\na &lt;- seq(0, 1, length.out = 11) \na\n\n [1] 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\n\n\nIf you want to create a “blank” vector that just has zeros in it, here you go:\n\nz &lt;- numeric(10)\nz\n\n [1] 0 0 0 0 0 0 0 0 0 0\n\n\nIn all of those cases, our vector contained numbers, but there is nothing special about numbers. Here is a vector where each value is a string (a piece of text):\n\npoetry &lt;- c(\"Mary\", \"had\", \"a\", \"little\", \"chainsaw\")\npoetry\n\n[1] \"Mary\"     \"had\"      \"a\"        \"little\"   \"chainsaw\"\n\n\nIf you have two vectors and you want to combine them into one, you can just use the c() command again. The “c” stands for “concatenate.” You’re concatenating, or joining, the vectors end-to-end:\n\nmore &lt;- c(\"whose\", \"chain\", \"was\", \"red\", \"as\")\n\nmasterpiece &lt;- c(poetry, more)\nmasterpiece\n\n [1] \"Mary\"     \"had\"      \"a\"        \"little\"   \"chainsaw\" \"whose\"   \n [7] \"chain\"    \"was\"      \"red\"      \"as\"",
    "crumbs": [
      "`R` Helpers",
      "Vectors"
    ]
  },
  {
    "objectID": "explainers/plot.html",
    "href": "explainers/plot.html",
    "title": "Plotting points",
    "section": "",
    "text": "Consider two vectors of the same length:\n\nx &lt;- c(0.3, 0.57, -0.11, -0.26, -0.77, pi, 4, 1, 1, 5)\ny &lt;- c(9, 4, 1, 8, 4, 1, 5, 7, 2, 0)\n\nIf you want to create a scatter plot of the (x, y) pairs, you can use the plot command:\n\nplot(x, y, pch = 19)\n\n\n\n\n\n\n\nIf you wanted, for example, to plot the PMF of the binomial distribution, then your x vector has every value in the range, and your y vector has all of the individual probabilities:\n\nn &lt;- 20\np &lt;- 0.5\nx &lt;- 0:n\npmf &lt;- dbinom(x, n, p)\nplot(x, pmf, pch = 19)\n\n\n\n\n\n\n\nNotice that we made use of the fact that the dbinom command is vectorized in its first argument; if you give it a vector with multiple values, it returns a vector with the probabilities for each one.",
    "crumbs": [
      "`R` Helpers",
      "Plotting points"
    ]
  },
  {
    "objectID": "explainers/lineplot.html",
    "href": "explainers/lineplot.html",
    "title": "Plotting lines",
    "section": "",
    "text": "You can use the curve function to plot a line graph of a function. If that function were \\(f(x) = x^2\\) for example, you would type this:\n\ncurve(x^2, from = -5, to = 5, n = 500, col = \"red\", ylab = \"f(x)\")\n\n\n\n\n\n\n\nThis function takes many arguments.\n\nthe first argument is a formula for the function you want to graph. The curve command implicitly assumes that x is the argument, and everything else is constant;\n\nfrom and to give the bounds of the \\(x\\)-axis;\nThis command is basically evaluating the function at many points and connecting the dots. n specifies how many dots. Too few, and the picture would look all cattywampus:\n\n\ncurve(x^2, from = -5, to = 5, n = 5)\n\n\n\n\n\n\n\nIf you want multiple lines on one graph, you can call curve multiple times, and include the add argument in all but the first call:\n\ncurve(1*x^2, from = -5, to = 5, n = 500, col = \"red\", ylab = \"\")\ncurve(2*x^2, from = -5, to = 5, n = 500, col = \"blue\", ylab = \"\", add = TRUE)\ncurve(3*x^2, from = -5, to = 5, n = 500, col = \"darkgreen\", ylab = \"\", add = TRUE)\nlegend(\"bottomleft\", c(\"a = 1\", \"a = 2\", \"a = 3\"), \n       col = c(\"red\", \"blue\", \"darkgreen\"), lty = 1, bty = \"n\")",
    "crumbs": [
      "`R` Helpers",
      "Plotting lines"
    ]
  },
  {
    "objectID": "explainers/hello-rstudio.html",
    "href": "explainers/hello-rstudio.html",
    "title": "The Lay of the Land",
    "section": "",
    "text": "R is a programming language, like Python, C++, Java, Julia, etc. RStudio is an IDE (integrated development environment) that allows you to interacts with R in a convenient way. So if R is the engine, RStudio is the dashboard.\nWhen you launch RStudio, you see something like this:\n\n\n\n\n\nThere are four panes:\n\n(bottom left) this is the console. It’s a command line where you can type input and get output. At the very least, you can use R as an overgrown calculator;\n(top left) this is the editor. If you are writing a bunch of code you want to save for later (like on your labs), you can edit the text file here;\n(top right) this is the environment. It’s a list of all the data and variables you have loaded;\n(bottom right) this panel is multipurpose. Depending on the tab you select, it can display the file directory, the plot you created, the documentation, or your rendered pdf.",
    "crumbs": [
      "`R` Helpers",
      "The lay of the land"
    ]
  },
  {
    "objectID": "explainers/probabilities.html",
    "href": "explainers/probabilities.html",
    "title": "Computing probabilities",
    "section": "",
    "text": "R provides basic commands that compute probabilities for our special families of distributions:\nEach family has its own commands, but the pattern is the same.",
    "crumbs": [
      "`R` Helpers",
      "Computing probabilities"
    ]
  },
  {
    "objectID": "explainers/probabilities.html#evaluating-the-pmf",
    "href": "explainers/probabilities.html#evaluating-the-pmf",
    "title": "Computing probabilities",
    "section": "Evaluating the PMF",
    "text": "Evaluating the PMF\nTo evaluate the PMF \\(P(X=x)\\), you use a d- command:\n\ndbinom(x, size, prob)\ndgeom(x, prob)\ndpois(x, lambda)\n\nFor example:\n\n# P(X = 0)\ndbinom(0, 5, 0.5)       \n\n[1] 0.03125\n\n# P(X = 1)\ndbinom(1, 5, 0.5)\n\n[1] 0.15625\n\n# P(X = 0 or X = 1)\ndbinom(c(0, 1), 5, 0.5)\n\n[1] 0.03125 0.15625\n\n# Add up every probability\nsum(dbinom(0:5, 5, 0.2))\n\n[1] 1\n\n# P(0 &lt;= Z &lt;- 6)\ndpois(0:6, 5)\n\n[1] 0.006737947 0.033689735 0.084224337 0.140373896 0.175467370 0.175467370\n[7] 0.146222808\n\n\nNotice that these d- commands are vectorized.",
    "crumbs": [
      "`R` Helpers",
      "Computing probabilities"
    ]
  },
  {
    "objectID": "explainers/probabilities.html#evaluating-the-cdf",
    "href": "explainers/probabilities.html#evaluating-the-cdf",
    "title": "Computing probabilities",
    "section": "Evaluating the CDF",
    "text": "Evaluating the CDF\nTo evaluate the CDF \\(F(q) = P(X\\leq q)\\), you use a p- command:\n\npbinom(q, size, prob)\npgeom(q, prob)\nppois(q, lambda)\n\nNotice right away that the p- commands and d- commands are consistent with one another:\n\n# P(X &lt;= 2)\npbinom(2, 5, 0.3)\n\n[1] 0.83692\n\n# P(X = 0) + P(X = 1) + P(X = 2)\nsum(dbinom(0:2, 5, 0.3))\n\n[1] 0.83692",
    "crumbs": [
      "`R` Helpers",
      "Computing probabilities"
    ]
  },
  {
    "objectID": "syllabus/syllabus_team.html",
    "href": "syllabus/syllabus_team.html",
    "title": "Teaching team",
    "section": "",
    "text": "Mug\nName\nRole\nOffice Hours\n\n\n\n\n\nHu, Yuang\nTA\nMon 7:30 PM - 9:30 PM\nremote (Zoom link on Canvas)\n\n\n\nLiu, Aurora\nHead TA\nWed 4:30 PM - 5:30 PM\nOld Chem 203B\nThu 4:30 PM - 5:30 PM\nPerkins LINK 087 (Classroom 3)\n\n\n\nMa, Liane\nTA\nSun 10:00 AM - 12:00 PM\nremote (Zoom link on Canvas)\n\n\n\nZito, John\nInstructor\nTue 3:00 PM - 6:00 PM\nor by appointment\nOld Chem 207",
    "crumbs": [
      "Syllabus",
      "Teaching team"
    ]
  },
  {
    "objectID": "syllabus/syllabus_policies.html",
    "href": "syllabus/syllabus_policies.html",
    "title": "Policies",
    "section": "",
    "text": "You are enthusiastically encouraged to work together and help one another on labs and problem sets. I would much rather you consult your peers than some daft language model. What I ask is that you acknowledge your collaborators. So, at the end of each problem in your write-up, leave a comment like “Ursula, Ignatius and I worked together on this problem,” or “Ethel explained to me how to do part b.” You are not being judged based on your acknowledgements, and there is no penalty for getting “too much” help from others. If you omit acknowledgements, I will assume you did your work solo. If your classmates tell a different story, I will have questions. But otherwise, so long as you are thorough and honest, there will be no problems.\nHaving said all of that, you should not be crassly handing your solutions to others for them to brainlessly copy. This is plagiarism, and all involved will earn a zero on the assignment and be referred to the conduct office, both sharers and recipients alike. The write-up you submit must be your own work.",
    "crumbs": [
      "Syllabus",
      "Policies"
    ]
  },
  {
    "objectID": "syllabus/syllabus_policies.html#collaboration",
    "href": "syllabus/syllabus_policies.html#collaboration",
    "title": "Policies",
    "section": "",
    "text": "You are enthusiastically encouraged to work together and help one another on labs and problem sets. I would much rather you consult your peers than some daft language model. What I ask is that you acknowledge your collaborators. So, at the end of each problem in your write-up, leave a comment like “Ursula, Ignatius and I worked together on this problem,” or “Ethel explained to me how to do part b.” You are not being judged based on your acknowledgements, and there is no penalty for getting “too much” help from others. If you omit acknowledgements, I will assume you did your work solo. If your classmates tell a different story, I will have questions. But otherwise, so long as you are thorough and honest, there will be no problems.\nHaving said all of that, you should not be crassly handing your solutions to others for them to brainlessly copy. This is plagiarism, and all involved will earn a zero on the assignment and be referred to the conduct office, both sharers and recipients alike. The write-up you submit must be your own work.",
    "crumbs": [
      "Syllabus",
      "Policies"
    ]
  },
  {
    "objectID": "syllabus/syllabus_policies.html#use-of-outside-resources-including-ai",
    "href": "syllabus/syllabus_policies.html#use-of-outside-resources-including-ai",
    "title": "Policies",
    "section": "Use of outside resources, including AI",
    "text": "Use of outside resources, including AI\nThere are at least two reasons you might seek outside resources:\n\n✅ Extra practice or alternative instruction: Go crazy. Knock yourself out. Have a ball. The internet is saturated with good (and horrible) resources for learning this material, so if you find something that really resonates with you, have at it;\n❌ Doing the problems for you: If you find a solution online, or ask a language model to generate one, and you copy it down and submit it as your own work, that is plagiarism. If we detect it, you will earn a zero for that part of your write-up.\n\n“Using ChatGPT to complete assignments is like bringing a forklift into the weight room; you will never improve your cognitive fitness that way.” Furthermore, 60% of your final course grade is determined by your performance on old school, no-tech exams. As such, outsourcing all of your thinking to an AI will probably end in humiliating disaster. To avoid this, I suggest you abstain from using language models to do the problems for you.",
    "crumbs": [
      "Syllabus",
      "Policies"
    ]
  },
  {
    "objectID": "syllabus/syllabus_policies.html#communication",
    "href": "syllabus/syllabus_policies.html#communication",
    "title": "Policies",
    "section": "Communication",
    "text": "Communication\nIf you wish to ask content-related questions in writing, please do not do so via e-mail. Instead, please use the course discussion forum Ed Discussion. That way all members of the teaching team can see your question, and all students can benefit from the ensuing discussion. You are also encouraged to answer one another’s questions.\nIf you have questions about personal matters that may not be appropriate for the public course forum (e.g. illness, accommodations, etc), then please e-mail the instructor directly (john.zito@duke.edu).\n\n\n\n\n\n\nNote\n\n\n\nYou can ask questions anonymously on Ed. The teaching team will still know your identity, but your peers will not.",
    "crumbs": [
      "Syllabus",
      "Policies"
    ]
  },
  {
    "objectID": "syllabus/syllabus_policies.html#late-work-and-extensions",
    "href": "syllabus/syllabus_policies.html#late-work-and-extensions",
    "title": "Policies",
    "section": "Late work and extensions",
    "text": "Late work and extensions\nNo late work will be accepted unless you request an extension in advance by e-mailing the instructor directly (john.zito@duke.edu). All reasonable requests will be entertained, but extensions will not be long.",
    "crumbs": [
      "Syllabus",
      "Policies"
    ]
  },
  {
    "objectID": "syllabus/syllabus_policies.html#regrade-requests",
    "href": "syllabus/syllabus_policies.html#regrade-requests",
    "title": "Policies",
    "section": "Regrade requests",
    "text": "Regrade requests\nIf you receive a graded assignment back, and you believe that some part of it was graded incorrectly, you may dispute the grade by submitting a regrade request in Gradescope. JZ is the sole reviewer of these. Note the following:\n\nYou have one week after you receive a grade to submit a regrade request;\nYou should submit separate regrade requests for each question you wish to dispute, not a single catch-all request;\nRequests will be considered if there was an error in the grade calculation or if a correct answer was mistakenly marked as incorrect;\nRequests to dispute the number of points deducted for an incorrect response will not be considered;\nNo grades will be changed after the final exam has been administered on Friday December 12.",
    "crumbs": [
      "Syllabus",
      "Policies"
    ]
  },
  {
    "objectID": "syllabus/syllabus_policies.html#attendance",
    "href": "syllabus/syllabus_policies.html#attendance",
    "title": "Policies",
    "section": "Attendance",
    "text": "Attendance\nLive your life. Attendance is not strictly required for any of the class meetings. The responsibility lies with us to make class meetings sufficiently engaging and informative that you choose to attend. Having said that, success in this class and regular attendance are probably highly positively correlated. Furthermore, while lab attendance is not required, regular attendance is probably the path of least resistance to earning full credit for the lab component of your final grade. The labs are designed so that they can be completed in one sitting more or less, and they are due at midnight the same day. So show up to lab, bang it out, and move on with your life.",
    "crumbs": [
      "Syllabus",
      "Policies"
    ]
  },
  {
    "objectID": "syllabus/syllabus_policies.html#accommodations",
    "href": "syllabus/syllabus_policies.html#accommodations",
    "title": "Policies",
    "section": "Accommodations",
    "text": "Accommodations\nIf you need accommodations for this class, you will need to register with the Student Disability Access Office (SDAO) and provide them with documentation related to your needs. SDAO will work with you to determine what accommodations are appropriate for your situation. Please note that accommodations are not retroactive and disability accommodations cannot be provided until a Faculty Accommodation Letter has been given to me. Please contact SDAO for more information: sdao@duke.edu or access.duke.edu.",
    "crumbs": [
      "Syllabus",
      "Policies"
    ]
  },
  {
    "objectID": "syllabus/syllabus_policies.html#duke-community-standard",
    "href": "syllabus/syllabus_policies.html#duke-community-standard",
    "title": "Policies",
    "section": "Duke Community Standard",
    "text": "Duke Community Standard\nDuke University is a community dedicated to scholarship, leadership, and service and to the principles of honesty, fairness, respect, and accountability. Members of this community commit to reflect upon and uphold these principles in all academic and non-academic endeavors, and to protect and promote a culture of integrity.\nDuke University has high expectations for students’ scholarship and conduct. In accepting admission, students indicate their willingness to subscribe to and be governed by the rules and regulations of the university, which flow from the Duke Community Standard (DCS).\nRegardless of course delivery format, it is the responsibility of all students to understand and follow all Duke policies, including but not limited to the academic integrity policy (e.g., completing one’s own work, following proper citation of sources, adhering to guidance around group work projects, and more). Ignoring these requirements is a violation of the DCS.\nStudents can direct any questions or concerns regarding academic integrity to the Office of Student Conduct and Community Standards at conduct@duke.edu and can access the DCS guide at https://dukecommunitystandard.students.duke.edu/.\nIn STA 240 specifically…\n\nIf a conduct violation results in a zero on a lab or problem set, that zero will not be dropped;\nIf a conduct violation results in a zero on a midterm, that zero will not be replaced with your final exam score;\nIf we discover that students are sharing and copying assignment solutions, all students involved will be penalized equally, the sharers the same as the recipients.",
    "crumbs": [
      "Syllabus",
      "Policies"
    ]
  },
  {
    "objectID": "syllabus/syllabus_materials.html",
    "href": "syllabus/syllabus_materials.html",
    "title": "Course materials",
    "section": "",
    "text": "You are not required to purchase a textbook for this class. However, if you wish to follow along with one, these are all good options:\n\n[Ross] A First Course in Probability by Sheldon Ross;\n[HTZ] Probability and Statistical Inference by Robert Hogg, Elliot Tanis, and Dale Zimmerman;\n[DS] Probability and Statistics by Morris DeGroot and Mark Schervish;\n[Wass] All of Statistics by Larry Wasserman.\n\nIn the PREPARE column of the course schedule, I will indicate which parts of a book correspond to the lectures.",
    "crumbs": [
      "Syllabus",
      "Materials"
    ]
  },
  {
    "objectID": "syllabus/syllabus_materials.html#textbooks",
    "href": "syllabus/syllabus_materials.html#textbooks",
    "title": "Course materials",
    "section": "",
    "text": "You are not required to purchase a textbook for this class. However, if you wish to follow along with one, these are all good options:\n\n[Ross] A First Course in Probability by Sheldon Ross;\n[HTZ] Probability and Statistical Inference by Robert Hogg, Elliot Tanis, and Dale Zimmerman;\n[DS] Probability and Statistics by Morris DeGroot and Mark Schervish;\n[Wass] All of Statistics by Larry Wasserman.\n\nIn the PREPARE column of the course schedule, I will indicate which parts of a book correspond to the lectures.",
    "crumbs": [
      "Syllabus",
      "Materials"
    ]
  },
  {
    "objectID": "syllabus/syllabus_materials.html#technology",
    "href": "syllabus/syllabus_materials.html#technology",
    "title": "Course materials",
    "section": "Technology",
    "text": "Technology\nLecture will largely be a low tech affair: pencil and paper should be sufficient most of the time. You should always plan to bring a laptop or tablet device to lab. In general, you will need access to a device with internet so that you can use the following:\n\nThis course page that you are on right now;\nR/RStudio via the Duke Container Manager;\nCanvas, through which you can access…\n\nGradescope;\nEd Discussion;\n\nZoom (e.g. for remote office hours).\n\nIf access to technology becomes a concern for you during the semester, contact the instructor immediately to discuss options.",
    "crumbs": [
      "Syllabus",
      "Materials"
    ]
  },
  {
    "objectID": "slides/2025-09-03-set2prob.html#office-hours",
    "href": "slides/2025-09-03-set2prob.html#office-hours",
    "title": "Welcome to STA 240!",
    "section": "Office hours",
    "text": "Office hours\n\n\n\n\n\n\n\n\n\nMug\nName\nRole\nOffice Hours\n\n\n\n\n\nHu, Yuang\nTA\nMon 7:30 PM - 9:30 PM\n\n\n\nLiu, Aurora\nHead TA\nWeTh 4:30 pm - 5:30 pm\n\n\n\nMa, Liane\nTA\nSun 10:00 am - 12:00 pm\n\n\n\nZito, John\nInstructor\nTue 3:00 pm - 6:00 pm"
  },
  {
    "objectID": "slides/2025-09-03-set2prob.html#problem-set-0wtf",
    "href": "slides/2025-09-03-set2prob.html#problem-set-0wtf",
    "title": "Welcome to STA 240!",
    "section": "Problem Set 0…wtf?",
    "text": "Problem Set 0…wtf?\n\nLow stakes: worth fewer points than other problem sets;\n\n(but it will be graded quite rigorously);\n\nIt’s about an 8/10 on the math difficulty scale in this class;\nYou will not see much calculus in Weeks 1 - 6 and Midterm 1;\n\n(mainly infinite series);\n\nSerious calculus begins Week 7 and looms large until the end;\n\n(be prepared to do some math on Midterm 2 and the Final).\n\n\n\n\n\n\n\n\n\nBottom line\n\n\nYou have until Week 7 to work the kinks out. If, by that time, you are confident that you understand what’s happening on Problem Set 0, then you are ready."
  },
  {
    "objectID": "slides/2025-09-03-set2prob.html#problem-set-1",
    "href": "slides/2025-09-03-set2prob.html#problem-set-1",
    "title": "Welcome to STA 240!",
    "section": "Problem Set 1",
    "text": "Problem Set 1\n\nProblems 1 - 3 currently accessible to you;\nProblems 4 - 8 accessible after today’s class;\nProblems 9 - 10 accessible after next week’s classes."
  },
  {
    "objectID": "slides/2025-09-03-set2prob.html#problem-sets-1---4",
    "href": "slides/2025-09-03-set2prob.html#problem-sets-1---4",
    "title": "Welcome to STA 240!",
    "section": "Problem Sets 1 - 4",
    "text": "Problem Sets 1 - 4\nEach has 10 problems following a “1 + 2 + 5 + 2” structure:\n\n\n1 problem is chatty and conceptual (no math);\n2 problems practice stuff from 2 weeks ago;\n5 problems practice stuff from last week;\n2 problems practice stuff from this week.\n\n\n\nSo, I lecture on new stuff MoWe, and you have to do two practice problems by that same Friday."
  },
  {
    "objectID": "slides/2025-09-03-set2prob.html#set-operations",
    "href": "slides/2025-09-03-set2prob.html#set-operations",
    "title": "Welcome to STA 240!",
    "section": "Set operations",
    "text": "Set operations\n\n\n\n\n\n\n\n\nSet\nPicture\nLogic\n\n\n\n\n\\(A\\cup B\\)\n\n(inclusive) OR\n\n\n\\(A\\cap B\\)\n\nAND\n\n\n\\(A^c\\)\n\nNOT"
  },
  {
    "objectID": "slides/2025-09-03-set2prob.html#algebraic-properties",
    "href": "slides/2025-09-03-set2prob.html#algebraic-properties",
    "title": "Welcome to STA 240!",
    "section": "Algebraic properties",
    "text": "Algebraic properties\n\\[\n\\begin{matrix}\n    \\text{Commutative} & A\\cup B=B\\cup A\\\\\n    & A\\cap B=B\\cap A\\\\\n    &\\\\\n    \\text{Associative} & (A\\cup B)\\cup C = A\\cup (B\\cup C)\\\\\n    & (A\\cap B)\\cap C = A\\cap (B\\cap C)\\\\\n    &\\\\\n    \\text{Distributive} & (A\\cup B)\\cap C = (A\\cap C)\\cup (B\\cap C)\\\\\n    &(A\\cap B)\\cup C = (A\\cup C)\\cap (B\\cup C)\\\\\n    &\\\\\n    \\text{De Morgan's Laws} & (A\\cup B)^c=A^c\\cap B^c\\\\\n    & (A\\cap B)^c=A^c\\cup B^c.\n\\end{matrix}\n\\]"
  },
  {
    "objectID": "slides/2025-09-03-set2prob.html#random-phenomena",
    "href": "slides/2025-09-03-set2prob.html#random-phenomena",
    "title": "Welcome to STA 240!",
    "section": "Random phenomena",
    "text": "Random phenomena\n\nthe outcome of a coin flip;\nthe outcome of a die roll;\nthe Poker hand dealt to you from a shuffled deck;\nthe outcome of a presidential election;\nwhether or not a basketball player makes a free throw shot;\nwhether or not my soufflé falls in the oven;\nwhether or not the opera singer hits the high note;\nthe next song in your Spotify shuffle;\nthe next word generated by ChatGPT;\nthe birth weight of a newborn child;\nthe number of costumers that will arrive at a store or restaurant on a given day;\nwhen and where a hurricane will make landfall;\nthe time until an unstable particle will decay;\nthe number of claims an insurance company receives in a month;\nthe bid-ask spread of Google stock at 2:37 pm ET next Wednesday.\n\n\nWhy are these things random?"
  },
  {
    "objectID": "slides/2025-09-03-set2prob.html#sample-spaces",
    "href": "slides/2025-09-03-set2prob.html#sample-spaces",
    "title": "Welcome to STA 240!",
    "section": "Sample spaces",
    "text": "Sample spaces\nThe set of possible outcomes of a random phenomenon:\n\n\n\n\n\nPhenomenon\n\n\nSample space \\(S\\)\n\n\n\n\n\n\nFlip two coins in order\n\n\n\\(\\{HH,\\:HT,\\:TT,\\:TH\\}\\)\n\n\n\n\nRoll a single die\n\n\n\\(\\{1,\\:2,\\:3,\\:4,\\:5,\\:6\\}\\)\n\n\n\n\nCard dealt from a shuffled deck\n\n\n\\(\\{2\\clubsuit,\\,3\\clubsuit,\\,4\\clubsuit,\\,\\ldots\\}\\)\n\n\n\n\nWinning party in US election\n\n\n\\(\\{\\text{R}, \\text{D}, \\text{L}, \\text{G}, ..., \\text{DSA}\\}\\)\n\n\n\n\nYour blood sodium level in mEq/L\n\n\n\\(\\mathbb{R}_+=(0,\\infty)\\)\n\n\n\n\n# of insurance claims in a week?\n\n\n\\(\\mathbb{N}\\)\n\n\n\n\nReturn on a risky asset\n\n\n\\(\\mathbb{R}\\)"
  },
  {
    "objectID": "slides/2025-09-03-set2prob.html#events",
    "href": "slides/2025-09-03-set2prob.html#events",
    "title": "Welcome to STA 240!",
    "section": "Events",
    "text": "Events\nA subset \\(A\\subseteq S\\) of the sample space:\n\n\n\n\nDescription\n\n\nEvent \\(A\\)\n\n\n\n\n\n\n“the first of two coin flips is a head”\n\n\n\\(\\{HH,\\:HT\\}\\)\n\n\n\n\n“the die is even”\n\n\n\\(\\{2,\\:4,\\:6\\}\\)\n\n\n\n\n“dealt a four”\n\n\n\\(\\{4\\clubsuit,\\,4\\heartsuit,\\,4\\spadesuit,\\,4\\diamondsuit\\}\\)\n\n\n\n\n“right-wing party wins”\n\n\n\\(\\{\\text{R},\\,\\text{L},\\,...\\}\\)\n\n\n\n\n“blood sodium in healthy range”\n\n\n\\([133,\\:145]\\)\n\n\n\n\n“over a thousand claims”\n\n\n\\(\\{1001,\\,1002,\\,1003,\\,\\ldots\\}\\)\n\n\n\n\n“your investment loses money”\n\n\n\\((-\\infty,\\:0)\\)"
  },
  {
    "objectID": "slides/2025-09-03-set2prob.html#running-example-where-will-the-meteor-land",
    "href": "slides/2025-09-03-set2prob.html#running-example-where-will-the-meteor-land",
    "title": "Welcome to STA 240!",
    "section": "Running example: where will the meteor land?",
    "text": "Running example: where will the meteor land?\nThe sample space is the set of all (long, lat) coordinates in this spatial region:"
  },
  {
    "objectID": "slides/2025-09-03-set2prob.html#example-events",
    "href": "slides/2025-09-03-set2prob.html#example-events",
    "title": "Welcome to STA 240!",
    "section": "Example events",
    "text": "Example events\n\n\n\nSymbol\nDescription\n\n\n\n\n\\(A\\)\n“Meteor lands in the United States”\n\n\n\\(B\\)\n“Meteor lands in Canada”\n\n\n\\(C\\)\n“Meteor lands in Mexico”\n\n\n\\(D\\)\n“Meteor lands in adjacent waters”\n\n\n\\(E\\)\n“Meteor lands in the Rocky Mountains”"
  },
  {
    "objectID": "slides/2025-09-03-set2prob.html#or-events-union",
    "href": "slides/2025-09-03-set2prob.html#or-events-union",
    "title": "Welcome to STA 240!",
    "section": "“OR” events (union)",
    "text": "“OR” events (union)\n\n\n\nSymbol\nDescription\n\n\n\n\n\\(A\\)\n“Meteor lands in the United States”\n\n\n\\(B\\)\n“Meteor lands in Canada”\n\n\n\\(C\\)\n“Meteor lands in Mexico”\n\n\n\\(D\\)\n“Meteor lands in adjacent waters”\n\n\n\\(E\\)\n“Meteor lands in the Rocky Mountains”\n\n\n\n\n“Meteor lands in the US or Canada”\n→ \\(A \\cup B\\)\n“Meteor lands in Mexico or adjacent waters”\n→ \\(C \\cup D\\)\n“Meteor lands in the Rocky Mountains or Canada”\n→ \\(E \\cup B\\)\n“Meteor lands in North America”\n→ \\(A\\cup B\\cup C\\)"
  },
  {
    "objectID": "slides/2025-09-03-set2prob.html#and-events-intersection",
    "href": "slides/2025-09-03-set2prob.html#and-events-intersection",
    "title": "Welcome to STA 240!",
    "section": "“AND” events (intersection)",
    "text": "“AND” events (intersection)\n\n\n\nSymbol\nDescription\n\n\n\n\n\\(A\\)\n“Meteor lands in the United States”\n\n\n\\(B\\)\n“Meteor lands in Canada”\n\n\n\\(C\\)\n“Meteor lands in Mexico”\n\n\n\\(D\\)\n“Meteor lands in adjacent waters”\n\n\n\\(E\\)\n“Meteor lands in the Rocky Mountains”\n\n\n\n\n“Meteor lands in the Rocky Mountains and the US”\n→ \\(E \\cap A\\)\n“Meteor lands in the US and Mexico”\n→ \\(A \\cap C = \\varnothing\\)\n“Meteor lands in North America and adjacent waters”\n→ \\((A \\cup B \\cup C) \\cap D = \\varnothing\\)"
  },
  {
    "objectID": "slides/2025-09-03-set2prob.html#not-events-complement",
    "href": "slides/2025-09-03-set2prob.html#not-events-complement",
    "title": "Welcome to STA 240!",
    "section": "“NOT” events (complement)",
    "text": "“NOT” events (complement)\n\n\n\nSymbol\nDescription\n\n\n\n\n\\(A\\)\n“Meteor lands in the United States”\n\n\n\\(B\\)\n“Meteor lands in Canada”\n\n\n\\(C\\)\n“Meteor lands in Mexico”\n\n\n\\(D\\)\n“Meteor lands in adjacent waters”\n\n\n\\(E\\)\n“Meteor lands in the Rocky Mountains”\n\n\n\n\n“Meteor does not land in the US”\n→ \\(A^c\\)\n“Meteor does not land in the Rocky Mountains”\n→ \\(E^c\\)\n“Meteor does not land on land at all”\n→ \\((A \\cup B \\cup C)^c = D\\)"
  },
  {
    "objectID": "slides/2025-09-03-set2prob.html#implication",
    "href": "slides/2025-09-03-set2prob.html#implication",
    "title": "Welcome to STA 240!",
    "section": "Implication",
    "text": "Implication\n\n\n\nSymbol\nDescription\n\n\n\n\n\\(G\\)\n“Meteor lands in Georgia”\n\n\n\\(S\\)\n“Meteor lands in The South”\n\n\n\\(U\\)\n“Meteor lands in the United States”\n\n\n\\(N\\)\n“Meteor lands in North America”\n\n\n\n\nNotice:\n\n\n\\[\nG\\subseteq S\\subseteq U\\subseteq N.\n\\]\n\nIf we learn that the meteor landed in The South, that implies that it landed in the USA, which implies that it landed in North America;\nIt may or may not have landed in Georgia."
  },
  {
    "objectID": "slides/2025-09-03-set2prob.html#summary",
    "href": "slides/2025-09-03-set2prob.html#summary",
    "title": "Welcome to STA 240!",
    "section": "Summary",
    "text": "Summary\n\n\n\nProbability\nSet theory\n\n\n\n\n\\(A\\) or \\(B\\) occur\n\\(A\\cup B\\)\n\n\n\\(A\\) and \\(B\\) occur\n\\(A\\cap B\\)\n\n\n\\(A\\) does not occur\n\\(A^c\\)\n\n\n\\(A\\) implies \\(B\\)\n\\(A\\subseteq B\\)\n\n\n\\(A\\) and \\(B\\) mutually exclusive\n\\(A\\cap B=\\varnothing\\)"
  },
  {
    "objectID": "slides/user_2025_08_09.html#about-me",
    "href": "slides/user_2025_08_09.html#about-me",
    "title": "Rhapsody in R\n",
    "section": "About me",
    "text": "About me\n\n\n\n\n\n\nIt’s my sixth go-around teaching intro probability;\nI’m a tad weary of coins, dice, playing cards, etc."
  },
  {
    "objectID": "slides/user_2025_08_09.html#trying-to-mix-it-up",
    "href": "slides/user_2025_08_09.html#trying-to-mix-it-up",
    "title": "Rhapsody in R\n",
    "section": "Trying to mix it up",
    "text": "Trying to mix it up\n\nPlenty of applications from the natural and social sciences:\n\n\n\n\n\nActuarial mathematics;\nContested elections;\nDating apps;\nDNA sequencing;\nExpert witness testimony;\nExtreme weather;\n\n\n\n\nI Ching divination;\nInvestment risk and return;\nLanguage models;\nPapal conclaves;\nPrediction markets;\nQuantum mechanics.\n\n\n\n\nBut what about the arts?"
  },
  {
    "objectID": "slides/user_2025_08_09.html#stochastic-music",
    "href": "slides/user_2025_08_09.html#stochastic-music",
    "title": "Rhapsody in R\n",
    "section": "Stochastic music",
    "text": "Stochastic music\n\n\nAKA: chance or aleatoric music.\n\nleaving some aspect of the composition up in the air until the moment of performance;\n⭐ simulating a random process to determine what notes to write down."
  },
  {
    "objectID": "slides/user_2025_08_09.html#iannis-xenakis-1922---2001",
    "href": "slides/user_2025_08_09.html#iannis-xenakis-1922---2001",
    "title": "Rhapsody in R\n",
    "section": "Iannis Xenakis (1922 - 2001)",
    "text": "Iannis Xenakis (1922 - 2001)\n\n\n\n\n\nStudied both traditional Western music and CS, statistical mechanics, stochastic processes, etc;\nIncorporated these ideas into his compositional process."
  },
  {
    "objectID": "slides/user_2025_08_09.html#pithoprakta-1956",
    "href": "slides/user_2025_08_09.html#pithoprakta-1956",
    "title": "Rhapsody in R\n",
    "section": "\nPithoprakta (1956)",
    "text": "Pithoprakta (1956)\n\nThink of each member of a 46-piece string orchestra as a Brownian particle drifting up and down the staff:\n\n\n\n\n\n\n\n\n“[E]very member of the orchestra is on the honor system.” Leonard Bernstein"
  },
  {
    "objectID": "slides/user_2025_08_09.html#pithoprakta-1956-1",
    "href": "slides/user_2025_08_09.html#pithoprakta-1956-1",
    "title": "Rhapsody in R\n",
    "section": "\nPithoprakta (1956)",
    "text": "Pithoprakta (1956)\n\n\n\nWell …it’s the thought that counts."
  },
  {
    "objectID": "slides/user_2025_08_09.html#main-idea",
    "href": "slides/user_2025_08_09.html#main-idea",
    "title": "Rhapsody in R\n",
    "section": "Main idea",
    "text": "Main idea\n\n\nThought: Can I prompt students to use what they know about probability distributions and simulation to write their own pieces of stochastic music?\nWorry: Depends. Can you work with music in R?"
  },
  {
    "objectID": "slides/user_2025_08_09.html#renfei-maos-gm-package",
    "href": "slides/user_2025_08_09.html#renfei-maos-gm-package",
    "title": "Rhapsody in R\n",
    "section": "Renfei Mao’s gm package",
    "text": "Renfei Mao’s gm package\n\n\n\n\n“grammar of music”\n\nRepresent music in R with a ggplot2-style interface;\n\n“generate music”\n\nintegration with MuseScore generates sheet music and MIDI playback."
  },
  {
    "objectID": "slides/user_2025_08_09.html#ravel-prélude-in-a-minor-m.-65",
    "href": "slides/user_2025_08_09.html#ravel-prélude-in-a-minor-m.-65",
    "title": "Rhapsody in R\n",
    "section": "Ravel: Prélude in A Minor, M. 65",
    "text": "Ravel: Prélude in A Minor, M. 65\n\n\nLet’s transcribe it!"
  },
  {
    "objectID": "slides/user_2025_08_09.html#the-starting-point-is-always-the-same",
    "href": "slides/user_2025_08_09.html#the-starting-point-is-always-the-same",
    "title": "Rhapsody in R\n",
    "section": "The starting point is always the same",
    "text": "The starting point is always the same\nWe will add layers to this:\n\nlibrary(gm)\n\nprelude &lt;- Music()\n\n\nAnalogous to this:\n\nlibrary(ggplot2)\n\nmyplot &lt;- ggplot()"
  },
  {
    "objectID": "slides/user_2025_08_09.html#add-meter-and-tempo",
    "href": "slides/user_2025_08_09.html#add-meter-and-tempo",
    "title": "Rhapsody in R\n",
    "section": "Add meter and tempo",
    "text": "Add meter and tempo"
  },
  {
    "objectID": "slides/user_2025_08_09.html#add-meter-and-tempo-1",
    "href": "slides/user_2025_08_09.html#add-meter-and-tempo-1",
    "title": "Rhapsody in R\n",
    "section": "Add meter and tempo",
    "text": "Add meter and tempo\n\nprelude &lt;- Music() + \n  Meter(3, 4) + \n  Tempo(60)"
  },
  {
    "objectID": "slides/user_2025_08_09.html#add-the-right-hand-part",
    "href": "slides/user_2025_08_09.html#add-the-right-hand-part",
    "title": "Rhapsody in R\n",
    "section": "Add the right hand part",
    "text": "Add the right hand part"
  },
  {
    "objectID": "slides/user_2025_08_09.html#brief-aside-scientific-pitch-notation",
    "href": "slides/user_2025_08_09.html#brief-aside-scientific-pitch-notation",
    "title": "Rhapsody in R\n",
    "section": "Brief aside: scientific pitch notation",
    "text": "Brief aside: scientific pitch notation\n\"F#5\" means the F♯ in the fifth octave on the piano, etc."
  },
  {
    "objectID": "slides/user_2025_08_09.html#add-the-right-hand-part-1",
    "href": "slides/user_2025_08_09.html#add-the-right-hand-part-1",
    "title": "Rhapsody in R\n",
    "section": "Add the right hand part",
    "text": "Add the right hand part\n\nright_hand &lt;- Line(\n  pitches = list(NA, \"E5\", \"F#5\", \"D5\", \"E5\", \"F#5\",\n                 \"B5\", \"G5\", \"D5\", \n                 \"E5\", \"G4\",\n                 c(\"D4\", \"F#4\", \"B4\"), c(\"C#4\", \"F4\", \"C5\"),\n                 c(\"G3\", \"C4\", \"E4\"), c(\"D#4\", \"G4\"), c(\"C4\", \"E4\"))\n)\n\nprelude &lt;- Music() + \n  Meter(3, 4) + \n  Tempo(60) +\n  right_hand"
  },
  {
    "objectID": "slides/user_2025_08_09.html#add-the-right-hand-part-2",
    "href": "slides/user_2025_08_09.html#add-the-right-hand-part-2",
    "title": "Rhapsody in R\n",
    "section": "Add the right hand part",
    "text": "Add the right hand part\n\ngm::show(prelude)\n\n\n\n\n\n\n\n\n\n\n(BTW: this plays nice with Quarto off-the-shelf.)"
  },
  {
    "objectID": "slides/user_2025_08_09.html#adjust-the-note-values",
    "href": "slides/user_2025_08_09.html#adjust-the-note-values",
    "title": "Rhapsody in R\n",
    "section": "Adjust the note values",
    "text": "Adjust the note values\n\nright_hand &lt;- Line(\n  pitches = list(NA, \"E5\", \"F#5\", \"D5\", \"E5\", \"F#5\",\n                 \"B5\", \"G5\", \"D5\", \n                 \"E5\", \"G4\",\n                 c(\"D4\", \"F#4\", \"B4\"), c(\"C#4\", \"F4\", \"C5\"),\n                 c(\"G3\", \"C4\", \"E4\"), c(\"D#4\", \"G4\"), c(\"C4\", \"E4\")),\n  durations = c(0.5, 0.5, 0.5, 0.5, 0.5, 0.5, \n                1, 1, 2, \n                1, 1, \n                2, 1, \n                1.5, 0.5, 1)\n)\n\nprelude &lt;- Music() + \n  Meter(3, 4) + \n  Tempo(60) +\n  right_hand"
  },
  {
    "objectID": "slides/user_2025_08_09.html#adjust-the-note-values-1",
    "href": "slides/user_2025_08_09.html#adjust-the-note-values-1",
    "title": "Rhapsody in R\n",
    "section": "Adjust the note values",
    "text": "Adjust the note values\n\ngm::show(prelude)"
  },
  {
    "objectID": "slides/user_2025_08_09.html#add-the-left-hand-part",
    "href": "slides/user_2025_08_09.html#add-the-left-hand-part",
    "title": "Rhapsody in R\n",
    "section": "Add the left hand part",
    "text": "Add the left hand part"
  },
  {
    "objectID": "slides/user_2025_08_09.html#add-the-left-hand-part-1",
    "href": "slides/user_2025_08_09.html#add-the-left-hand-part-1",
    "title": "Rhapsody in R\n",
    "section": "Add the left hand part",
    "text": "Add the left hand part\n\nleft_hand &lt;- Line(\n  pitches = c(\"A2\", \"E3\", \"G3\", \"C4\", \"E4\", \n              \"G4\", \"C5\", NA, \n              NA, \"E2\", \"E3\", \"F#3\", \"G#3\",\n              NA, \"A2\", \"E3\", \"B3\", \"G3\"),\n  durations = c(rep(0.5, 7), 2, \n                rep(0.5, 4), 1, \n                rep(0.5, 4), 1),\n  bar = 2, offset = 0.5\n)\n\nprelude &lt;- Music() + \n  Meter(3, 4) + \n  Tempo(60) +\n  right_hand + \n  left_hand + \n  Clef(\"F\")\n\nNote: rests correspond to missing values (NA) in the line."
  },
  {
    "objectID": "slides/user_2025_08_09.html#add-the-left-hand-part-2",
    "href": "slides/user_2025_08_09.html#add-the-left-hand-part-2",
    "title": "Rhapsody in R\n",
    "section": "Add the left hand part",
    "text": "Add the left hand part\n\ngm::show(prelude)"
  },
  {
    "objectID": "slides/user_2025_08_09.html#add-dynamics",
    "href": "slides/user_2025_08_09.html#add-dynamics",
    "title": "Rhapsody in R\n",
    "section": "Add dynamics",
    "text": "Add dynamics"
  },
  {
    "objectID": "slides/user_2025_08_09.html#add-dynamics-1",
    "href": "slides/user_2025_08_09.html#add-dynamics-1",
    "title": "Rhapsody in R\n",
    "section": "Add dynamics",
    "text": "Add dynamics\n\nprelude &lt;- Music() + \n  Meter(3, 4) + \n  Tempo(60) +\n  right_hand + \n  Dynamic(\"p\", 1) + \n  left_hand + \n  Clef(\"F\") +\n  Dynamic(\"p\", 1)"
  },
  {
    "objectID": "slides/user_2025_08_09.html#add-expressive-indications",
    "href": "slides/user_2025_08_09.html#add-expressive-indications",
    "title": "Rhapsody in R\n",
    "section": "Add expressive indications",
    "text": "Add expressive indications"
  },
  {
    "objectID": "slides/user_2025_08_09.html#add-expressive-indications-1",
    "href": "slides/user_2025_08_09.html#add-expressive-indications-1",
    "title": "Rhapsody in R\n",
    "section": "Add expressive indications",
    "text": "Add expressive indications\n\nprelude &lt;- Music() + \n  Meter(3, 4) + \n  Tempo(60) +\n  right_hand + \n  Dynamic(\"p\", 1) + \n  Slur(2, 11) + \n  Slur(12, 16) + \n  left_hand + \n  Clef(\"F\") +\n  Dynamic(\"p\", 1) + \n  Slur(1, 7) + \n  Slur(10, 13) + \n  Slur(15, 18) + \n  Pedal(1, 7)"
  },
  {
    "objectID": "slides/user_2025_08_09.html#pretty-close",
    "href": "slides/user_2025_08_09.html#pretty-close",
    "title": "Rhapsody in R\n",
    "section": "Pretty close!",
    "text": "Pretty close!\n\ngm::show(prelude)"
  },
  {
    "objectID": "slides/user_2025_08_09.html#change-the-instrumentation-optional",
    "href": "slides/user_2025_08_09.html#change-the-instrumentation-optional",
    "title": "Rhapsody in R\n",
    "section": "Change the instrumentation (optional)",
    "text": "Change the instrumentation (optional)\n\nprelude &lt;- Music() + \n  Meter(3, 4) + \n  Tempo(65) + \n  right_hand + \n  Dynamic(\"p\", 1) + \n  Slur(2, 11) + \n  Slur(12, 16) + \n  Instrument(47) + # Harp!\n  left_hand + \n  Clef(\"F\") +\n  Dynamic(\"p\", 1) + \n  Slur(1, 7) + \n  Slur(10, 13) + \n  Slur(15, 18) + \n  Instrument(43) # Cello!"
  },
  {
    "objectID": "slides/user_2025_08_09.html#change-the-instrumentation-optional-1",
    "href": "slides/user_2025_08_09.html#change-the-instrumentation-optional-1",
    "title": "Rhapsody in R\n",
    "section": "Change the instrumentation (optional)",
    "text": "Change the instrumentation (optional)\n\n?Instrument\n\n\n\n\nAcoustic Grand Piano\nBright Acoustic Piano\nElectric Grand Piano\nHonky-Tonk Piano\nElectric Piano 1\nElectric Piano 2\nHarpsichord\nClavinet\n\n\n\nCelesta\nGlockenspiel\nMusic Box\nVibraphone\nMarimba\nXylophone\nTubular Bells\n\n… and so on"
  },
  {
    "objectID": "slides/user_2025_08_09.html#change-the-instrumentation-optional-2",
    "href": "slides/user_2025_08_09.html#change-the-instrumentation-optional-2",
    "title": "Rhapsody in R\n",
    "section": "Change the instrumentation (optional)",
    "text": "Change the instrumentation (optional)\n\ngm::show(prelude)"
  },
  {
    "objectID": "slides/user_2025_08_09.html#summary-the-gm-package",
    "href": "slides/user_2025_08_09.html#summary-the-gm-package",
    "title": "Rhapsody in R\n",
    "section": "Summary: the gm package",
    "text": "Summary: the gm package\nA ggplot2-style interface for music:\n\n\n\nprelude &lt;- Music() + \n  Meter(3, 4) + \n  Tempo(65) + \n  right_hand + \n  Dynamic(\"p\", 1) + \n  Slur(2, 11) + \n  Slur(12, 16) + \n  Instrument(47) +\n  left_hand + \n  Clef(\"F\") +\n  Dynamic(\"p\", 1) + \n  Slur(1, 7) + \n  Slur(10, 13) + \n  Slur(15, 18) + \n  Instrument(43) \n\n\n\ngm::show(prelude)\n\n\n\n\n\n\n\n\n\nThanks Renfei!\n\n\nNow, let’s write some crazy music!"
  },
  {
    "objectID": "slides/user_2025_08_09.html#every-pitch-on-the-piano",
    "href": "slides/user_2025_08_09.html#every-pitch-on-the-piano",
    "title": "Rhapsody in R\n",
    "section": "Every pitch on the piano",
    "text": "Every pitch on the piano\n\npitches &lt;- c(\"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \"F#\", \"G\", \"G#\", \"A\", \"A#\", \"B\")\noctaves &lt;- 1:7\nall_pitches &lt;- c(\"A0\", \"A#0\", \"B0\", \n                 paste(rep(pitches, length(octaves)), \n                       sort(rep(octaves, length(pitches))), \n                       sep = \"\"), \n                 \"C8\")\nall_pitches\n\n [1] \"A0\"  \"A#0\" \"B0\"  \"C1\"  \"C#1\" \"D1\"  \"D#1\" \"E1\"  \"F1\"  \"F#1\" \"G1\"  \"G#1\"\n[13] \"A1\"  \"A#1\" \"B1\"  \"C2\"  \"C#2\" \"D2\"  \"D#2\" \"E2\"  \"F2\"  \"F#2\" \"G2\"  \"G#2\"\n[25] \"A2\"  \"A#2\" \"B2\"  \"C3\"  \"C#3\" \"D3\"  \"D#3\" \"E3\"  \"F3\"  \"F#3\" \"G3\"  \"G#3\"\n[37] \"A3\"  \"A#3\" \"B3\"  \"C4\"  \"C#4\" \"D4\"  \"D#4\" \"E4\"  \"F4\"  \"F#4\" \"G4\"  \"G#4\"\n[49] \"A4\"  \"A#4\" \"B4\"  \"C5\"  \"C#5\" \"D5\"  \"D#5\" \"E5\"  \"F5\"  \"F#5\" \"G5\"  \"G#5\"\n[61] \"A5\"  \"A#5\" \"B5\"  \"C6\"  \"C#6\" \"D6\"  \"D#6\" \"E6\"  \"F6\"  \"F#6\" \"G6\"  \"G#6\"\n[73] \"A6\"  \"A#6\" \"B6\"  \"C7\"  \"C#7\" \"D7\"  \"D#7\" \"E7\"  \"F7\"  \"F#7\" \"G7\"  \"G#7\"\n[85] \"A7\"  \"A#7\" \"B7\"  \"C8\""
  },
  {
    "objectID": "slides/user_2025_08_09.html#the-simplest-piece-of-stochastic-music",
    "href": "slides/user_2025_08_09.html#the-simplest-piece-of-stochastic-music",
    "title": "Rhapsody in R\n",
    "section": "The simplest piece of stochastic music",
    "text": "The simplest piece of stochastic music\nSample the pitches with replacement:\n\nset.seed(8675309)\n\nline1 &lt;- Line(\n  pitches = sample(all_pitches, 64, replace = TRUE),\n  durations = .25\n)\n\nline2 &lt;- Line(\n  pitches = sample(all_pitches, 64, replace = TRUE),\n  durations = .25\n)\n\nkitten &lt;- Music() + \n  Meter(4, 4) + \n  Tempo(120) + \n  line1 + \n  line2 + \n  Dynamic(\"p\", 1) +\n  Dynamic(\"ffff\", 64) +\n  Hairpin(\"&lt;\", 2, 63)"
  },
  {
    "objectID": "slides/user_2025_08_09.html#angry-kitten-on-the-keys",
    "href": "slides/user_2025_08_09.html#angry-kitten-on-the-keys",
    "title": "Rhapsody in R\n",
    "section": "(Angry) Kitten on the keys",
    "text": "(Angry) Kitten on the keys"
  },
  {
    "objectID": "slides/user_2025_08_09.html#what-is-a-melody-wrong-answers-only",
    "href": "slides/user_2025_08_09.html#what-is-a-melody-wrong-answers-only",
    "title": "Rhapsody in R\n",
    "section": "What is a melody? (wrong answers only)",
    "text": "What is a melody? (wrong answers only)\n\nIt’s a time series!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGame:\n\nFix a key (C minor) and a stable accompaniment (waltz);\nSimulate a melody from a Markov chain."
  },
  {
    "objectID": "slides/user_2025_08_09.html#unmarkovchained-melodies",
    "href": "slides/user_2025_08_09.html#unmarkovchained-melodies",
    "title": "Rhapsody in R\n",
    "section": "Un(Markov)chained melodies",
    "text": "Un(Markov)chained melodies\nFrom Nierhaus’ Algorithmic Composition (2009 Springer):"
  },
  {
    "objectID": "slides/user_2025_08_09.html#unmarkovchained-melodies-1",
    "href": "slides/user_2025_08_09.html#unmarkovchained-melodies-1",
    "title": "Rhapsody in R\n",
    "section": "Un(Markov)chained melodies",
    "text": "Un(Markov)chained melodies\nA first-order Markov chain on the C minor scale is described by its transition probabilities: given the note we’re on right now, what are the probabilities for the next note to come?\n\nLoads of options to explore:\n\n\n\nuniform (sampling with replacement, again);\nupweight leaps into “chord tones;”\nupweight leaps between chord tones (arpeggio-like);\nupweight leaps into the tonic (“resolutions”);\nrandom walk (flip a coin every beat and move \\(\\pm1\\))."
  },
  {
    "objectID": "slides/user_2025_08_09.html#iid-melody",
    "href": "slides/user_2025_08_09.html#iid-melody",
    "title": "Rhapsody in R\n",
    "section": "IID Melody",
    "text": "IID Melody"
  },
  {
    "objectID": "slides/user_2025_08_09.html#random-walk-melody",
    "href": "slides/user_2025_08_09.html#random-walk-melody",
    "title": "Rhapsody in R\n",
    "section": "Random walk melody",
    "text": "Random walk melody"
  },
  {
    "objectID": "slides/user_2025_08_09.html#favors-leaps-between-chord-tones",
    "href": "slides/user_2025_08_09.html#favors-leaps-between-chord-tones",
    "title": "Rhapsody in R\n",
    "section": "Favors leaps between chord tones",
    "text": "Favors leaps between chord tones"
  },
  {
    "objectID": "slides/user_2025_08_09.html#the-jacobson-method",
    "href": "slides/user_2025_08_09.html#the-jacobson-method",
    "title": "Rhapsody in R\n",
    "section": "The Jacobson method",
    "text": "The Jacobson method\n\n\n\n\nTake an existing piece of music and “add noise.”\nSo this:\n\\[\n\\mathbf{y}=f(\\mathbf{x})+\\boldsymbol{\\varepsilon},\n\\]\nonly…it’s music?"
  },
  {
    "objectID": "slides/user_2025_08_09.html#name-that-tune",
    "href": "slides/user_2025_08_09.html#name-that-tune",
    "title": "Rhapsody in R\n",
    "section": "Name that tune",
    "text": "Name that tune"
  },
  {
    "objectID": "slides/user_2025_08_09.html#the-students-mission",
    "href": "slides/user_2025_08_09.html#the-students-mission",
    "title": "Rhapsody in R\n",
    "section": "The students’ mission",
    "text": "The students’ mission\n\nWrite your own piece of stochastic music;\nExplore the relationship between the probability rules governing the system and the sound of the resulting music;\nKey words:\n\nPlay;\nSurprise yourself."
  },
  {
    "objectID": "slides/user_2025_08_09.html#randomizing-the-rhythm",
    "href": "slides/user_2025_08_09.html#randomizing-the-rhythm",
    "title": "Rhapsody in R\n",
    "section": "Randomizing the rhythm",
    "text": "Randomizing the rhythm"
  },
  {
    "objectID": "slides/user_2025_08_09.html#i-just-like-how-this-looks",
    "href": "slides/user_2025_08_09.html#i-just-like-how-this-looks",
    "title": "Rhapsody in R\n",
    "section": "I just like how this looks",
    "text": "I just like how this looks"
  },
  {
    "objectID": "slides/user_2025_08_09.html#playing-with-texture",
    "href": "slides/user_2025_08_09.html#playing-with-texture",
    "title": "Rhapsody in R\n",
    "section": "Playing with texture",
    "text": "Playing with texture\n(John, please don’t forget to lower the volume on this.)"
  },
  {
    "objectID": "slides/user_2025_08_09.html#they-called-it-nightmare-ballet",
    "href": "slides/user_2025_08_09.html#they-called-it-nightmare-ballet",
    "title": "Rhapsody in R\n",
    "section": "They called it “Nightmare Ballet”",
    "text": "They called it “Nightmare Ballet”"
  },
  {
    "objectID": "slides/user_2025_08_09.html#just-trolling-me",
    "href": "slides/user_2025_08_09.html#just-trolling-me",
    "title": "Rhapsody in R\n",
    "section": "Just trolling me",
    "text": "Just trolling me"
  },
  {
    "objectID": "slides/user_2025_08_09.html#not-quite-random-and-yet",
    "href": "slides/user_2025_08_09.html#not-quite-random-and-yet",
    "title": "Rhapsody in R\n",
    "section": "Not quite random, and yet…",
    "text": "Not quite random, and yet…"
  },
  {
    "objectID": "slides/user_2025_08_09.html#future-improvements",
    "href": "slides/user_2025_08_09.html#future-improvements",
    "title": "Rhapsody in R\n",
    "section": "Future improvements",
    "text": "Future improvements\n\nCan gm handle any Hz, and not just the discrete pitches of the Western system?\nHow closely can I replicate Xenakis’ methods?\nPlay with Markov melodies based on estimated transition probabilities from famous composers;\nRandomize other features: harmony, rhythm, articulation, orchestration, dynamics, etc;\nStudents need more time and better coaching/inspiration;\nHow can I make it sillier?"
  },
  {
    "objectID": "slides/user_2025_08_09.html#quick-takeaways",
    "href": "slides/user_2025_08_09.html#quick-takeaways",
    "title": "Rhapsody in R\n",
    "section": "Quick takeaways",
    "text": "Quick takeaways\n\nIntro probability is a mixed audience. I aim for maximum variety in the examples and applications;\nIf you as an instructor have any creative or artistic interests, mix them in. You will have a tremendous amount of fun;\nActivities like this totally tilt the material on its ear;\nThe gm package is so much fun to play with!"
  },
  {
    "objectID": "math/logexp.html",
    "href": "math/logexp.html",
    "title": "Rules for exponents and logarithms",
    "section": "",
    "text": "Let \\(a\\), \\(b\\), \\(c\\), and \\(d\\) be arbitrary real numbers, and let \\(n\\) be a positive integer:\n\\[\n\\begin{aligned}\na^0&=1\\\\\na^1&=a\\\\\na^ca^d&=a^{c+d}\\\\\n\\frac{a^c}{a^d}&=a^{c - d}\\\\\na^{-c}&=\\frac{1}{a^c}\\\\\n(a^c)^d&=a^{cd}\\\\\n(ab)^c&=a^cb^c\\\\\n\\left(\\frac{a}{b}\\right)^c&=\\frac{a^c}{b^c}\\\\\na^{1/n}&=\\sqrt[n]{a}.\n\\end{aligned}\n\\]\nA particularly important function for us is the exponential function \\(e^x\\), where \\(e\\approx 2.71828\\) is Euler’s number. Because it is defined as an exponent, the exponential function obeys all the rules above: \\(e^xe^y=e^{x+y}\\), \\(e^0=1\\), etc.\nHere’s the graph of the function:\n\nCodepar(mar = c(4, 4, 4, 0.5))\ncurve(exp(x), \n      from = -3, \n      to = 3, \n      n = 500, \n      col = \"red\", \n      lwd = 2,\n      yaxt = \"n\", \n      yaxs = \"i\",\n      bty = \"n\",\n      ylab = expression(e^x),\n      main = \"Graph of the exponential function\",\n      ylim = c(0, 20))\naxis(2, pos = 0)\n\n\n\n\n\n\n\nA few things to notice:\n\nthe exponential is strictly increasing (never plateaus or goes down);\nit increases very quickly;\nit is defined for all real numbers \\(x\\);\nit is strictly positive: \\(e^x&gt;0\\) for all \\(x\\in\\mathbb{R}\\).\n\n\n\n\n\n\n\nAlternative notation\n\n\n\nSometimes we will write \\(\\exp(x)\\) instead of \\(e^x\\), but they mean the same thing. We do this to make complicated expressions like this a little easier to read: \\[\ne^{-\\frac{1}{2}\\frac{(x-\\mu)^2}{\\sigma^2}}=\\exp\\left(-\\frac{1}{2}\\frac{(x-\\mu)^2}{\\sigma^2}\\right).\n\\]",
    "crumbs": [
      "Math Helpers",
      "Exponents and logarithms"
    ]
  },
  {
    "objectID": "math/logexp.html#exponent-rules",
    "href": "math/logexp.html#exponent-rules",
    "title": "Rules for exponents and logarithms",
    "section": "",
    "text": "Let \\(a\\), \\(b\\), \\(c\\), and \\(d\\) be arbitrary real numbers, and let \\(n\\) be a positive integer:\n\\[\n\\begin{aligned}\na^0&=1\\\\\na^1&=a\\\\\na^ca^d&=a^{c+d}\\\\\n\\frac{a^c}{a^d}&=a^{c - d}\\\\\na^{-c}&=\\frac{1}{a^c}\\\\\n(a^c)^d&=a^{cd}\\\\\n(ab)^c&=a^cb^c\\\\\n\\left(\\frac{a}{b}\\right)^c&=\\frac{a^c}{b^c}\\\\\na^{1/n}&=\\sqrt[n]{a}.\n\\end{aligned}\n\\]\nA particularly important function for us is the exponential function \\(e^x\\), where \\(e\\approx 2.71828\\) is Euler’s number. Because it is defined as an exponent, the exponential function obeys all the rules above: \\(e^xe^y=e^{x+y}\\), \\(e^0=1\\), etc.\nHere’s the graph of the function:\n\nCodepar(mar = c(4, 4, 4, 0.5))\ncurve(exp(x), \n      from = -3, \n      to = 3, \n      n = 500, \n      col = \"red\", \n      lwd = 2,\n      yaxt = \"n\", \n      yaxs = \"i\",\n      bty = \"n\",\n      ylab = expression(e^x),\n      main = \"Graph of the exponential function\",\n      ylim = c(0, 20))\naxis(2, pos = 0)\n\n\n\n\n\n\n\nA few things to notice:\n\nthe exponential is strictly increasing (never plateaus or goes down);\nit increases very quickly;\nit is defined for all real numbers \\(x\\);\nit is strictly positive: \\(e^x&gt;0\\) for all \\(x\\in\\mathbb{R}\\).\n\n\n\n\n\n\n\nAlternative notation\n\n\n\nSometimes we will write \\(\\exp(x)\\) instead of \\(e^x\\), but they mean the same thing. We do this to make complicated expressions like this a little easier to read: \\[\ne^{-\\frac{1}{2}\\frac{(x-\\mu)^2}{\\sigma^2}}=\\exp\\left(-\\frac{1}{2}\\frac{(x-\\mu)^2}{\\sigma^2}\\right).\n\\]",
    "crumbs": [
      "Math Helpers",
      "Exponents and logarithms"
    ]
  },
  {
    "objectID": "math/logexp.html#logarithm-rules",
    "href": "math/logexp.html#logarithm-rules",
    "title": "Rules for exponents and logarithms",
    "section": "Logarithm rules",
    "text": "Logarithm rules\nThe natural logarithm function \\(\\ln(x)\\) is the inverse of the exponential function \\(e^x\\). So when we write \\(\\ln(4)\\), we’re asking “\\(e\\) raised to what power is equal to 4?” Turns out it’s \\(\\ln(4)\\approx 1.386\\), so \\(e^{1.386}\\approx 4\\).\nHere’s the graph of the function:\n\nCodepar(mar = c(4, 4, 1, 0.5))\ncurve(log(x), \n      from = 0, \n      to = 8, \n      n = 500, \n      col = \"red\", \n      lwd = 2,\n      xaxt = \"n\", \n      xaxs = \"i\",\n      bty = \"n\",\n      ylab = \"ln(x)\",\n      main = \"Graph of the natural logarithm\")\naxis(1, pos = 0)\n\n\n\n\n\n\n\nA few things to notice:\n\nthe natural log is strictly increasing (never plateaus or goes down);\nit increases veeery slowly;\nit is only defined for strictly positive numbers;\nit returns both positive and negative values;\nthere is a vertical asymptote at \\(x=0\\).\n\nSo, word to the wise: if you find yourself plugging negative numbers into the natural log, you’ve probably made a mistake someplace.\nAnyhow, the natural logarithm has several properties you need to know. Let \\(x\\) and \\(y\\) be arbitrary positive numbers, and let \\(c\\) be any real number (could be negative):\n\\[\n\\begin{aligned}\n\\ln(xy)&=\\ln(x)+\\ln(y)\\\\\n\\ln\\left(\\frac{x}{y}\\right)&=\\ln(x)-\\ln(y)\\\\\n\\ln(x^c)&=c\\ln(x)\\\\\n\\ln(1/x)&=-\\ln(x)\\\\\n\\ln(e^x)&=x\\\\\ne^{\\ln x}&=x\\\\\n\\ln(e)&=1\\\\\n\\ln(1)&=0.\n\\end{aligned}\n\\]",
    "crumbs": [
      "Math Helpers",
      "Exponents and logarithms"
    ]
  },
  {
    "objectID": "math/differentiation.html",
    "href": "math/differentiation.html",
    "title": "Differentiation",
    "section": "",
    "text": "The derivative of a function \\(f(x)\\) is its instantaneous rate of change. On a graph, this corresponds to the slope of the tangent line. We will denote derivatives by \\(f'(x)\\) or \\(\\text{d}f/\\text{d}x\\) or something like that.\nWhen it exists, which it doesn’t always, the derivative is defined as the limit of difference quotients:\n\\[\nf'(x)=\\lim_{\\Delta x\\to0}\\frac{f(x+\\Delta x)-f(x)}{\\Delta x}.\n\\]\nSo, “rise over run” as the “run” bit gets arbitrarily small.\nAs such, the derivative is the continuous analog to a discrete difference.",
    "crumbs": [
      "Math Helpers",
      "Differentiation"
    ]
  },
  {
    "objectID": "math/differentiation.html#derivative-rules",
    "href": "math/differentiation.html#derivative-rules",
    "title": "Differentiation",
    "section": "Derivative rules",
    "text": "Derivative rules\n\n\n\n\n\n\n\nrule\nfunction\nderivative\n\n\n\nconstant\n\\(h(x)=c\\)\n\\(h'(x)=0\\)\n\n\nscaling\n\\(h(x)=c\\cdot f(x)\\)\n\\(h'(x)=c\\cdot f'(x)\\)\n\n\nsum/difference\n\\(h(x)=f(x)\\pm g(x)\\)\n\\(h'(x)=f'(x)\\pm g'(x)\\)\n\n\nlinearity\n\\(h(x)=a\\cdot f(x)\\pm b\\cdot g(x)\\)\n\\(h'(x)=a\\cdot f'(x)\\pm b\\cdot g'(x)\\)\n\n\npower\n\\(h(x)=x^n\\)\n\\(h'(x)=nx^{n-1}\\)\n\n\nproduct\n\\(h(x)=f(x)g(x)\\)\n\\(h'(x)=f'(x)g(x)+f(x)g'(x)\\)\n\n\nquotient\n\\(h(x)=f(x)/g(x)\\)\n\\(h'(x)=\\frac{f'(x)g(x)+f(x)g'(x)}{g(x)^2}\\)\n\n\nchain\n\\(h(x)=f(g(x))\\)\n\\(h'(x)=f'(g(x))g'(x)\\)\n\n\nexponential\n\\(h(x)=e^x\\)\n\\(h'(x)=e^x\\)\n\n\nnatural log\n\\(h(x)=\\ln(x)\\)\n\\(h'(x)=1/x\\)",
    "crumbs": [
      "Math Helpers",
      "Differentiation"
    ]
  },
  {
    "objectID": "math/differentiation.html#curvature",
    "href": "math/differentiation.html#curvature",
    "title": "Differentiation",
    "section": "Curvature",
    "text": "Curvature\nIf you take the derivative of \\(f\\), you get the first derivative \\(f'\\). If you take the derivative of the derivative, you get the second derivative \\(f''\\). The derivative of the derivative of the derivative is the third derivative \\(f'''\\), and on and on. But let’s focus on the first two. The derivatives contain information about the shape of the original function:\n\nthe first derivative tells you if the function is increasing or decreasing;\n\nif \\(f'(x)&gt;0\\), then \\(f\\) is increasing at \\(x\\);\nif \\(f'(x)&lt;0\\), then \\(f\\) is decreasing at \\(x\\);\nif \\(f'(x)=0\\), then \\(f\\) is not changing at \\(x\\);\n\n\nif the second derivative tells you about the curvature (aka concavity) of the function:\n\nif \\(f''(x)&gt;0\\), then \\(f\\) is concave up (\\(\\cup\\), smiling) at \\(x\\);\nif \\(f''(x)&lt;0\\), then \\(f\\) is concave down (\\(\\cap\\), frowning) at \\(x\\).\n\n\n\nHere is an illustration with the function \\(f(x)=x^3-x\\) and its derivatives \\(f'(x)=3x^2-1\\) and \\(f''(x)=6x\\) (power rule!):\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\nWhen the first derivative changes sign (at \\(x^\\star=\\pm1/\\sqrt{3}\\)), that corresponds to the original function switching from increasing to decreasing, or vice versa. When the second derivative changes sign (at \\(x_0=0\\)), that corresponds to the original function switching from concave down (frowning) to concave up (smiling). A point like \\(x=0\\) in this example where the concavity changes is called an inflection point. If \\(x_0\\) is an inflection point, then that means it’s also a root of the second derivative: \\(f''(x_0)=0\\). In order for the second derivative to change sign, it has to cross the \\(x\\)-axis.\nHowever, just because the second derivative is zero doesn’t mean there was inflection. Consider \\(f(x)=x^4\\) and its derivatives \\(f'(x)=4x^3\\) and \\(f''(x)=12x^2\\):\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\nIn this case, we see that while the second derivative has a root at zero, it doesn’t actually cross the \\(x\\)-axis. It’s just swooping down and giving it a gentle lil’ smooch from above. Correspondingly, we see that the original function never changes concavity. Therefore, we say that \\(f''=0\\) is a necessary but not sufficient condition for the original \\(f\\) to change concavity. To verify, you must actually check that the second derivative has different sign before and after the root. Bummer!",
    "crumbs": [
      "Math Helpers",
      "Differentiation"
    ]
  },
  {
    "objectID": "math/differentiation.html#optimization",
    "href": "math/differentiation.html#optimization",
    "title": "Differentiation",
    "section": "Optimization",
    "text": "Optimization\nDifferentiation is useful when we seek to optimize a function. That is, find where it has local maxima or minima. For purposes of illustration, I’ll focus on maxima. Formally, given an objective function \\(f\\), we seek to find the \\(x\\)-values at which \\(f\\) has a maximum:\n\\[\n\\underset{x\\in\\mathbb{R}}{\\arg\\max}\\,f(x).\n\\]\nNotice that I did not write \\(\\max f(x)\\). In this class, it turns out that we will seldom care about the actual largest value that the function obtains. Instead, we care only for where that value occurs. So we are interested in the maximizer; the location of the maximum; the \\(x\\)-value that does the maximizing. Hence, argmax and not max. Here’s an illustration:\n\n\n\n\n\n\n\n\nInspecting this picture, we notice that the maximum corresponds to a point where the derivative (slope of the tangent line) is zero. In order for the function to “top out,” it must at some point have switched from being increasing (positive derivative) to decreasing (negative derivative). When we ask where the maximum occurs, that’s the same as asking where the first derivative switched sign (ie crossed the \\(x\\)-axis). So to optimize \\(f\\), perhaps we can compute its first derivative, set it equal to 0, and solve.\n\\[\n\\begin{aligned}\nf'(x)\n=\n-2x+6&=0\n\\quad\n\\implies\n\\quad\nx^\\star=3.\n\\end{aligned}\n\\]\nA point like \\(x^\\star\\) in this example is called a critical point. A critical point may be the location of a maximum, but you have to check. If you recall the example in Figure 1, there are two critical points \\(x^\\star = \\pm 1/\\sqrt{3}\\), but they correspond to different things: one is a minimum and one is a maximum. In order to classify the critical points as maximizers or minimizers, you perform the second derivative test:\n\nif \\(f'(x^\\star)=0\\) and \\(f''(x^\\star)&lt;0\\), then \\(x^\\star\\) is a maximizer;\nif \\(f'(x^\\star)=0\\) and \\(f''(x^\\star)&gt;0\\), then \\(x^\\star\\) is a minimizer;\nif \\(f'(x^\\star)=0\\) and \\(f''(x^\\star)=0\\), then the test is inconclusive.",
    "crumbs": [
      "Math Helpers",
      "Differentiation"
    ]
  },
  {
    "objectID": "math/greeks.html",
    "href": "math/greeks.html",
    "title": "Greek alphabet",
    "section": "",
    "text": "For better or worse, Greek letters will be flying in this class, so you should have a working knowledge of the most common ones used in mathematics:\n\n\n\n\nGreek Letter\nEnglish Name\n\n\n\n\n\\(A, \\alpha\\)\nalpha\n\n\n\\(B, \\beta\\)\nbeta\n\n\n\\(\\Gamma, \\gamma\\)\ngamma\n\n\n\\(\\Delta, \\delta\\)\ndelta\n\n\n\\(E, \\epsilon\\)\nepsilon\n\n\n\\(Z, \\zeta\\)\nzeta\n\n\n\\(H, \\eta\\)\neta\n\n\n\\(\\Theta, \\theta\\)\ntheta\n\n\n\\(I, \\iota\\)\niota\n\n\n\\(K, \\kappa\\)\nkappa\n\n\n\\(\\Lambda, \\lambda\\)\nlambda\n\n\n\\(M, \\mu\\)\nmu (“mew”)\n\n\n\\(N, \\nu\\)\nnu\n\n\n\\(\\Xi, \\xi\\)\nxi\n\n\n\\(O, o\\)\nomicron\n\n\n\\(\\Pi, \\pi\\)\npi\n\n\n\\(P, \\rho\\)\nrho (“row”)\n\n\n\\(\\Sigma, \\sigma\\)\nsigma\n\n\n\\(T, \\tau\\)\ntau\n\n\n\\(Y, \\upsilon\\)\nupsilon\n\n\n\\(\\Phi, \\phi\\)\nphi\n\n\n\\(X, \\chi\\)\nchi (“kai”)\n\n\n\\(\\Psi, \\psi\\)\npsi\n\n\n\\(\\Omega, \\omega\\)\nomega",
    "crumbs": [
      "Math Helpers",
      "Greek alphabet"
    ]
  },
  {
    "objectID": "math/massage-and-squint.html",
    "href": "math/massage-and-squint.html",
    "title": "“Massage and squint”",
    "section": "",
    "text": "In this class, we have to simplify many sums, infinite series, and integrals (eg when computing expected values). Sometimes, we have to do this using the formal techniques we learned in Calc II, but other times, the sum or integral is just a familiar object in disguise. If we can recognize that, we can avoid doing any hardcore math.\nHere’s the principle:\n\nTake your sum or integral and rewrite it (“massage” it) until you recognize (after “squinting” at it) that it is equivalent to a familiar object. Then plug in what you know.\n\nLet’s unpack that:\n\nmassaging: u-substitution if it’s an integral, re-indexing if it’s a sum, multiply by a cleverly written 1, add a cleverly written 0, complete the square, etc;\nfamiliar object: geometric series, Taylor series of a friendly function (probably \\(e^x\\)), binomial theorem, gamma function, kernel of special density like the Gaussian, etc.\n\n\n\n\n\n\n\nPay attention!\n\n\n\nIf you are going to be a stats major/minor, I know for an absolute fact that you will be doing this stuff early and often in STA 332 and STA 402, so take the practice seriously.\n\n\n\nKernel tricks\n\nProbability density functions must integrate to 1.\n\nLike “energy is conserved” or “demand curves slope down,” the above statement is true and simple, but surprisingly powerful.\nAs we have seen several times, all densities have the following format:\n\\[\nf(x;\\,\\boldsymbol{\\theta})=\\underbrace{c(\\boldsymbol{\\theta})}_{\\text{normalizing constant}}\\underbrace{k(x;\\,\\boldsymbol{\\theta})}_{\\text{kernel}}.\n\\]\nHere, \\(\\boldsymbol{\\theta}\\) is generic notation for the parameters of the distribution. So \\(\\boldsymbol{\\theta}=(\\mu,\\, \\sigma^2)\\) for the normal, or \\(\\boldsymbol{\\theta}=(\\alpha,\\,\\beta)\\) for the gamma. \\(k(x;\\,\\boldsymbol{\\theta})\\) is the density kernel or unnormalized density. It’s the part of the density formula that actually depends on the argument \\(x\\). \\(c(\\boldsymbol{\\theta})\\) is the normalizing constant that hangs out in the front and serves just to make sure that the whole thing integrates to 1. Since\n\\[\n\\int_{-\\infty}^\\infty f(x;\\,\\boldsymbol{\\theta})\\,\\text{d} x\n=\n\\int_{-\\infty}^\\infty c(\\boldsymbol{\\theta})k(x;\\,\\boldsymbol{\\theta})\\,\\text{d} x\n=\nc(\\boldsymbol{\\theta})\\int_{-\\infty}^\\infty k(x;\\,\\boldsymbol{\\theta})\\,\\text{d} x=1,\n\\]\nthen\n\\[\n\\int_{-\\infty}^\\infty k(x;\\,\\boldsymbol{\\theta})\\,\\text{d} x=\\frac{1}{c(\\boldsymbol{\\theta})},\n\\]\nSo, the integral of the kernel is the inverse of the normalizing constant. Based on this simple fact, every probability density function you encounter gives you a free integral identity that you can use in subsequent computations.\n\n\n\n\n\n\nExample: Gaussian integral\n\n\n\nFor the Gaussian density:\n\\[\n\\begin{align*}\n    c(\\boldsymbol{\\theta})\n    &=\n    \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\n    \\\\\n    k(x;\\,\\boldsymbol{\\theta})\n    &=\n    \\exp\\left(-\\frac{1}{2}\\frac{(x-\\mu)^2}{\\sigma^2}\\right)\n    .\n\\end{align*}\n\\]\nSo for any values of \\(\\mu\\in\\mathbb{R}\\) and \\(\\sigma&gt;0\\), we get\n\\[\n\\int_{-\\infty}^\\infty\\exp\\left(-\\frac{1}{2}\\frac{(x-\\mu)^2}{\\sigma^2}\\right)\\text{d} x=\\sqrt{2\\pi\\sigma^2}.\n\\]\nUse this fact to make your life easier!\n\n\n\n\n\n\n\n\nExample: gamma integral\n\n\n\nFor the gamma density:\n\\[\n\\begin{align*}\n    c(\\boldsymbol{\\theta})\n    &=\n    \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\n    \\\\\n    k(x;\\,\\boldsymbol{\\theta})\n    &=\n    x^{\\alpha-1}\n    e^{-\\beta x}\n\\end{align*}\n\\]\nSo for any values of \\(\\alpha,\\,\\beta&gt;0\\), we get\n\\[\n\\int_0^\\infty x^{\\alpha-1}e^{-\\beta x}\\,\\text{d} x=\\frac{\\Gamma(\\alpha)}{\\beta^\\alpha}\n\\]\nUse this fact to make your life easier!\n\n\n\n\nExamples\n“Massage and squint” is just a grand exercise in pattern recognition, so here is a running list of examples from the course. Study these carefully:\n\nPoisson mean from PSET 0 #3;\nPoisson MGF from lecture;\nStandard normal MGF from lecture;\nGamma MGF from lecture;\nPSET 5 #4: \\(\\Gamma(1/2)=\\sqrt{\\pi}\\);\nPSET 5 #6c;\nPSET 6 #7;\nPoisson-binomial hierarchy from lecture;\nGamma-normal hierarchy from lecture;\nPSET 6 #9: the beta-gamma hierarchy;\nPSET 6 #10: gamma-gamma hierarchy.",
    "crumbs": [
      "Math Helpers",
      "Massage and squint"
    ]
  },
  {
    "objectID": "qa/exam-1-q-and-a.html",
    "href": "qa/exam-1-q-and-a.html",
    "title": "Midterm 1 Q&A",
    "section": "",
    "text": "Answer\n\n\n\n\n\nReindexing a sum is analogous to applying a \\(u\\)-substitution to an integral, so a lot of your intuitions about that carry over. But to be concrete, let me draw your attention to some examples from the course where a reindex happened:\n\nProblem Set 0 #2;\nProb\n\nIn each case, the form of the original expression tips you off about\nDo whatever you have to do to muscle it into that form."
  },
  {
    "objectID": "qa/exam-1-q-and-a.html#when-simplifying-a-sum-how-do-you-know-when-to-reindex",
    "href": "qa/exam-1-q-and-a.html#when-simplifying-a-sum-how-do-you-know-when-to-reindex",
    "title": "Midterm 1 Q&A",
    "section": "",
    "text": "Answer\n\n\n\n\n\nReindexing a sum is analogous to applying a \\(u\\)-substitution to an integral, so a lot of your intuitions about that carry over. But to be concrete, let me draw your attention to some examples from the course where a reindex happened:\n\nProblem Set 0 #2;\nProb\n\nIn each case, the form of the original expression tips you off about\nDo whatever you have to do to muscle it into that form."
  },
  {
    "objectID": "qa/exam-1-q-and-a.html#how-do-you-start-proofs",
    "href": "qa/exam-1-q-and-a.html#how-do-you-start-proofs",
    "title": "Midterm 1 Q&A",
    "section": "2.1 How do you start proofs?",
    "text": "2.1 How do you start proofs?\n\n\n\n\n\n\nAnswer"
  },
  {
    "objectID": "qa/exam-1-q-and-a.html#how-do-you-tell-if-order-matters-or-not",
    "href": "qa/exam-1-q-and-a.html#how-do-you-tell-if-order-matters-or-not",
    "title": "Midterm 1 Q&A",
    "section": "3.1 How do you tell if order matters or not?",
    "text": "3.1 How do you tell if order matters or not?\n\n\n\n\n\n\nAnswer"
  },
  {
    "objectID": "qa/exam-1-q-and-a.html#under-what-circumstances-do-the-ordered-and-the-unordered-answers-agree",
    "href": "qa/exam-1-q-and-a.html#under-what-circumstances-do-the-ordered-and-the-unordered-answers-agree",
    "title": "Midterm 1 Q&A",
    "section": "3.2 Under what circumstances do the “ordered” and the “unordered” answers agree?",
    "text": "3.2 Under what circumstances do the “ordered” and the “unordered” answers agree?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe saw this on the Spotify."
  },
  {
    "objectID": "qa/exam-1-q-and-a.html#i-am-confused-about-stars-and-bars.",
    "href": "qa/exam-1-q-and-a.html#i-am-confused-about-stars-and-bars.",
    "title": "Midterm 1 Q&A",
    "section": "3.3 I am confused about “stars and bars.”",
    "text": "3.3 I am confused about “stars and bars.”\n\n\n\n\n\n\nAnswer"
  },
  {
    "objectID": "problems/exam-midterm-2.html",
    "href": "problems/exam-midterm-2.html",
    "title": "Midterm 2 Study Guide",
    "section": "",
    "text": "Midterm Exam 2 will take place on Thursday November 13 during your lab section. There will be 6 problems, and they will break down as follows:\nBelow are practice problems for each of these areas, as well as guidance about what course materials to refer to if you want more review on a particular topic.",
    "crumbs": [
      "Exam Study Guides",
      "Midterm 2"
    ]
  },
  {
    "objectID": "problems/exam-midterm-2.html#problem-1",
    "href": "problems/exam-midterm-2.html#problem-1",
    "title": "Midterm 2 Study Guide",
    "section": "Problem 1",
    "text": "Problem 1\nLet \\(A_1\\), \\(A_2\\), \\(A_3\\), \\(A_4\\), \\(A_5\\), and \\(A_6\\) be events in some sample space, each with the same probability \\(0&lt;p&lt;1/6\\). Define the indicator random variables\n\\[\nI_k\n=\n\\begin{cases}\n0 & \\text{if }A_k^c\\text{ happens}\\\\\n1 & \\text{if }A_k\\text{ happens},\n\\end{cases}\n\\]\nand let \\(X\\) be their sum\n\\[\nX = I_1 + I_2 + I_3 + I_4+I_5+I_6.\n\\]\n\nAssume the events \\(A_i\\) are mutually disjoint and compute…\n\n\\(E(X)\\).\n\\(P(X=1)\\).\n\\(P(X=3)\\).\n\nAssume the \\(A_i\\) are independent and compute…\n\n\\(E(X)\\).\n\\(P(X=1)\\).\n\\(P(X=3)\\).",
    "crumbs": [
      "Exam Study Guides",
      "Midterm 2"
    ]
  },
  {
    "objectID": "problems/exam-midterm-2.html#problem-2",
    "href": "problems/exam-midterm-2.html#problem-2",
    "title": "Midterm 2 Study Guide",
    "section": "Problem 2",
    "text": "Problem 2\nLet \\(X\\sim\\text{Poisson}(\\lambda)\\) and define a new random variable \\(Y\\) that is a zero-truncated version of \\(X\\). That is, we start with \\(\\text{Range}(X)=\\{0,\\,1,\\,2,\\,3,\\,...\\}\\), and then we rig it so that \\(\\text{Range}(Y)=\\{1,\\,2,\\,3,\\,...\\}\\). To do this, we define the PMF of the new random variable \\(Y\\) by\n\\[\nP(Y=k)\n=\nP(X=k\\mid X &gt; 0),\\quad k=1,\\,2,\\,3,\\,4,\\,...\n\\]\n\nShow that the new PMF sums to one;\nCompute the mean of \\(Y\\);\nCompute the variance of \\(Y\\).",
    "crumbs": [
      "Exam Study Guides",
      "Midterm 2"
    ]
  },
  {
    "objectID": "problems/exam-midterm-2.html#want-more-review",
    "href": "problems/exam-midterm-2.html#want-more-review",
    "title": "Midterm 2 Study Guide",
    "section": "Want more review?",
    "text": "Want more review?\n\nLecture notes introducing random variables;\nLecture notes on special families of discrete random variables;\nLecture notes on expected value;\nLab 5 exercises;\nProblem Set 2: problem 7;\nProblem Set 3: problems 9 - 10;\nProblem Set 4: problems 4 - 10.",
    "crumbs": [
      "Exam Study Guides",
      "Midterm 2"
    ]
  },
  {
    "objectID": "problems/exam-midterm-2.html#problem-3",
    "href": "problems/exam-midterm-2.html#problem-3",
    "title": "Midterm 2 Study Guide",
    "section": "Problem 3",
    "text": "Problem 3\nConsider a nonnegative, absolutely continuous random variable with cdf\n\\[\nF_X(x)\n=\n\\begin{cases}\n1 - \\exp\\left(-\\frac{x^2}{2}\\right) & x\\geq 0\n\\\\\n0 & x&lt;0.\n\\end{cases}\n\\]\n\nWhat is \\(P(2\\leq X\\leq 3)\\);\nWhat is the density of \\(X\\)?\nWhat is \\(E(X^n)\\) for any \\(n\\in\\mathbb{N}\\)?\nWhat is \\(\\text{var}(X)\\)?\nWhat is the median of \\(X\\)?",
    "crumbs": [
      "Exam Study Guides",
      "Midterm 2"
    ]
  },
  {
    "objectID": "problems/exam-midterm-2.html#problem-4",
    "href": "problems/exam-midterm-2.html#problem-4",
    "title": "Midterm 2 Study Guide",
    "section": "Problem 4",
    "text": "Problem 4\nHere is the pdf of some absolutely continuous random variable:\n\\[\nf(x)=\\begin{cases}\nax^2e^{-bx^2} & x\\geq 0\\\\\n0 & x&lt; 0.\n\\end{cases}\n\\]\nSolve for \\(a\\) in terms of \\(b\\). So your final answer will look like\n\\[\na=\\text{``some formula including $b$ and a bunch of constants''}\n\\]",
    "crumbs": [
      "Exam Study Guides",
      "Midterm 2"
    ]
  },
  {
    "objectID": "problems/exam-midterm-2.html#want-more-review-1",
    "href": "problems/exam-midterm-2.html#want-more-review-1",
    "title": "Midterm 2 Study Guide",
    "section": "Want more review?",
    "text": "Want more review?\n\nExamples from lecture;\nLab 8;\nProblem Set 5: problems 2 - 8;\nProblem Set 6: problems 1 - 2;\nOdd-numbered exercises in DeGroot & Schervish Chapters 3.2, 5.6, 5.7.",
    "crumbs": [
      "Exam Study Guides",
      "Midterm 2"
    ]
  },
  {
    "objectID": "problems/exam-midterm-2.html#problem-5",
    "href": "problems/exam-midterm-2.html#problem-5",
    "title": "Midterm 2 Study Guide",
    "section": "Problem 5",
    "text": "Problem 5\nLet \\(X\\) have the discrete uniform on \\(\\{1,\\,2,\\,...,\\,n\\}\\). This means \\(P(X=i)=1/n\\) for all \\(i = 1,\\,2,\\,...,\\,n\\).\n\nCompute the MGF of \\(X\\);\nUse it to compute the mean;\nUse it to compute the variance.",
    "crumbs": [
      "Exam Study Guides",
      "Midterm 2"
    ]
  },
  {
    "objectID": "problems/exam-midterm-2.html#problem-6",
    "href": "problems/exam-midterm-2.html#problem-6",
    "title": "Midterm 2 Study Guide",
    "section": "Problem 6",
    "text": "Problem 6\nConsider a continuous random variable \\(X\\) with density\n\\[\nf(x)=\\frac{1}{\\beta}\\exp\\left(-\\frac{x}{\\beta}-e^{-x/\\beta}\\right),\\quad x\\in\\mathbb{R}.\n\\]\nThe parameter \\(\\beta&gt;0\\) is just a positive constant. Compute the moment-generating function of \\(X\\).",
    "crumbs": [
      "Exam Study Guides",
      "Midterm 2"
    ]
  },
  {
    "objectID": "problems/exam-midterm-2.html#want-more-review-2",
    "href": "problems/exam-midterm-2.html#want-more-review-2",
    "title": "Midterm 2 Study Guide",
    "section": "Want more review?",
    "text": "Want more review?\n\nworked examples from lecture;\nnew theorems and examples from lecture about updating MGFs under linear transformations;\nProblem Set 5: problems 9 - 10;\nProblem Set 6: problems 3 - 5.\nOdd-numbered exercises in DeGroot & Schervish Chapter 4.4.",
    "crumbs": [
      "Exam Study Guides",
      "Midterm 2"
    ]
  },
  {
    "objectID": "problems/exam-midterm-2.html#problem-7",
    "href": "problems/exam-midterm-2.html#problem-7",
    "title": "Midterm 2 Study Guide",
    "section": "Problem 7",
    "text": "Problem 7\nThis is an important technical skill, so let’s drill it. For each of these, plot the transformation and then derive the range and the density of the new distribution:\n\nLet \\(X\\sim\\text{Unif}(0,\\,1)\\) and consider \\(Y=1/X\\);\nLet \\(X\\) have density \\(f_X(x)=42x^5(1-x)\\) for \\(0&lt;x&lt;1\\) and consider \\(Y=X^3\\);\nLet \\(X\\sim\\text{Exponential}(\\lambda)\\) and consider \\(Y=4X+3\\);\nLet \\(X\\) have cdf \\(F_X(x)=(x+1)^3/8\\) for \\(-1&lt;x&lt;1\\) and consider \\(Y=1-X^2\\);\nLet \\(X\\sim\\text{N}(0,\\,1)\\) and consider \\(Y=|X|\\).",
    "crumbs": [
      "Exam Study Guides",
      "Midterm 2"
    ]
  },
  {
    "objectID": "problems/exam-midterm-2.html#want-more-review-3",
    "href": "problems/exam-midterm-2.html#want-more-review-3",
    "title": "Midterm 2 Study Guide",
    "section": "Want more review?",
    "text": "Want more review?\n\nLab 7;\nProblem Set 6: problem 6;\nRead DeGroot & Schervish Chapter 3.8 and do the odd-numbered exercises.",
    "crumbs": [
      "Exam Study Guides",
      "Midterm 2"
    ]
  },
  {
    "objectID": "problems/exam-midterm-2.html#problem-8",
    "href": "problems/exam-midterm-2.html#problem-8",
    "title": "Midterm 2 Study Guide",
    "section": "Problem 8",
    "text": "Problem 8\nHere is the joint density for a random pair:\n\\[\nf_{XY}(x,\\,y)\n=\n\\frac{24}{7}\nx(2-y),\\quad 0&lt;x&lt;1\\text{ and }0&lt;y&lt;1-x.\n\\]\n\nSketch the joint range of \\((X,\\,Y)\\).\nFind the marginal density of \\(Y\\).\nFind the marginal density of \\(X\\).\nFind the conditional density of \\(X\\) given \\(Y=y\\).\nUse this to compute \\(P(X&gt;1/2\\,|\\,Y=1/3)\\).\nWhat is \\(E(X\\,|\\,Y=y)\\)?",
    "crumbs": [
      "Exam Study Guides",
      "Midterm 2"
    ]
  },
  {
    "objectID": "problems/exam-midterm-2.html#problem-9",
    "href": "problems/exam-midterm-2.html#problem-9",
    "title": "Midterm 2 Study Guide",
    "section": "Problem 9",
    "text": "Problem 9\nConsider this joint distribution for random variables \\(X\\) and \\(Y\\), written in hierarchical form:\n\\[\n\\begin{aligned}\nX&\\sim\\text{Gamma}\\left(\\alpha,\\,\\beta\\right)\\\\\nY\\mid X=x&\\sim \\text{Exponential}(x).\n\\end{aligned}\n\\]\n\nSketch the joint range of \\((X,\\, Y)\\).\nWhat is the joint density of the random pair \\((X,\\, Y)\\)?\nWhat is the marginal density of \\(Y\\)?\nWhat is the density of \\(X\\) conditional on the event \\(Y=y\\)? Is it recognizable?",
    "crumbs": [
      "Exam Study Guides",
      "Midterm 2"
    ]
  },
  {
    "objectID": "problems/exam-midterm-2.html#want-more-review-4",
    "href": "problems/exam-midterm-2.html#want-more-review-4",
    "title": "Midterm 2 Study Guide",
    "section": "Want more review?",
    "text": "Want more review?\n\njointly discrete examples from lecture;\njointly contnuous examples from lecture;\nmixed Poisson-gamma example from Lab 9;\nProblem Set 6: problems 7 - 10;\nOdd-numbered exercises in DeGroot & Schervish Chapters 3.4 - 3.6.",
    "crumbs": [
      "Exam Study Guides",
      "Midterm 2"
    ]
  },
  {
    "objectID": "problems/pset-2.html",
    "href": "problems/pset-2.html",
    "title": "Problem Set 2",
    "section": "",
    "text": "As with Problem Set 1, this problem set asks you to prove new results about how probability measures behave. You are being evaluated on your reasoning and the fluency with which you make use of set theory, the probability axioms, and the various rules we’ve seen (complement, inclusion/exclusion, etc). Please be thorough and explain all of the steps in your logic.\nAs I mentioned here, the lecture notes on set theory and the probability rules provide a model for how the proofs should look. If you emulate the structure, level of detail, and amount of discussion featured in those examples, you should be good.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 2"
    ]
  },
  {
    "objectID": "problems/pset-2.html#before-you-start",
    "href": "problems/pset-2.html#before-you-start",
    "title": "Problem Set 2",
    "section": "",
    "text": "As with Problem Set 1, this problem set asks you to prove new results about how probability measures behave. You are being evaluated on your reasoning and the fluency with which you make use of set theory, the probability axioms, and the various rules we’ve seen (complement, inclusion/exclusion, etc). Please be thorough and explain all of the steps in your logic.\nAs I mentioned here, the lecture notes on set theory and the probability rules provide a model for how the proofs should look. If you emulate the structure, level of detail, and amount of discussion featured in those examples, you should be good.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 2"
    ]
  },
  {
    "objectID": "problems/pset-2.html#problem-0",
    "href": "problems/pset-2.html#problem-0",
    "title": "Problem Set 2",
    "section": "Problem 0",
    "text": "Problem 0\nRecommend some music for us to listen to while we grade this.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 2"
    ]
  },
  {
    "objectID": "problems/pset-2.html#problem-1",
    "href": "problems/pset-2.html#problem-1",
    "title": "Problem Set 2",
    "section": "Problem 1",
    "text": "Problem 1\nHere is a list of events that may or may not happen, but I believe we will know the final outcome before the end of the semester:\n\nZohran Mamdani wins the New York City mayoral election;\n\nWicked: For Good makes more money at the box office (inflation adjusted) on its opening weekend than Wicked: Part I;\nThe Philadelphia Phillies make it to the 2025 World Series;\nSean Combs is sentenced to prison for more than 3 years;\nA greater percentage of Duke undergraduates participate in this year’s Duke Marriage Pact than last year’s. About 43% participated last year;\nThe total number of points scored at the Countdown to Craziness exhibition game is greater than 50;\nBefore the end of the year, the Federal Reserve announces that Lisa Cook is no longer a member of the Board of Governors;\nThe United States federal government is shut down due to a lapse in appropriations by October 1, 2025.\n\nChoose three of these, state the probabilities that you assign to the events happening, and explain in a few paragraphs how you formulated your beliefs. What reasoning and evidence did you consider, and how did you weigh it? Feel free to provide links to online sources as needed.\n\n\n\n\n\n\nAvoid wishful thinking\n\n\n\nFor some of these, you may have strong preferences about the outcome. Try to set them aside and report your honest assessment of the likelihood of the event, whether it pleases you or not.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 2"
    ]
  },
  {
    "objectID": "problems/pset-2.html#problem-2",
    "href": "problems/pset-2.html#problem-2",
    "title": "Problem Set 2",
    "section": "Problem 2",
    "text": "Problem 2\nLet \\(A,\\,B\\subseteq S\\) be any pair of events in a sample space \\(S\\), and show that\n\\[\n\\max\\{0,\\,P(A)+P(B)-1\\}\\leq P(A\\cap B)\\leq\\min\\{P(A),\\,P(B)\\}.\n\\]",
    "crumbs": [
      "Problem Sets",
      "Problem Set 2"
    ]
  },
  {
    "objectID": "problems/pset-2.html#problem-3",
    "href": "problems/pset-2.html#problem-3",
    "title": "Problem Set 2",
    "section": "Problem 3",
    "text": "Problem 3\nLet \\(A_1,\\,A_2,\\,A_3,\\,...\\subseteq S\\) be an infinite sequence of possibly overlapping events in some probability space. Based on this arbitrary sequence, define a new sequence of events that starts with \\(B_1 = A_1\\) and then has \\(B_i=A_i\\cap \\left(\\bigcup_{j=1}^{i-1}A_j\\right)^c\\) for all \\(i&gt;1\\).\n\nShow that the \\(B_i\\) are pairwise disjoint.\nShow that\n\n\\[\n\\bigcup_{i=1}^\\infty A_i=\\bigcup_{i=1}^\\infty B_i.\n\\]\n\nUse the previous parts to show that\n\n\\[\nP\\left(\\bigcup_{i=1}^\\infty A_i\\right)\\leq\\sum\\limits_{i=1}^\\infty P(A_i).\n\\]",
    "crumbs": [
      "Problem Sets",
      "Problem Set 2"
    ]
  },
  {
    "objectID": "problems/pset-2.html#problem-4",
    "href": "problems/pset-2.html#problem-4",
    "title": "Problem Set 2",
    "section": "Problem 4",
    "text": "Problem 4\nSuppose NC license plates are issued at random, each having three letters followed by 4 digits. The current distribution of plates includes all those on which the first letter is either H or J.\n\nWhat is the probability that you will get JET-5375? What about HAT-8007?\nWhat is the probability that you get a plate where the three letters are the same and the four digits are the same?",
    "crumbs": [
      "Problem Sets",
      "Problem Set 2"
    ]
  },
  {
    "objectID": "problems/pset-2.html#problem-5",
    "href": "problems/pset-2.html#problem-5",
    "title": "Problem Set 2",
    "section": "Problem 5",
    "text": "Problem 5\nA box contains 20 balls, 5 each of colors cyan, magenta, yellow and black. Answer the following for an experiment where 10 balls are selected at random without replacement from the box. What is the probability that:\n\nMore than two colors are missing from the selection?\nMagenta and yellow are missing from the selection?\nExactly two colors are missing from the selection?\nOnly the color cyan is missing from the selection?\nExactly one color is missing from the selection?\nAt least one color will be missing from the selection?\nNo colors are missing from the selection?",
    "crumbs": [
      "Problem Sets",
      "Problem Set 2"
    ]
  },
  {
    "objectID": "problems/pset-2.html#problem-6",
    "href": "problems/pset-2.html#problem-6",
    "title": "Problem Set 2",
    "section": "Problem 6",
    "text": "Problem 6\nA five-card hand is randomly dealt to you from a well-shuffled deck of 52 standard playing cards. What is the probability that your hand contains exactly two suits?",
    "crumbs": [
      "Problem Sets",
      "Problem Set 2"
    ]
  },
  {
    "objectID": "problems/pset-2.html#problem-7",
    "href": "problems/pset-2.html#problem-7",
    "title": "Problem Set 2",
    "section": "Problem 7",
    "text": "Problem 7\nLet \\(B_{10}\\) be the set of all length-10 binary strings. So for example, (1, 0, 0, 1, 0, 1, 1, 1, 0, 1) is an element of the set \\(B_{10}\\). Now, consider that we randomly draw two strings from the set \\(B_{10}\\) with replacement:\n\\[\n\\begin{aligned}\n\\mathbf{a}\n&=\n(a_1,\\,a_2,\\,a_3,\\,...,\\,a_{10})\\in B_{10}\\\\\n\\mathbf{b}\n&=\n(b_1,\\,b_2,\\,b_3,\\,...,\\,b_{10})\\in B_{10}.\n\\end{aligned}\n\\]\nIf we multiply the entries and add\n\\[\nX=a_1b_1+a_2b_2+\\cdots +a_{10}b_{10},\n\\]\nwhat is the probability that the sum \\(X\\) is equal to \\(k\\), for each \\(k=0,\\,1,\\,2,\\,3,\\,...,\\,10\\)?\n\nDerive a generic formula for \\(P(X=k)\\) that is a function of \\(k\\), but then plug into your formula and produce an \\(11\\times 2\\) table listing the actual decimal numbers for each \\(k\\);\nWrite a lil’ program in R that simulates 10,000 random trials of this phenomenon. Tally up the number of times you get each value of \\(k\\), and compare the empirical proportions to the actual probabilities in part a. In a large enough number of trials, they should be close by the law of large numbers, which we will study in a month or two.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 2"
    ]
  },
  {
    "objectID": "problems/pset-2.html#problem-8",
    "href": "problems/pset-2.html#problem-8",
    "title": "Problem Set 2",
    "section": "Problem 8",
    "text": "Problem 8\n\n\nAs you may have noticed, Spotify shuffle is not a perfectly random shuffle. If you have a playlist with \\(n\\) songs, Spotify’s shuffle algorithm definitely does not make all \\(n!\\) permutations equally likely. Why not? Because people don’t actually want that. Spotify used to implement a pure random shuffle, but users complained that it was too patternful. For better or worse, the human brain is wired to detect meaningless patterns in random noise. For instance, the picture on the left is a “truly” random scatter of points; the one on the right is manipulated so the points don’t “clump”:\n\n\n\n\n\n\n\n\nWhen you survey people that don’t know any better, they often feel like the second picture is “more random” than the first because it lacks clumps, but this is a misunderstanding. A truly random playlist shuffle will often have clumps: clumps of artists, clumps of genres, etc. So when people say that they want “random,” what they really mean is just “variety” in some vague sense, and this is what Spotify shuffle tries to deliver. I could not find a good source for what Spotify shuffle currently does, but whatever it is, I personally dislike it.\n\nImagine your playlist of \\(n\\geq 3\\) unique tracks contains \\(2\\leq k\\leq \\lceil n/2\\rceil\\) songs by the same artist. What is the probability that a random permutation of the tracks will contain at least one streak of songs by that artist? A streak is two or more of their songs in a row;\n\nThis is the playlist I use when I grade exams (something I enjoy doing, btw). It contains \\(n = 14\\) songs. There are 11 artists with one song apiece1, and then Chaka Khan has \\(k=3\\) songs, because, why wouldn’t she? Shuffle this playlist at least twenty times2 and compute the proportion of the time the shuffle contained a “Chaka streak.” Using the previous part, compare this empirical proportion to the theoretical probability that would prevail if the shuffle was truly random.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 2"
    ]
  },
  {
    "objectID": "problems/pset-2.html#problem-9",
    "href": "problems/pset-2.html#problem-9",
    "title": "Problem Set 2",
    "section": "Problem 9",
    "text": "Problem 9\nPearl Bailey and Elaine Stritch go target shooting. Suppose that each of Pearl’s shots hits a wooden duck target with probability \\(p_1\\), while each shot of Elaine’s hits it with probability \\(p_2\\), independent of Pearl. Suppose that they shoot simultaneously at the same target. If the wooden duck is knocked over (indicating that it was hit), what is the probability that both shots hit the duck?",
    "crumbs": [
      "Problem Sets",
      "Problem Set 2"
    ]
  },
  {
    "objectID": "problems/pset-2.html#problem-10",
    "href": "problems/pset-2.html#problem-10",
    "title": "Problem Set 2",
    "section": "Problem 10",
    "text": "Problem 10\nIf we know that \\(A\\) and \\(B\\) are independent events, show that their complements \\(A^c\\) and \\(B^c\\) are also independent events.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 2"
    ]
  },
  {
    "objectID": "problems/pset-2.html#submission",
    "href": "problems/pset-2.html#submission",
    "title": "Problem Set 2",
    "section": "Submission",
    "text": "Submission\nYou are free to compose your solutions for this problem set however you wish (scan or photograph written work, handwriting capture on a tablet device, LaTeX, Quarto, whatever) as long as the final product is a single PDF file. You must upload this to Gradescope and mark the pages associated with each problem.\nDo not forget to include the following:\n\nFor each problem, please acknowledge your collaborators;\nIf a problem required you to code something, please include both the code and the output. “Including the code” can be as crude as a screenshot, but you might also use Quarto to get a nice lil’ pdf that you can merge with the rest of your submission.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 2"
    ]
  },
  {
    "objectID": "problems/pset-2.html#footnotes",
    "href": "problems/pset-2.html#footnotes",
    "title": "Problem Set 2",
    "section": "Footnotes",
    "text": "Footnotes\n\nMartha Wash is technically heard twice, once in a solo capacity and once as a member of The Weather Girls, but let’s please not quibble;↩︎\nIf you don’t have a Spotify account, collaborate with a classmate who does and please acknowledge them.↩︎",
    "crumbs": [
      "Problem Sets",
      "Problem Set 2"
    ]
  },
  {
    "objectID": "problems/pset-0.html",
    "href": "problems/pset-0.html",
    "title": "Problem Set 0",
    "section": "",
    "text": "Nature laughs at the difficulties of integration.\nThis is ostensibly calculus review, but each problem is a piece of probability in disguise. We will illuminate these connections throughout the semester, and I will refer to Problem Set 0 often. Stay tuned!",
    "crumbs": [
      "Problem Sets",
      "Problem Set 0"
    ]
  },
  {
    "objectID": "problems/pset-0.html#problem-0",
    "href": "problems/pset-0.html#problem-0",
    "title": "Problem Set 0",
    "section": "Problem 0",
    "text": "Problem 0\nRecommend some music for us to listen to while we grade this.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 0"
    ]
  },
  {
    "objectID": "problems/pset-0.html#problem-1",
    "href": "problems/pset-0.html#problem-1",
    "title": "Problem Set 0",
    "section": "Problem 1",
    "text": "Problem 1\nExplain why this is horrific notation:\n\\[\n    \\int_0^x f(x)\\,\\textrm{d} x.\n\\]\nHow should it be fixed?",
    "crumbs": [
      "Problem Sets",
      "Problem Set 0"
    ]
  },
  {
    "objectID": "problems/pset-0.html#problem-2",
    "href": "problems/pset-0.html#problem-2",
    "title": "Problem Set 0",
    "section": "Problem 2",
    "text": "Problem 2\nSimplify this:\n\\[\n\\ln\\left(e^{a_1}e^{a_2}e^{a_3}\\cdots e^{a_n}\\right).\n\\]",
    "crumbs": [
      "Problem Sets",
      "Problem Set 0"
    ]
  },
  {
    "objectID": "problems/pset-0.html#problem-3",
    "href": "problems/pset-0.html#problem-3",
    "title": "Problem Set 0",
    "section": "Problem 3",
    "text": "Problem 3\nAssume \\(\\lambda&gt;0\\) is a constant and compute\n\\[\n    \\sum\\limits_{n=0}^\\infty n \\frac{\\lambda^n}{n!}e^{-\\lambda}\n    .\n\\]",
    "crumbs": [
      "Problem Sets",
      "Problem Set 0"
    ]
  },
  {
    "objectID": "problems/pset-0.html#problem-4",
    "href": "problems/pset-0.html#problem-4",
    "title": "Problem Set 0",
    "section": "Problem 4",
    "text": "Problem 4\nHere is a very silly function:\n\\[\nh(x)\n=\n\\exp\\left(-\\frac{1}{2}\\frac{(x-\\mu)^2}{\\sigma^2}\\right)\n,\\quad\n-\\infty&lt;x&lt;\\infty\n.\n\\]\nTreat \\(-\\infty&lt;\\mu&lt;\\infty\\) and \\(\\sigma&gt;0\\) as constants and compute the value(s) of \\(x\\) at which \\(h\\) has inflection points.\n\n\n\n\n\n\nHint\n\n\n\n\n\nHere is an example of what \\(h\\) might look like in the special case where \\(\\mu = 1\\) and \\(\\sigma=2\\):\n\nCodem = 1\ns = 2\npar(mar = c(4, 4, 0.1, 4))\ncurve(exp(-0.5 * ((x - m) / s)^2), \n      from = -6, to = 8, n = 500,\n      xlab = \"x\", ylab = \"h(x)\",\n      xaxt = \"n\")\naxis(1, at = -6:8)\n\n\n\n\n\n\n\nBefore you start doing any math, can you use the picture to guess what the answer will be?",
    "crumbs": [
      "Problem Sets",
      "Problem Set 0"
    ]
  },
  {
    "objectID": "problems/pset-0.html#problem-5",
    "href": "problems/pset-0.html#problem-5",
    "title": "Problem Set 0",
    "section": "Problem 5",
    "text": "Problem 5\nHere is another inordinately silly function:\n\\[\n\\Gamma(x)=\\int_0^\\infty y^{x-1}e^{-y}\\,\\textrm{d} y,\\quad x&gt;0.\n\\]\nProve that \\(\\Gamma(x+1)=x\\Gamma(x)\\).\n\n\n\n\n\n\nHint\n\n\n\n\n\nStart on the left-hand side by writing out \\(\\Gamma(x+1)\\) and evaluating the integral by parts.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 0"
    ]
  },
  {
    "objectID": "problems/pset-0.html#problem-6",
    "href": "problems/pset-0.html#problem-6",
    "title": "Problem Set 0",
    "section": "Problem 6",
    "text": "Problem 6\nLet \\(f\\) be any function with the following properties:\n\n\n\\(f\\) is twice continuously differentiable in a neighborhood of zero1;\n\n\\(f(0) = 0\\);\n\n\\(f'(0) = 0\\);\n\n\\(f''(0) = 1\\).\n\nAssume \\(t\\) is a constant and compute\n\\[\n    \\lim_{x\\to\\infty} xf\\left(\\frac{t}{\\sqrt{x}}\\right)\n    .\n\\]\n\n\n\n\n\n\nContinuity?\n\n\n\nA full credit solution must clearly explain how and why continuity is being used along the way.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 0"
    ]
  },
  {
    "objectID": "problems/pset-0.html#problem-7",
    "href": "problems/pset-0.html#problem-7",
    "title": "Problem Set 0",
    "section": "Problem 7",
    "text": "Problem 7\nConsider this integral:\n\\[\n    \\int_2^\\infty\n    \\frac{1}{x(\\ln x)^p}\\textrm{d} x\n    .\n\\]\n\n\nUse R to create a single plot with many lines, each graphing the integrand for a different value of \\(p\\). Consider \\(p\\) equal to -2, -1.5, -1, 0, 1, and 5, and make the \\(x\\)-axis of your plot run from 2 to 15.\nShow that \\(\\lim_{x\\to\\infty}\\frac{1}{x(\\ln x)^p}=0\\) for all values of \\(-\\infty&lt;p&lt;\\infty\\).\nFor what values of \\(p\\) does the integral converge? When it does converge, what is its value?\nConsult the picture you created in part (a), and write a few sentences explaining conceptually why the integral converges for some values of \\(p\\) but not others.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nWhen taking the limit or evaluating the integral, can you use the same technique for all values of \\(p\\), or do you need a different technique depending on what \\(p\\) is?",
    "crumbs": [
      "Problem Sets",
      "Problem Set 0"
    ]
  },
  {
    "objectID": "problems/pset-0.html#submission",
    "href": "problems/pset-0.html#submission",
    "title": "Problem Set 0",
    "section": "Submission",
    "text": "Submission\nYou are free to compose your solutions for this problem set however you wish (scan or photograph written work, handwriting capture on a tablet device, LaTeX, Quarto, whatever) as long as the final product is a single PDF file. You must upload this to Gradescope and mark the pages associated with each problem.\nDo not forget to include the following:\n\nFor each problem, please acknowledge your collaborators;\nIf a problem required you to code something, please include both the code and the output. “Including the code” can be as crude as a screenshot, but you might also use Quarto to get a nice lil’ pdf that you can merge with the rest of your submission.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 0"
    ]
  },
  {
    "objectID": "problems/pset-0.html#footnotes",
    "href": "problems/pset-0.html#footnotes",
    "title": "Problem Set 0",
    "section": "Footnotes",
    "text": "Footnotes\n\nThis means that \\(f\\) and its first two derivatives are all continuous functions at and around zero.↩︎",
    "crumbs": [
      "Problem Sets",
      "Problem Set 0"
    ]
  },
  {
    "objectID": "problems/pset-5.html",
    "href": "problems/pset-5.html",
    "title": "Problem Set 5",
    "section": "",
    "text": "Brace yourself for a gear shift. Now, seemingly out of nowhere, STA 240 becomes a calculus class.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 5"
    ]
  },
  {
    "objectID": "problems/pset-5.html#problem-0",
    "href": "problems/pset-5.html#problem-0",
    "title": "Problem Set 5",
    "section": "Problem 0",
    "text": "Problem 0\nRecommend some music for us to listen to while we grade this.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 5"
    ]
  },
  {
    "objectID": "problems/pset-5.html#problem-1",
    "href": "problems/pset-5.html#problem-1",
    "title": "Problem Set 5",
    "section": "Problem 1",
    "text": "Problem 1\nFrom now to the end of the course, we will use all of the tools of single-variable calculus. You got a preview of this on Problem Set 0, and our course page also includes some math refreshers (derivatives, integrals, etc). Please review these.\nWhat areas of calculus are you the least comfortable with at the moment? How do you think the course materials could be improved to better prepare you for this aspect of the course?",
    "crumbs": [
      "Problem Sets",
      "Problem Set 5"
    ]
  },
  {
    "objectID": "problems/pset-5.html#problem-2",
    "href": "problems/pset-5.html#problem-2",
    "title": "Problem Set 5",
    "section": "Problem 2",
    "text": "Problem 2\nConsider this function:\n\\[\nF(x) = \\begin{cases}\n1 - e^{-g(x)} & x\\geq 0\\\\\n0 & \\text{else}.\n\\end{cases}\n\\]\nAssume \\(g(x)\\) is continuous. I want \\(F(x)\\) to be a continuous function, and I want \\(F(x)\\) to be a valid cumulative distribution function (CDF). What properties must \\(g(x)\\) satisfy in order to make that happen? Give three examples of \\(g\\) that have these properties.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 5"
    ]
  },
  {
    "objectID": "problems/pset-5.html#problem-3",
    "href": "problems/pset-5.html#problem-3",
    "title": "Problem Set 5",
    "section": "Problem 3",
    "text": "Problem 3\nAn absolutely continuous random variable \\(X\\) has pdf\n\\[\nf(x)=\\begin{cases}\n\\frac{3}{22}[5 - (x-1)^2] & 1\\leq x \\leq3\\\\\n0 & \\text{else}.\n\\end{cases}\n\\]\n\nWhat is the range of \\(X\\)?\nConfirm that \\(f\\) is a valid pdf.\nDerive the formula for the cdf of \\(X\\) and plot it.\nCompute \\(P(0.9 &lt; X &lt; 1.1)\\).\nCompute \\(E(X)\\).\nCompute \\(\\text{var}(X)\\).",
    "crumbs": [
      "Problem Sets",
      "Problem Set 5"
    ]
  },
  {
    "objectID": "problems/pset-5.html#problem-4",
    "href": "problems/pset-5.html#problem-4",
    "title": "Problem Set 5",
    "section": "Problem 4",
    "text": "Problem 4\nRecall the gamma function\n\\[\n\\Gamma(x)=\\int_0^\\infty y^{x-1}e^{-y}\\,\\text{d}y.\n\\]\nShow that \\(\\Gamma\\left(\\frac{1}{2}\\right)=\\sqrt{\\pi}\\).",
    "crumbs": [
      "Problem Sets",
      "Problem Set 5"
    ]
  },
  {
    "objectID": "problems/pset-5.html#problem-5",
    "href": "problems/pset-5.html#problem-5",
    "title": "Problem Set 5",
    "section": "Problem 5",
    "text": "Problem 5\nA point is chosen at random on a line segment of length \\(L\\). Interpret this statement, and find the probability that the ratio of the shorter to the longer segment is less than \\(\\frac{1}{4}\\).\n\n\n\n\n\n\nHint\n\n\n\n\n\nIt might help to read about the continuous uniform distribution.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 5"
    ]
  },
  {
    "objectID": "problems/pset-5.html#problem-6",
    "href": "problems/pset-5.html#problem-6",
    "title": "Problem Set 5",
    "section": "Problem 6",
    "text": "Problem 6\nHere is the cdf of an absolutely continuous random variable \\(X\\):\n\\[\nF(x)\n=\n\\begin{cases}\n1-\\exp\\left(-\\sqrt{x}\\right)&x\\geq 0\\\\\n0&\\text{else}.\n\\end{cases}\n\\]\n\nWhat is \\(P(4\\leq X &lt; 9)\\)?\nWhat is the pdf of \\(X\\)?\nCompute \\(E(X^n)\\) for any \\(n\\in\\mathbb{N}\\).\nWhat is the variance of \\(X\\)?\nWhat is the median of \\(X\\)?\n\n\n\n\n\n\n\nHint\n\n\n\nAll of the moments of this distribution are finite, but nevertheless, it doesn’t have a moment-generating function (MGF). So you’ll hit a dead-end if you try using that to compute the moments. Instead, use LOTUS. It’s a subtle calculation, but it actually works out pretty nice.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 5"
    ]
  },
  {
    "objectID": "problems/pset-5.html#problem-7",
    "href": "problems/pset-5.html#problem-7",
    "title": "Problem Set 5",
    "section": "Problem 7",
    "text": "Problem 7\nOne of the technical themes of our course is “how do I compute an expected value without wanting to vomit?” We will have many tools in our tool belt for getting this done, and you must become fluent in knowing what to use when. Below are two nifty facts that might come in handy one day.\nLet \\(X\\) be an absolutely continuous random variable with pdf \\(f\\) and cdf \\(F\\).\n\nShow that:\n\n\\[\nE(X)=\\int_0^1F^{-1}(x)\\,\\text{d} x.\n\\]\n\nIf we make the extra assumption that \\(X\\) is a non-negative random variable, show that:\n\n\\[\nE(X)=\\int_0^\\infty P(X&gt;x)\\,\\text{d} x.\n\\]\n\n\n\n\n\n\nHint\n\n\n\n\n\nThere are at least two ways to do part b. If you’ve taken multivariable calculus, you can use double integral tricks. If you haven’t taken multivariable calculus yet, try integrating by parts.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 5"
    ]
  },
  {
    "objectID": "problems/pset-5.html#problem-8",
    "href": "problems/pset-5.html#problem-8",
    "title": "Problem Set 5",
    "section": "Problem 8",
    "text": "Problem 8\nLet \\(X\\) be any absolutely continuous random variable with pdf \\(f\\) and cdf \\(F\\), and assume that \\(E[(X-a)^2]\\) and \\(E\\left[|X-a|\\right]\\) are finite for all \\(a\\in \\mathbb{R}\\).\n\nCompute and interpret\n\n\\[\na_0=\\underset{a\\in\\mathbb{R}}{\\arg\\min}\\,E[(X-a)^2].\n\\]\n\nCompute and interpret\n\n\\[\nb_0=\\underset{b\\in\\mathbb{R}}{\\arg\\min}\\,E\\left[|X-b|\\right].\n\\]",
    "crumbs": [
      "Problem Sets",
      "Problem Set 5"
    ]
  },
  {
    "objectID": "problems/pset-5.html#problem-9",
    "href": "problems/pset-5.html#problem-9",
    "title": "Problem Set 5",
    "section": "Problem 9",
    "text": "Problem 9\n\nCompute the moment generating function of the geometric distribution. For what values of \\(t\\) is it defined?\nUse the mgf to compute the mean and variance of the geometric distribution.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 5"
    ]
  },
  {
    "objectID": "problems/pset-5.html#problem-10",
    "href": "problems/pset-5.html#problem-10",
    "title": "Problem Set 5",
    "section": "Problem 10",
    "text": "Problem 10\nConsider \\(X\\sim\\textrm{Gamma}(\\alpha,\\,\\beta)\\).\n\nFind \\(M_X(t)=E[e^{tX}]\\), the moment generating function of \\(X\\). For what values of \\(t\\) is it defined?\nUse the moment-generating function to compute \\(E(X)\\).\nUse the moment-generating function to compute \\(\\text{var}(X)\\).\nIf \\(c&gt;0\\), what is the distribution of \\(Y=cX\\)?",
    "crumbs": [
      "Problem Sets",
      "Problem Set 5"
    ]
  },
  {
    "objectID": "problems/pset-5.html#submission",
    "href": "problems/pset-5.html#submission",
    "title": "Problem Set 5",
    "section": "Submission",
    "text": "Submission\nYou are free to compose your solutions for this problem set however you wish (scan or photograph written work, handwriting capture on a tablet device, LaTeX, Quarto, whatever) as long as the final product is a single PDF file. You must upload this to Gradescope and mark the pages associated with each problem.\nDo not forget to include the following:\n\nFor each problem, please acknowledge your collaborators;\nIf a problem required you to code something, please include both the code and the output. “Including the code” can be as crude as a screenshot, but you might also use Quarto to get a nice lil’ pdf that you can merge with the rest of your submission.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 5"
    ]
  },
  {
    "objectID": "problems/pset-6.html",
    "href": "problems/pset-6.html",
    "title": "Problem Set 6",
    "section": "",
    "text": "Doodle a cute character that will cheer you on during this assignment.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 6"
    ]
  },
  {
    "objectID": "problems/pset-6.html#problem-0",
    "href": "problems/pset-6.html#problem-0",
    "title": "Problem Set 6",
    "section": "",
    "text": "Doodle a cute character that will cheer you on during this assignment.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 6"
    ]
  },
  {
    "objectID": "problems/pset-6.html#problem-1",
    "href": "problems/pset-6.html#problem-1",
    "title": "Problem Set 6",
    "section": "Problem 1",
    "text": "Problem 1\nHere is the cdf of an absolutely continuous random variable \\(X\\):\n\\[\nF(x;\\,\\alpha,\\,\\theta)\n=\n\\begin{cases}\n1-\\left(\\frac{\\theta}{x + \\theta}\\right)^\\alpha & x &gt;0 \\\\\n0 & \\text{else}.\n\\end{cases}\n\\]\nThe parameters \\(\\alpha\\) and \\(\\theta\\) are just positive constants.\n\nFind the pdf of \\(X\\) and plot it for \\(\\alpha=1,\\, 2,\\, 3\\) and \\(\\theta = 1\\);\nCompute the median of \\(X\\);\nCompute \\(E(X)\\). Is it finite for all values of the parameters?\nCompute \\(\\textrm{var}(X)\\). Is it finite for all values of the parameters?\nFix \\((\\alpha,\\,\\theta) = (3,\\, 100)\\) and compute \\(P(X &gt; 75\\,|\\, X &gt; 50)\\).\n\n\n\n\n\n\n\nTip\n\n\n\nOn Problem Set 5 you met some cute alternatives for computing expected values, and they may be helpful here.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 6"
    ]
  },
  {
    "objectID": "problems/pset-6.html#problem-2",
    "href": "problems/pset-6.html#problem-2",
    "title": "Problem Set 6",
    "section": "Problem 2",
    "text": "Problem 2\nQuantum mechanics has a reputation for being an intimidating branch of theoretical physics, and JZ certainly doesn’t understand a ding dong thing about it. Even so, because we have studied some basic probability theory, the mathematics of quantum mechanics is more accessible than you might guess. Behold:\n\n\nImagine a single particle at the (sub)atomic level1. A particle’s quantum state encodes information about all of its measurable properties, such as position, momentum, spin, and energy. In quantum mechanics, measuring these properties is fundamentally probabilistic. Before measurement, the particle doesn’t have a single, definite position or momentum; it has only probabilities for the possible outcomes. The quantum state of a particle is fully characterized by its wave function, and the squared magnitude of the wave function is a probability density that describes the distribution of the particle’s position, or momentum, or whatever else you wish to study. Let’s explore this in a very simple example.\nConsider a single, one-dimensional particle located somewhere in the interval \\([0,\\,L]\\). The wave function associated with the particle’s quantum state is\n\\[\n\\psi_n(x)\n=\n\\begin{cases}\nA\\cdot\\sin\\left(\\frac{n\\pi}{L}x\\right) & 0\\leq x\\leq L\\\\\n0 & \\text{else},\n\\end{cases}\n\\]\nwhere \\(A\\geq 0\\) is the amplitude, and \\(n=1,\\,2,\\,3,\\,...\\) indexes the particle’s energy level. The higher the energy level, the more “excited” the particle is. Behold:\n\n\n\n\n\n\n\n\nBecause the particle does not have a definite position before measurement, we treat the position \\(X_n\\in[0,\\,L]\\) of the particle as a continuous random variable whose probability density function is \\(f_n(x)=|\\psi_n(x)|^2\\).\n\nCompute the value of \\(A\\) that ensures that \\(f_n\\) is a valid pdf;\nCompute the cdf of the random variable \\(X_n\\) and plot it for various values of \\(n\\);\nWhat is the expected location of the particle?\nWhat is the probability that the particle is located at the midpoint of the interval?\nLet \\(n=4\\) and \\(L=1\\). What is the probability that the particle is located in the interval \\([0.2, 0.3]\\)?\nThe parameter \\(n=1,\\,2,\\,3,\\,...\\) indexes the energy level of the particle. The larger the value of \\(n\\), the more excited the particle is. If \\(n\\to\\infty\\), then the particle is, like, super stoked. What happens to the distribution of \\(X_n\\) as \\(n\\to\\infty\\)?",
    "crumbs": [
      "Problem Sets",
      "Problem Set 6"
    ]
  },
  {
    "objectID": "problems/pset-6.html#problem-3",
    "href": "problems/pset-6.html#problem-3",
    "title": "Problem Set 6",
    "section": "Problem 3",
    "text": "Problem 3\nLet \\(X\\) be a discrete random variable with\n\n\n\n\\(x\\)\n-1\n3\n7\n\n\n\\(P(X=x)\\)\n0.5\n0.2\n0.3\n\n\n\n\nCompute the mgf of \\(X\\).\nCompute the mean two ways: using the definition, and using the mgf. Confirm that you get the same answer.\nCompute the variance two ways: using the definition, and using the mgf. Confirm that you get the same answer.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 6"
    ]
  },
  {
    "objectID": "problems/pset-6.html#problem-4",
    "href": "problems/pset-6.html#problem-4",
    "title": "Problem Set 6",
    "section": "Problem 4",
    "text": "Problem 4\n\nCompute the moment-generating function of the binomial distribution;\nUse the mgf to compute the mean;\nUse the mgf to compute the variance.\n\nNote: we already know from lecture that the mean is \\(np\\) and the variance is \\(np(1-p)\\), so you know you did it right when you get the same answer.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 6"
    ]
  },
  {
    "objectID": "problems/pset-6.html#problem-5",
    "href": "problems/pset-6.html#problem-5",
    "title": "Problem Set 6",
    "section": "Problem 5",
    "text": "Problem 5\nLet \\(X\\) have density\n\\[\nf_X(x)\n=\n\\frac{1}{2}\ne^{-|x|},\\quad x\\in\\mathbb{R}.\n\\]\n\nWhat is the mgf of \\(X\\)?\nWhat is the mean?\nWhat is the variance?",
    "crumbs": [
      "Problem Sets",
      "Problem Set 6"
    ]
  },
  {
    "objectID": "problems/pset-6.html#problem-6",
    "href": "problems/pset-6.html#problem-6",
    "title": "Problem Set 6",
    "section": "Problem 6",
    "text": "Problem 6\nOn Lab 7, we learned how to derive the distribution of a transformation. This is an important technical skill that crops up often in mathematical probability, so let’s get our reps in:\n\nLet \\(X\\sim\\text{Unif}(0,\\,1)\\), and find the range and density of \\(Y=e^X\\);\nLet \\(X\\sim\\textrm{Unif}(0,\\,1)\\), and find the range and density of \\(Y=\\sqrt{X}\\);\nLet \\(X\\sim\\text{Unif}(0,\\,1)\\), and find the range and density of \\(Y=X^2\\);\nLet \\(X\\sim\\text{Gamma}(a,\\,b)\\), and find the range and density of \\(Y=1/X\\);\nLet \\(Z\\sim\\text{N}(0,\\,1)\\), and find the range and density of \\(Z^2\\).",
    "crumbs": [
      "Problem Sets",
      "Problem Set 6"
    ]
  },
  {
    "objectID": "problems/pset-6.html#problem-7",
    "href": "problems/pset-6.html#problem-7",
    "title": "Problem Set 6",
    "section": "Problem 7",
    "text": "Problem 7\nSuppose \\(X\\) and \\(Y\\) are jointly absolutely continuous random variables with joint density\n\\[\nf_{XY}(x,\\,y)=xe^{-x(y+1)},\\quad x,\\,y&gt;0.\n\\]\n\nFind the marginal density of \\(X\\).\nFind the marginal density of \\(Y\\).\nAre \\(X\\) and \\(Y\\) independent?\nFind the conditional density of \\(X\\) given \\(Y=y\\).\nFind the conditional density of \\(Y\\) given \\(X=x\\).",
    "crumbs": [
      "Problem Sets",
      "Problem Set 6"
    ]
  },
  {
    "objectID": "problems/pset-6.html#problem-8",
    "href": "problems/pset-6.html#problem-8",
    "title": "Problem Set 6",
    "section": "Problem 8",
    "text": "Problem 8\nLet \\(X\\) and \\(Y\\) be jointly absolutely continuous with density\n\\[\nf_{XY}(x,\\, y)=\\frac{1}{\\pi},\\quad x^2+y^2\\leq 1.\n\\]\nSo \\(X\\) and \\(Y\\) jointly possess the uniform distribution on the unit disc.\n\nThe joint density is a surface in three-dimensional space. Sketch what the joint density looks like.\nCompute the marginal densities of \\(X\\) and \\(Y\\).\nAre \\(X\\) and \\(Y\\) independent?",
    "crumbs": [
      "Problem Sets",
      "Problem Set 6"
    ]
  },
  {
    "objectID": "problems/pset-6.html#problem-9",
    "href": "problems/pset-6.html#problem-9",
    "title": "Problem Set 6",
    "section": "Problem 9",
    "text": "Problem 9\nConsider a random pair \\((X,\\,Y)\\) with a joint distribution given by this hierarchy:\n\\[\n\\begin{aligned}\n    X\n    &\\sim\n    \\textrm{Beta}(a,\\, b)\n    \\\\\n    Y\\mid X = x\n    &\\sim\n    \\textrm{Gamma}\\left(a+b,\\, \\frac{c}{x}\\right).\n\\end{aligned}\n\\]\n\nWhat is the joint range?\nWhat is the marginal density of \\(Y\\)?\nWhat is the conditional density of \\(X\\)?\n\n\n\n\n\n\n\nNew distribution!\n\n\n\nIf \\(X\\sim \\textrm{Beta}(a,\\, b)\\), then \\(\\text{Range}(X)=(0,\\,1)\\) and the density is\n\\[\nf(x)=\\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)}x^{a-1}(1-x)^{b-1},\\quad0&lt;x&lt;1.\n\\]\nThat’s new to you, so see if you can work with it.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 6"
    ]
  },
  {
    "objectID": "problems/pset-6.html#problem-10",
    "href": "problems/pset-6.html#problem-10",
    "title": "Problem Set 6",
    "section": "Problem 10",
    "text": "Problem 10\nConsider the joint distribution of random variables \\(X\\) and \\(Y\\), written in hierarchical form:\n\\[\n\\begin{aligned}\nX&\\sim\\textrm{Gamma}\\left(\\frac{d_2}{2},\\, \\frac{d_2}{2}\\right)\\\\\nY\\,|\\, X = x&\\sim\\textrm{Gamma}\\left(\\frac{d_1}{2},\\, \\frac{d_1}{2}x\\right).\\\\\n\\end{aligned}\n\\]\nDo some serious “massage and squint” to show that the marginal pdf of \\(Y\\) is\n\\[\nf_Y(y)=\\frac{\\Gamma\\left(\\frac{d_1}{2}+\\frac{d_2}{2}\\right)}{\\Gamma\\left(\\frac{d_1}{2}\\right)\\Gamma\\left(\\frac{d_2}{2}\\right)}\\left(\\frac{d_1}{d_2}\\right)^{\\frac{d_1}{2}}y^{\\frac{d_1}{2}-1}\\left(1+\\frac{d_1}{d_2}y\\right)^{-\\frac{d_1+d_2}{2}},\\quad y&gt;0.\n\\]\nThis means that \\(Y\\) has the F-distribution.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 6"
    ]
  },
  {
    "objectID": "problems/pset-6.html#submission",
    "href": "problems/pset-6.html#submission",
    "title": "Problem Set 6",
    "section": "Submission",
    "text": "Submission\nYou are free to compose your solutions for this problem set however you wish (scan or photograph written work, handwriting capture on a tablet device, LaTeX, Quarto, whatever) as long as the final product is a single PDF file. You must upload this to Gradescope and mark the pages associated with each problem.\nDo not forget to include the following:\n\nFor each problem, please acknowledge your collaborators;\nIf a problem required you to code something, please include both the code and the output. “Including the code” can be as crude as a screenshot, but you might also use Quarto to get a nice lil’ pdf that you can merge with the rest of your submission.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 6"
    ]
  },
  {
    "objectID": "problems/pset-6.html#footnotes",
    "href": "problems/pset-6.html#footnotes",
    "title": "Problem Set 6",
    "section": "Footnotes",
    "text": "Footnotes\n\nIf you’re like me, you cannot do this. Oh well.↩︎",
    "crumbs": [
      "Problem Sets",
      "Problem Set 6"
    ]
  },
  {
    "objectID": "problems/bank/mle/mle-normal-mean.html",
    "href": "problems/bank/mle/mle-normal-mean.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Consider these data:\n\\[\nX_1,\\,X_2,\\,...,\\,X_n\\overset{\\text{iid}}{\\sim}\\text{N}(\\theta,\\,1).\n\\]\n\nWhat is the maximum likelihood estimator of \\(\\theta\\in\\mathbb{R}\\)?\nWhat is the sampling distribution of the estimator?\nWhat is the MSE of the estimator?\nBased on the MSE, what are the statistical properties of this estimator?"
  },
  {
    "objectID": "problems/bank/mle/mle-laplace.html",
    "href": "problems/bank/mle/mle-laplace.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Let \\(X_1\\), \\(X_2\\), …, \\(X_n\\) be iid from some member of this parametric family:\n\\[\nf(x\\,|\\,\\theta)\n=\n\\frac{1}{2\\theta}\\exp\\left(-\\frac{|x|}{\\theta}\\right), \\quad -\\infty&lt;x&lt;\\infty.\n\\]\n\nWhat is the maximum likelihood estimator of \\(\\theta&gt;0\\)?\nWhat is the sampling distribution of the estimator?\nWhat is the MSE of the estimator?\nBased on the MSE, what are the statistical properties of this estimator?"
  },
  {
    "objectID": "problems/bank/mle/blitz.html",
    "href": "problems/bank/mle/blitz.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Blitz practice. For each of these, set up the (log-)likelihood function and compute the maximum likelihood estimator (MLE)."
  },
  {
    "objectID": "problems/bank/random-variables-continuous/expectation-short-cuts.html",
    "href": "problems/bank/random-variables-continuous/expectation-short-cuts.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "One of the technical themes of our course is “how do I compute an expected value without wanting to vomit?” We will have many tools in our tool belt for getting this done, and you must become fluent in knowing what to use when. Below are two nifty facts that might come in handy one day.\nLet \\(X\\) be an absolutely continuous random variable with pdf \\(f\\) and cdf \\(F\\).\n\nShow that:\n\n\\[\nE(X)=\\int_0^1F^{-1}(x)\\,\\text{d} x.\n\\]\n\nIf we make the extra assumption that \\(X\\) is a non-negative random variable, show that:\n\n\\[\nE(X)=\\int_0^\\infty P(X&gt;x)\\,\\text{d} x.\n\\]\n\n\n\n\n\n\nHint\n\n\n\n\n\nThere are at least two ways to do part b. If you’ve taken multivariable calculus, you can use double integral tricks. If you haven’t taken multivariable calculus yet, try integrating by parts."
  },
  {
    "objectID": "problems/bank/random-variables-continuous/a-in-terms-of-b.html",
    "href": "problems/bank/random-variables-continuous/a-in-terms-of-b.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Here is the pdf of some absolutely continuous random variable:\n\\[\nf(x)=\\begin{cases}\nax^2e^{-bx^2} & x\\geq 0\\\\\n0 & x&lt; 0.\n\\end{cases}\n\\]\nSolve for \\(a\\) in terms of \\(b\\). So your final answer will look like\n\\[\na=\\text{``some formula including $b$ and a bunch of constants''}\n\\]"
  },
  {
    "objectID": "problems/bank/random-variables-continuous/pareto.html",
    "href": "problems/bank/random-variables-continuous/pareto.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Here is the cdf of an absolutely continuous random variable \\(X\\):\n\\[\nF(x;\\,\\alpha,\\,\\theta)\n=\n\\begin{cases}\n1-\\left(\\frac{\\theta}{x + \\theta}\\right)^\\alpha & x &gt;0 \\\\\n0 & \\text{else}.\n\\end{cases}\n\\]\nThe parameters \\(\\alpha\\) and \\(\\theta\\) are just positive constants.\n\nFind the pdf of \\(X\\) and plot it for \\(\\alpha=1,\\, 2,\\, 3\\) and \\(\\theta = 1\\);\nCompute the median of \\(X\\);\nCompute \\(E(X)\\). Is it finite for all values of the parameters?\nCompute \\(\\textrm{var}(X)\\). Is it finite for all values of the parameters?\nFix \\((\\alpha,\\,\\theta) = (3,\\, 100)\\) and compute \\(P(X &gt; 75\\,|\\, X &gt; 50)\\).\n\n\n\n\n\n\n\nTip\n\n\n\nOn Problem Set 5 you met some cute alternatives for computing expected values, and they may be helpful here."
  },
  {
    "objectID": "problems/bank/random-variables-continuous/another-weibull.html",
    "href": "problems/bank/random-variables-continuous/another-weibull.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Here is a cdf for some nonnegative random variable \\(X\\):\n\\[\nF(x)\n=\n\\begin{cases}\n1-\\exp(-\\sqrt[3]{x}) & x\\geq 0 \\\\\n0 & \\text{else}.\n\\end{cases}\n\\]\n\nWhat is \\(P(1 &lt; X &lt;8)\\)?\nWhat is the pdf of \\(X\\)?\nCompute \\(E(X^n)\\) for any \\(n\\in\\mathbb{N}\\).\nWhat is the variance of \\(X\\)?"
  },
  {
    "objectID": "problems/bank/random-variables-continuous/weird.html",
    "href": "problems/bank/random-variables-continuous/weird.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Here is the goofy pdf of some nonnegative random variable \\(X\\):\n\\[\nf(x)\n=\ne^{1-e^x+x},\\quad x\\geq 0.\n\\]\n\nCompute the cdf of \\(X\\);\nCompute the median of \\(X\\)."
  },
  {
    "objectID": "problems/bank/random-variables-continuous/rayleigh.html",
    "href": "problems/bank/random-variables-continuous/rayleigh.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Consider a random variable \\(X\\) with pdf\n\\[\nf(x;\\,\\theta) =\\begin{cases}\n\\frac{x}{\\theta}e^{-\\frac{x^2}{2\\theta}} & x\\geq 0\\\\\n0 & \\text{else},\n\\end{cases}\n\\]\nwhere \\(\\theta &gt;0\\) is a parameter.\n\nCompute the CDF of \\(X\\).\nCompute the median of \\(X\\)."
  },
  {
    "objectID": "problems/bank/random-variables-continuous/continuous-cdf-g.html",
    "href": "problems/bank/random-variables-continuous/continuous-cdf-g.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Consider this function:\n\\[\nF(x) = \\begin{cases}\n1 - e^{-g(x)} & x\\geq 0\\\\\n0 & \\text{else}.\n\\end{cases}\n\\]\nAssume \\(g(x)\\) is continuous. I want \\(F(x)\\) to be a continuous function, and I want \\(F(x)\\) to be a valid cumulative distribution function (CDF). What properties must \\(g(x)\\) satisfy in order to make that happen? Give three examples of \\(g\\) that have these properties."
  },
  {
    "objectID": "problems/bank/random-variables-continuous/gamma-mgf.html",
    "href": "problems/bank/random-variables-continuous/gamma-mgf.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Consider \\(X\\sim\\textrm{Gamma}(\\alpha,\\,\\beta)\\).\n\nFind \\(M_X(t)=E[e^{tX}]\\), the moment generating function of \\(X\\). For what values of \\(t\\) is it defined?\nUse the moment-generating function to compute \\(E(X)\\).\nUse the moment-generating function to compute \\(\\text{var}(X)\\).\nIf \\(c&gt;0\\), what is the distribution of \\(Y=cX\\)?"
  },
  {
    "objectID": "problems/bank/conditional-probability/darts.html",
    "href": "problems/bank/conditional-probability/darts.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Figure 1\n\n\n\nJerry Lewis is throwing darts at the board pictured in Figure 1. It has the following properties:\n\nThe board has radius \\(r\\);\nThe board is partitioned into five rings, each with the same “thickness” \\(r/5\\);\nThe board is mounted on a wall of area \\(A\\).\n\nJerry is not very skilled. His dart is guaranteed to reach the wall and stick someplace, but beyond that, the probability that it lands in any given region is just proportional to that region’s area.\nTask: Derive a general formula for the conditional probability that a dart lands in ring \\(i\\) given that it hits the board and not the wall. Your formula should be a function of \\(i=1,\\,2,\\,3,\\,4,\\,5\\). Check that these probabilities are nonnegative and sum to one."
  },
  {
    "objectID": "problems/bank/conditional-probability/conditional-independence.html",
    "href": "problems/bank/conditional-probability/conditional-independence.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Let \\(A\\), \\(B\\), and \\(C\\) be events with \\(P(C) &gt; 0\\). \\(A\\) and \\(B\\) are conditionally independent given \\(C\\) if and only if\n\\[\nP(A \\cap B \\mid C) = P(A \\mid C)P(B \\mid C).\n\\]\nShow that the above implies \\(P(A \\mid B\\cap C) = P(A \\mid C)\\)."
  },
  {
    "objectID": "problems/bank/conditional-probability/target-practice.html",
    "href": "problems/bank/conditional-probability/target-practice.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Pearl Bailey and Elaine Stritch go target shooting. Suppose that each of Pearl’s shots hits a wooden duck target with probability \\(p_1\\), while each shot of Elaine’s hits it with probability \\(p_2\\), independent of Pearl. Suppose that they shoot simultaneously at the same target. If the wooden duck is knocked over (indicating that it was hit), what is the probability that both shots hit the duck?"
  },
  {
    "objectID": "problems/bank/conditional-probability/conditional-inequality.html",
    "href": "problems/bank/conditional-probability/conditional-inequality.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Assume that \\(P(A)&gt;0\\) and prove that\n\\[\nP(A\\cap B\\,|\\, A)\\geq P(A\\cap B\\,|\\, A\\cup B).\n\\]"
  },
  {
    "objectID": "problems/bank/conditional-probability/pairwise-versus-mutual-independence.html",
    "href": "problems/bank/conditional-probability/pairwise-versus-mutual-independence.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Let \\(S\\) be the set of all permutations of \\(a\\), \\(b\\), \\(c\\), together with the triples \\(aaa\\), \\(bbb\\), and \\(ccc\\). Imagine we draw an outcome randomly from this set. Define the event \\(A_k=\\{\\text{the kth spot is occupied by the letter a}\\}\\) for \\(k=1,\\,2,\\,3\\). Compute the probabilities \\(P(A_k)\\), \\(P(A_i\\cap A_j)\\), \\(P(A_1\\cap A_2\\cap A_3)\\), and comment on the independence of the three of events;\nImagine we roll two fair, six-sided die. Compute \\(P(A)\\), \\(P(B)\\), \\(P(C)\\), \\(P(A\\cap B)\\), \\(P(A\\cap C)\\), \\(P(B\\cap C)\\), and \\(P(A\\cap B\\cap C)\\) for the following three events and comment on their independence:\n\n\\[\n\\begin{aligned}\nA&=\\text{first die is 1, 2, 3}\\\\\nB&=\\text{first die is 3, 4, 5}\\\\\nC&=\\text{the sum of the two rolls is 9}.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "problems/bank/conditional-probability/axioms-conditional.html",
    "href": "problems/bank/conditional-probability/axioms-conditional.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Let \\(S\\) be a sample space with probability measure \\(P\\), and let \\(B\\subseteq S\\) be some event with \\(P(B)&gt;0\\). Show that the function \\(G(A)=P(A\\,|\\, B)\\) is a new probability measure on \\(S\\). That is, show that \\(G\\) satisfies the axioms:\n\ntotal measure 1;\nnonnegativity;\ncountable additivity."
  },
  {
    "objectID": "problems/bank/conditional-probability/bdj-circuit.html",
    "href": "problems/bank/conditional-probability/bdj-circuit.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "The figure above displays a circuit. Switch \\(S_i\\) closes with probability \\(p_i\\), and the switches close independently of one another. What is the probability that electricity can flow from \\(A\\) to \\(G\\)?"
  },
  {
    "objectID": "problems/bank/set-theory/picture-proofs.html",
    "href": "problems/bank/set-theory/picture-proofs.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "“Prove” each set identity with a picture:\n\nDistributive property: \\(A\\cup(B\\cap C)=(A\\cup B)\\cap(A\\cup C)\\);\nDe Morgan: \\((A\\cap B)^{c}=A^{c}\\cup B^{c}\\);"
  },
  {
    "objectID": "problems/bank/random-variables-discrete/memoryless-geometric.html",
    "href": "problems/bank/random-variables-discrete/memoryless-geometric.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Let \\(D \\sim \\text{Geometric}(p)\\), and fix positive integers \\(n\\) and \\(k\\). What is \\(P(D = n + k \\,|\\, D &gt; k)\\)? Does this look familiar?"
  },
  {
    "objectID": "problems/bank/random-variables-discrete/mak-rv.html",
    "href": "problems/bank/random-variables-discrete/mak-rv.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Let \\(X\\) be a random variable with the following pmf:\n\n\n\n\n\\(x\\)\n\\(P(X=x)\\)\n\n\n\n\n-1\n0.500\n\n\n0\n0.250\n\n\n1\n0.125\n\n\n2\n0.125\n\n\n\n\n\nWhat is the mean of \\(X\\)?\nWhat is the pmf of \\(Y = 2X-3\\)?\nWhat is the mean of \\(Y\\)?\nWhat is the pmf of \\(W = X^2\\)?\nWhat is the mean of \\(W\\)?\nWhat is the conditional pmf of \\(X\\) given \\(X \\neq 0\\)?\nWhat is the conditional mean of \\(X\\) given \\(X \\neq 0\\)?"
  },
  {
    "objectID": "problems/bank/random-variables-discrete/binomial-lotus.html",
    "href": "problems/bank/random-variables-discrete/binomial-lotus.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "If \\(X\\sim\\text{Binomial}(n,\\,p)\\), show that\n\\[\nE\\left[\\frac{1}{X+1}\\right]=\\frac{1-(1-p)^{n+1}}{(n+1)p}.\n\\]"
  },
  {
    "objectID": "problems/bank/random-variables-discrete/contested-election.html",
    "href": "problems/bank/random-variables-discrete/contested-election.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Not all election challenges are frivolous and cynical. Sometimes there are illegal/ineligible ballots in a close election, and if a candidate raises a legal challenge, a court must make a decision: certify the original results? adjust the vote totals somehow, possibly reversing the election? void the results entirely and order a new election? The concern of course is that the presence of the bad ballots might have swung the outcome, but how likely is this? Let’s model it.\nImagine an election where Angela Lansbury receives \\(a\\) votes, Bernadette Peters receives \\(b\\) votes, and Angela provisionally wins with a margin of victory \\(a-b&gt;0\\). However, of the total \\(a+b\\) votes, we learn that \\(k&gt;a-b\\) were invalid or illegal. But we know nothing else. We do not know who these folks are, where they came from, or who they voted for. We just know that they exist. If the \\(k\\) invalid ballots were removed from the total, what is the probability that it would flip the result of the election? If this probability is “too big,” we might start to worry.\nTo model this, think of the original \\(a+b\\) ballots as balls in the proverbial urn. \\(a\\) of these balls are red, and \\(b\\) of these balls are blue, and assume that \\(a,\\, b&gt;k\\). Removing the \\(k\\) invalid ballots is like drawing \\(k\\) balls out of the urn without replacement. Since we know nothing about the illegal ballots apart from the fact that they exist, assume that each of the \\(a+b\\) original ballots is equally likely to have been invalid. Next, imagine we randomly remove \\(k\\) balls from the urn, and let \\(X\\) be a random variable that counts the number of the removed/invalid ballots that belong to Angela. In other words, the number of red balls removed. If \\(X\\) is large enough, it will swing the election in Bernadette’s favor.\n\nWhat are the range and the pmf of the random variable \\(X\\)?\nFor what values of \\(X\\) does removing \\(k\\) ballots reverse the outcome of the election?\nBased on your answers to the previous parts, what is the probability that the election result is reversed when the \\(k\\) bad ballots are removed? Give a general formula for this probability, and then compute the probability for each of the real elections listed in Table 1;\nThe random variable \\(X\\) that you derived in the first part can be approximated by \\(\\text{Binom}(k,\\,1/2)\\). Explain why this approximation might be “good enough,” and verify that it’s not too bad by computing the approximate probability of reversal for the elections in Table 1. Compare the approximate probabilities to the actual ones from the previous part;\nLet’s say that we consider the probability of reversal “too big” if it is greater than 5%. Create a plot in R like Figure 1 with the following features:\n\nthe margin of victory \\(a-b\\) on the horizontal axis;\nthe number of bad ballots \\(k\\) on the vertical axis;\nscatterplot where each point represents an election in Table 1, with the points colored according to the decision that was made in the case;\ndivide the plane into two regions, one where the approximate probability of reversal is less than 0.05 and one where it is greater.\n\n\nWrite a few pithy paragraphs commenting on the following:\n\nthe assumptions of our lil’ model. Do you think they are innocuous? What might you change, and how?\nthe match or lack thereof between our model and the decisions that were actually made in the real cases;\nHow do you think a court should handle a legitimate election challenge? What are their responsibilities to the electorate, and what should they prioritize? Should they err on the side of upholding or voiding results? Should they think probabilistically? Should they consult this model, or one like it?\n\n\n\n\n\nTable 1: Close elections\n\n\n\n\n\n\n\n\n\n\n\n\n\nelection\nyear\nwin\nloss\ntotal\nmargin\ninvalid\ndecision\n\n\n\nIppolito v. Power\n1968\n1,422\n1,405\n2,827\n17\n101\nvoid\n\n\nSantucci v. Power\n1969\n58,076\n57,981\n116,057\n95\n448\nvoid\n\n\nDeMartini v. Power\n1970\n2,656\n2,594\n5,250\n62\n136\nupheld\n\n\nMaine House\n1976\n1,193\n1,060\n2,253\n133\n208\nvoid\n\n\nAnn Arbor mayor\n1977\n10,660\n10,659\n21,319\n1\n20\nvoid\n\n\nBrunswick ME town council\n1980\n2,390\n2,383\n4,773\n7\n16\nupheld\n\n\nWashington governor\n2004\n1,373,357\n1,373,228\n2,746,585\n129\n1,439\nupheld\n\n\n\n\n\n\n\nCodeelections &lt;- data.frame(\n  election = c(\"Ippolito\", \"Santucci\", \"DeMartini\", \"Maine\", \"Ann Arbor\", \"Brunswick\", \"Washington\"),\n  a = c(1422, 58076, 2656, 1193, 10660, 2390, 1373357),\n  b = c(1405, 57981, 2594, 1060, 10659, 2383, 1373228),\n  k = c(101, 448, 136, 208, 20, 16, 1439),\n  decision = c(\"void\", \"void\", \"upheld\", \"void\", \"void\", \"upheld\", \"upheld\"),\n  col = c(\"red\", \"red\", \"blue\", \"red\", \"red\", \"blue\", \"blue\")\n)\n\nreversal_prob_binom &lt;- function(a, b, k){\n  cutoff = ceiling(0.5 * (k + a - b))\n  1 - pbinom(cutoff - 1, k, 1/2)\n}\n\nmargin_grid &lt;- 0:150\nk_grid &lt;- 0:450\nn_points &lt;- length(margin_grid) * length(k_grid)\n\ngrid &lt;- data.frame(\n  margin = numeric(n_points),\n  k = numeric(n_points),\n  color = rep(\"lightblue\", n_points)\n)\n\nrow &lt;- 0\n\nfor(x in margin_grid){\n  for(y in k_grid){\n    row &lt;- row + 1\n    grid[row, 1] &lt;- x\n    grid[row, 2] &lt;- y\n    if(reversal_prob_binom(1 + x, 1, y) &gt;= 0.05){\n      grid[row, 3] &lt;- \"pink\"\n    }\n  }\n}\n\nplot(grid$margin, grid$k, col = grid$color, \n     pch = 19, cex = 0.2,\n     xlab = \"margin of victory (a - b)\", \n     ylab = \"number of invalid ballots (k)\",\n     main = \"Approximate probability of election reversal\")\npoints(elections$a - elections$b, elections$k, \n       pch = 19, col = elections$col)\nlegend(-7, 450, \"&gt; 0.05\", text.col = \"salmon\", bty = \"n\", cex = 1.5)\nlegend(115, 450, \"&lt; 0.05\", text.col = \"blue\", bty = \"n\", cex = 1.5)\nlegend(125, 85, c(\"Voided\", \"Upheld\"), pch = 19, col = c(\"red\", \"blue\"), bty = \"n\")\n\n\n\n\n\n\nFigure 1"
  },
  {
    "objectID": "problems/bank/random-variables-discrete/zero-truncated-poisson.html",
    "href": "problems/bank/random-variables-discrete/zero-truncated-poisson.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Let \\(X\\sim\\text{Poisson}(\\lambda)\\) and define a new random variable \\(Y\\) that is a zero-truncated version of \\(X\\). That is, we start with \\(\\text{Range}(X)=\\{0,\\,1,\\,2,\\,3,\\,...\\}\\), and then we rig it so that \\(\\text{Range}(Y)=\\{1,\\,2,\\,3,\\,...\\}\\). To do this, we define the PMF of the new random variable \\(Y\\) by\n\\[\nP(Y=k)\n=\nP(X=k\\mid X &gt; 0),\\quad k=1,\\,2,\\,3,\\,4,\\,...\n\\]\n\nShow that the new PMF sums to one;\nCompute the mean of \\(Y\\);\nCompute the variance of \\(Y\\)."
  },
  {
    "objectID": "problems/bank/random-variables-discrete/gandalf-test.html",
    "href": "problems/bank/random-variables-discrete/gandalf-test.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "King Elessar summons the wizard Gandalf to court, seeking his counsel. A foul plague has been incapacitating the soldiers. King Elessar asks Gandalf if there exists a test for the disease that is 100% sensitive and 100% specific (ie perfect). Gandalf says that a perfect blood test does exist, but it is very expensive, requiring the venom of a special lizard found only in the Morgul Vale. Each blood test would cost one gold piece, but the Royal Purse only has 500 gold pieces to spare, and there are 1000 soldiers that need to be tested. Gandalf sleeps on it and returns the next day with a plan for testing all of them that won’t break the bank:\n\nTake a blood sample from each of the 1000 soldiers;\nSplit each of these samples into two parts;\nTake one part of each sample and mix batches of ten of the samples together to create 100 mixed samples.\nTest each of those mixed samples;\nA few things could happen:\n\nIf the test gives a negative result, we know all of the original ten samples are clear, and we can move on;\nIf the test gives a positive result, then we know at least one of the original ten samples is positive. In that case we test the second part of the sample for each of the ten soldiers. We can then identify which soldiers are positive.\n\n\nGandalf finishes by saying “I expect sixty of the mixed batches to be negative. For the other forty batches, I expect the results to be positive, and hence we will have to test all four hundred of the original samples. This gives a total expected cost of 500 gold pieces.”\n\nWhat disease prevalence is Gandalf implicitly assuming here?\nCan Gandalf revise his scheme to reduce the expected cost even more?"
  },
  {
    "objectID": "problems/bank/random-variables-discrete/fibonacci-dice.html",
    "href": "problems/bank/random-variables-discrete/fibonacci-dice.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Signore Fibonacci has created a a balanced, six-sided die whose faces are labeled, naturally, 1, 2, 3, 5, 8, 13. He gives a pair to Enzo, who rolls them repeatedly, eager to get double 13s. Let \\(X\\) be the number of times he has to roll the pair of dice in order to see double 13s for the first time.\n\nWhat is the pmf of \\(X\\)?\nWhat is the expected value of \\(X\\)?\nFind a closed-form expression for \\(P(X\\geq m)\\)."
  },
  {
    "objectID": "problems/bank/random-variables-discrete/pascal.html",
    "href": "problems/bank/random-variables-discrete/pascal.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "If the discrete random variable \\(X\\) has the funky-ass distribution (FAS), then its range is \\(\\text{Range}(X)=\\mathbb{N}\\) and its pmf is\n\\[\nP(X=k)=\\binom{k+r-1}{k}(1-p)^kp^r\\quad k=0,\\,1,\\,2,\\,3,\\,...,\n\\]\nwhere the parameters \\(r\\in\\mathbb{N}\\) and \\(0&lt;p&lt;1\\) are constants. We denote this \\(X\\sim\\text{FAS}(r,\\,p)\\).\n\nUse what you know about probability to find the value of this infinite series:\n\n\\[\n\\sum\\limits_{k=0}^\\infty\n\\binom{k+r-1}{k}(1-p)^k.\n\\]\n\nCompute the MGF of \\(X\\);\nUse the definition of the expected value to compute the mean of \\(X\\);\nUse the MGF to compute the mean of \\(X\\) and verify that you get the same answer you got before;\nConsider an iid collection \\(X_i\\overset{\\text{iid}}{\\sim}\\text{FAS}(r,\\,p)\\) and derive the distribution of the sum \\(S_n=\\sum_{i=1}^nX_i\\)."
  },
  {
    "objectID": "problems/bank/conceptual/lousy-forecasting.html",
    "href": "problems/bank/conceptual/lousy-forecasting.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Here is a list of events that may or may not happen, but I believe we will know the final outcome before the end of the semester:\n\nZohran Mamdani wins the New York City mayoral election;\nWicked: For Good makes more money at the box office (inflation adjusted) on its opening weekend than Wicked: Part I;\nThe Philadelphia Phillies make it to the 2025 World Series;\nSean Combs is sentenced to prison for more than 3 years;\nA greater percentage of Duke undergraduates participate in this year’s Duke Marriage Pact than last year’s. About 43% participated last year;\nThe total number of points scored at the Countdown to Craziness exhibition game is greater than 50;\nBefore the end of the year, the Federal Reserve announces that Lisa Cook is no longer a member of the Board of Governors;\nThe United States federal government is shut down due to a lapse in appropriations by October 1, 2025.\n\nChoose three of these, state the probabilities that you assign to the events happening, and explain in a few paragraphs how you formulated your beliefs. What reasoning and evidence did you consider, and how did you weigh it? Feel free to provide links to online sources as needed.\n\n\n\n\n\n\nAvoid wishful thinking\n\n\n\nFor some of these, you may have strong preferences about the outcome. Try to set them aside and report your honest assessment of the likelihood of the event, whether it pleases you or not."
  },
  {
    "objectID": "problems/bank/conceptual/diaconis.html",
    "href": "problems/bank/conceptual/diaconis.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Persi Diaconis is a famous researcher in probability and statistics. He has a cute line of research where he probes the randomness that we take for granted in simple things like coin tossing, dice, and playing cards. In the interview above he discusses coin tossing, and says both “coin tossing is as close to a random phenomenon as I know” and “coin tossing is a deterministic process. What’s random about it?” Wut?\nWatch the interview, and report back with a brief summary of Diaconis’ explanation of what makes a coin toss random. Do you agree or disagree?"
  },
  {
    "objectID": "problems/bank/bayes/gamma-lomax.html",
    "href": "problems/bank/bayes/gamma-lomax.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Recall this infernal distribution family that you met on Problem Set 6, Midterm 2, and again in lecture:\n\\[\nf(x\\mid \\theta)=\\theta(x+1)^{-(\\theta+1)},\\quad x&gt;0.\n\\]\nWe’ve done MLE for this, now let’s go Bayes. Consider this model:\n\\[\n\\begin{aligned}\n\\theta&\\sim\\text{Gamma}(a_0,\\,b_0)\\\\\nX_{1:n}\\mid\\theta&\\overset{\\text{iid}}{\\sim}f(x\\mid \\theta).\n\\end{aligned}\n\\]\n\nWhat is the posterior distribution for \\(\\theta\\) conditional on the data?\nDerive the posterior mean and show that it is a convex combination of the prior mean and the MLE;\nThe posterior mean can be thought of as an alternative estimator, in a classical sense. So, what are its mean, variance, bias, and MSE?\nIs it possible for this new estimator to be superior (lower MSE) than the pure MLE?"
  },
  {
    "objectID": "problems/bank/bayes/beta-geometric.html",
    "href": "problems/bank/bayes/beta-geometric.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Consider the following Bayesian model:\n\\[\n\\begin{aligned}\np&\\sim\\text{Beta}(a_0,\\,b_0)&& \\text{(prior)}\\\\\nX_1,\\,X_2,\\,...,\\,X_n\\,|\\,p&\\overset{\\text{iid}}{\\sim}\\text{Geometric}(p).&& \\text{(likelihood)}\n\\end{aligned}\n\\]\n\nWhat is the posterior distribution?\nCompute the posterior mean and show that it is a weighted average of the prior mean and the maximum likelihood estimator."
  },
  {
    "objectID": "problems/bank/bayes/normal-normal.html",
    "href": "problems/bank/bayes/normal-normal.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Consider this Bayesian model:\n\\[\n\\begin{aligned}\n\\theta&\\sim\\text{N}(m_0,\\,\\tau_0^2)\\\\\nX_{1:n}\\mid\\theta&\\overset{\\text{iid}}{\\sim}\\text{N}(\\theta,\\,1).\n\\end{aligned}\n\\]\n\nWhat is the posterior distribution for \\(\\theta\\) conditional on the data?\nDerive the posterior mean and show that it is a convex combination of the prior mean \\(m_0\\) and the MLE.\nThe posterior mean can be thought of as an alternative estimator, in a classical sense. So, what are its mean, variance, bias, and MSE?\nIs it possible for this new estimator to be superior (lower MSE) than the pure MLE?"
  },
  {
    "objectID": "problems/bank/calculus-review/calc-bad-notation.html",
    "href": "problems/bank/calculus-review/calc-bad-notation.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Explain why this is horrific notation:\n\\[\n    \\int_0^x f(x)\\,\\textrm{d} x.\n\\]\nHow should it be fixed?"
  },
  {
    "objectID": "problems/bank/calculus-review/calc-pareto-ish.html",
    "href": "problems/bank/calculus-review/calc-pareto-ish.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Consider this integral:\n\\[\n    \\int_2^\\infty\n    \\frac{1}{x(\\ln x)^p}\\textrm{d} x\n    .\n\\]\n\nUse R to create a single plot with many lines, each graphing the integrand for a different value of \\(p\\). Consider \\(p\\) equal to -2, -1.5, -1, 0, 1, and 5, and make the \\(x\\)-axis of your plot run from 2 to 15.\nShow that \\(\\lim_{x\\to\\infty}\\frac{1}{x(\\ln x)^p}=0\\) for all values of \\(-\\infty&lt;p&lt;\\infty\\).\nFor what values of \\(p\\) does the integral converge? When it does converge, what is its value?\nConsult the picture you created in part (a), and write a few sentences explaining conceptually why the integral converges for some values of \\(p\\) but not others.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nWhen taking the limit or evaluating the integral, can you use the same technique for all values of \\(p\\), or do you need a different technique depending on what \\(p\\) is?"
  },
  {
    "objectID": "problems/bank/calculus-review/calc-logexp.html",
    "href": "problems/bank/calculus-review/calc-logexp.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Simplify this:\n\\[\n\\ln\\left(e^{a_1}e^{a_2}e^{a_3}\\cdots e^{a_n}\\right).\n\\]"
  },
  {
    "objectID": "problems/bank/basic-rules/set-minus.html",
    "href": "problems/bank/basic-rules/set-minus.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Let \\(S\\) be a sample space, let \\(A,\\, B\\subseteq S\\) be events, and define a new set\n\\[\nA-B = \\{x\\in S:x\\in A\\text{ and }x\\notin B\\}.\n\\]\n\nDraw a well-labeled picture of \\(A\\), \\(B\\), \\(S\\), and the set \\(A-B\\);\nWrite down an equivalent expression for \\(A-B\\) that only makes use of our three basic operations: union, intersection, and complement;\nProve that \\(P(A-B)=P(A)-P(A\\cap B)\\)."
  },
  {
    "objectID": "problems/bank/basic-rules/basic-mutually-exclusive-practice.html",
    "href": "problems/bank/basic-rules/basic-mutually-exclusive-practice.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Suppose that \\(A\\) and \\(B\\) are mutually exclusive events for which \\(P(A) = 0.3\\) and \\(P(B) = 0.5\\). What is the probability that…\n\neither \\(A\\) or \\(B\\) occurs?\n\\(A\\) occurs but \\(B\\) does not?\nboth \\(A\\) and \\(B\\) occur?\nAre \\(A\\) and \\(B\\) independent? Why or why not?"
  },
  {
    "objectID": "problems/bank/basic-rules/incl-excl-3.html",
    "href": "problems/bank/basic-rules/incl-excl-3.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Let \\(S\\) be a sample space, and consider events \\(A,\\, B,\\, C\\subseteq S\\). Recall that the law of inclusion/exclusion says that\n\\[\nP(A\\cup B)=P(A)+P(B)-P(A\\cap B).\n\\]\n\nHow should this be extended to unions of three events? \\[\nP(A\\cup B\\cup C)=P(A)+P(B)+P(C) +\\,...{???}\n\\] Explain your conjecture with words and pictures.\nUse set theory and the probabiity axioms to prove your conjecture in the previous part."
  },
  {
    "objectID": "problems/bank/basic-rules/intersection-bound.html",
    "href": "problems/bank/basic-rules/intersection-bound.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Let \\(A,\\,B\\subseteq S\\) be any pair of events in a sample space \\(S\\), and show that\n\\[\n\\max\\{0,\\,P(A)+P(B)-1\\}\\leq P(A\\cap B)\\leq\\min\\{P(A),\\,P(B)\\}.\n\\]"
  },
  {
    "objectID": "problems/bank/basic-rules/weather-app.html",
    "href": "problems/bank/basic-rules/weather-app.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "One day last semester I looked down at my phone and saw this screen:\n\n\n\n\n\nInterpret the three probabilities and explain how they fit together."
  },
  {
    "objectID": "problems/bank/basic-rules/bonferroni.html",
    "href": "problems/bank/basic-rules/bonferroni.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Let \\(A_1,\\,A_2,\\,...,\\,A_n\\subseteq S\\) be a finite collection of possibly overlapping events in some probability space. Show that\n\\[\nP\\left(\\bigcap_{i=1}^n A_i\\right)\\geq \\sum\\limits_{i=1}^nP(A_i)-n+1.\n\\]"
  },
  {
    "objectID": "problems/bank/basic-rules/xor-2.html",
    "href": "problems/bank/basic-rules/xor-2.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Let \\(A\\), \\(B\\), and \\(C\\) be three arbitrary events, and let \\(D\\) denote the event that “exactly one of these three events occurs.”\n\nDraw a cartoon of \\(A\\), \\(B\\), \\(C\\), and \\(D\\);\nUse the three basic set operations (union, intersection, and complement) to express \\(D\\) in terms of \\(A\\), \\(B\\), and \\(C\\);\nShow that\n\n\\[\nP(D)=P(A) + P(B) + P(C) - 2P(A \\cap B) - 2P(A \\cap C) - 2P(B \\cap C) + 3P(A \\cap B \\cap C).\n\\]"
  },
  {
    "objectID": "problems/bank/transformations/twerkout.html",
    "href": "problems/bank/transformations/twerkout.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "On Lab 7, we learned how to derive the distribution of a transformation. This is an important technical skill that crops up often in mathematical probability, so let’s get our reps in:\n\nLet \\(X\\sim\\text{Unif}(0,\\,1)\\), and find the range and density of \\(Y=e^X\\);\nLet \\(X\\sim\\textrm{Unif}(0,\\,1)\\), and find the range and density of \\(Y=\\sqrt{X}\\);\nLet \\(X\\sim\\text{Unif}(0,\\,1)\\), and find the range and density of \\(Y=X^2\\);\nLet \\(X\\sim\\text{Gamma}(a,\\,b)\\), and find the range and density of \\(Y=1/X\\);\nLet \\(Z\\sim\\text{N}(0,\\,1)\\), and find the range and density of \\(Z^2\\)."
  },
  {
    "objectID": "problems/bank/transformations/twerkout-2.html",
    "href": "problems/bank/transformations/twerkout-2.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "This is an important technical skill, so let’s drill it. For each of these, plot the transformation and then derive the range and the density of the new distribution:\n\nLet \\(X\\sim\\text{Unif}(0,\\,1)\\) and consider \\(Y=1/X\\);\nLet \\(X\\) have density \\(f_X(x)=42x^5(1-x)\\) for \\(0&lt;x&lt;1\\) and consider \\(Y=X^3\\);\nLet \\(X\\sim\\text{Exponential}(\\lambda)\\) and consider \\(Y=4X+3\\);\nLet \\(X\\) have cdf \\(F_X(x)=(x+1)^3/8\\) for \\(-1&lt;x&lt;1\\) and consider \\(Y=1-X^2\\);\nLet \\(X\\sim\\text{N}(0,\\,1)\\) and consider \\(Y=|X|\\)."
  },
  {
    "objectID": "problems/bank/moments/direct-mgf.html",
    "href": "problems/bank/moments/direct-mgf.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "A certain random variable \\(X\\) has moment-generating function\n\\[\nM(t)=\\frac{e^{2t}}{1-3t},\\quad t&lt;1/3.\n\\]\n\nCompute the mean and variance of \\(X\\).\nWhat is the moment generating function of the new random variable\n\n\\[\nY=\\frac{\\pi}{2}-\\frac{X}{3}.\n\\]"
  },
  {
    "objectID": "problems/bank/moments/normal-mgf.html",
    "href": "problems/bank/moments/normal-mgf.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Let \\(Z\\sim\\text{N}(0,\\,1)\\), and recall what that means:\n\\[\n\\begin{aligned}\nf_Z(z)&=\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}z^2\\right), && -\\infty&lt;z&lt;\\infty\\\\\nM_Z(t)&=\\exp\\left(\\frac{1}{2}t^2\\right), && -\\infty&lt;t&lt;\\infty.\n\\end{aligned}\n\\]\nNext, let \\(X=\\mu+\\sigma Z\\) for some constants \\(\\mu\\in\\mathbb{R}\\) and \\(\\sigma&gt;0\\).\n\nUse the change-of-variables formula to derive density of \\(X\\). What is its distribution?\nWhat is the moment-generating function of \\(X\\)?\nWhat are the mean and variance of \\(X\\)? Make sure you justify your answer with some type of derivation.\nConsider \\(X_1,\\,X_2,\\,...,\\,X_n\\overset{\\text{iid}}{\\sim}\\text{N}(\\mu,\\,\\sigma^2)\\) and derive the distribution of their sum and their average."
  },
  {
    "objectID": "problems/bank/moments/gumbel-mgf.html",
    "href": "problems/bank/moments/gumbel-mgf.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Consider a continuous random variable \\(X\\) with density\n\\[\nf(x)=\\frac{1}{\\beta}\\exp\\left(-\\frac{x}{\\beta}-e^{-x/\\beta}\\right),\\quad x\\in\\mathbb{R}.\n\\]\nThe parameter \\(\\beta&gt;0\\) is just a positive constant. Compute the moment-generating function of \\(X\\)."
  },
  {
    "objectID": "problems/bank/moments/compound-poisson.html",
    "href": "problems/bank/moments/compound-poisson.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Consider the following:\n\\[\n\\begin{aligned}\nN&\\sim\\text{Poisson}(\\lambda)\\\\\nX_1,\\,X_2,\\,X_3,\\,...&\\overset{\\text{iid}}{\\sim}M\\\\\nS&=\\sum\\limits_{i=1}^NX_i.\n\\end{aligned}\n\\]\n\\(N\\) is independent of all of the \\(X_i\\). The \\(X_i\\) are an infinite sequence of iid random variables each sharing a common moment-generating function \\(M(t)=E(e^{tX_1})\\). \\(S\\) is a random sum of random variables. The terms are random, and so is \\(N\\), the number of terms being summed.\n\nWhat is the MGF of \\(S\\)?\nIf I had an iid collection of \\(S_1\\), \\(S_2\\), …, \\(S_m\\) each possessing the same distribution that you just derived, what would be the distribution of \\(T=S_1+S_2+...+S_m\\)?"
  },
  {
    "objectID": "problems/bank/joint-distributions/gamma-exponential-hierarchy.html",
    "href": "problems/bank/joint-distributions/gamma-exponential-hierarchy.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Consider this joint distribution for random variables \\(X\\) and \\(Y\\), written in hierarchical form:\n\\[\n\\begin{aligned}\nX&\\sim\\text{Gamma}\\left(\\alpha,\\,\\beta\\right)\\\\\nY\\mid X=x&\\sim \\text{Exponential}(x).\n\\end{aligned}\n\\]\n\nSketch the joint range of \\((X,\\, Y)\\).\nWhat is the joint density of the random pair \\((X,\\, Y)\\)?\nWhat is the marginal density of \\(Y\\)?\nWhat is the density of \\(X\\) conditional on the event \\(Y=y\\)? Is it recognizable?"
  },
  {
    "objectID": "problems/bank/joint-distributions/gamma-weibull-hierarchy.html",
    "href": "problems/bank/joint-distributions/gamma-weibull-hierarchy.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Consider a random pair \\((Q,\\,Z)\\) of continuous random variables whose joint distribution is given by this hierarchy:\n\\[\n\\begin{aligned}\nQ&\\sim \\text{Gamma}(a,\\,b)\\\\\nZ\\mid Q=q&\\sim\\text{GF}(k, 1/q).\n\\end{aligned}\n\\]\nSo conditionally, \\(Z\\) has the distribution introduced in Problem 13 above, with \\(1/q\\) serving as the second parameter. \\(k&gt;0\\) is just a constant throughout.\n\nWhat is the joint density of \\((Q,\\,Z)\\)?\nWhat is the marginal distribution of \\(Z\\)?\nWhat is the conditional distribution of \\(Q\\) given \\(Z=z\\)?"
  },
  {
    "objectID": "problems/bank/joint-distributions/secret-bayes.html",
    "href": "problems/bank/joint-distributions/secret-bayes.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Suppose \\(X\\) and \\(Y\\) are jointly absolutely continuous random variables with joint density\n\\[\nf_{XY}(x,\\,y)=xe^{-x(y+1)},\\quad x,\\,y&gt;0.\n\\]\n\nFind the marginal density of \\(X\\).\nFind the marginal density of \\(Y\\).\nAre \\(X\\) and \\(Y\\) independent?\nFind the conditional density of \\(X\\) given \\(Y=y\\).\nFind the conditional density of \\(Y\\) given \\(X=x\\)."
  },
  {
    "objectID": "problems/bank/joint-distributions/uniform-on-the-disc.html",
    "href": "problems/bank/joint-distributions/uniform-on-the-disc.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Let \\(X\\) and \\(Y\\) be jointly absolutely continuous with density\n\\[\nf_{XY}(x,\\, y)=\\frac{1}{\\pi},\\quad x^2+y^2\\leq 1.\n\\]\nSo \\(X\\) and \\(Y\\) jointly possess the uniform distribution on the unit disc.\n\nThe joint density is a surface in three-dimensional space. Sketch what the joint density looks like.\nCompute the marginal densities of \\(X\\) and \\(Y\\).\nAre \\(X\\) and \\(Y\\) independent?"
  },
  {
    "objectID": "problems/bank/counting/20-balls-4-colors.html",
    "href": "problems/bank/counting/20-balls-4-colors.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "A box contains 20 balls, 5 each of colors cyan, magenta, yellow and black. Answer the following for an experiment where 10 balls are selected at random without replacement from the box. What is the probability that:\n\nMore than two colors are missing from the selection?\nMagenta and yellow are missing from the selection?\nExactly two colors are missing from the selection?\nOnly the color cyan is missing from the selection?\nExactly one color is missing from the selection?\nAt least one color will be missing from the selection?\nNo colors are missing from the selection?"
  },
  {
    "objectID": "problems/bank/counting/random-inner-product.html",
    "href": "problems/bank/counting/random-inner-product.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Let \\(B_{10}\\) be the set of all length-10 binary strings. So for example, (1, 0, 0, 1, 0, 1, 1, 1, 0, 1) is an element of the set \\(B_{10}\\). Now, consider that we randomly draw two strings from the set \\(B_{10}\\) with replacement:\n\\[\n\\begin{aligned}\n\\mathbf{a}\n&=\n(a_1,\\,a_2,\\,a_3,\\,...,\\,a_{10})\\in B_{10}\\\\\n\\mathbf{b}\n&=\n(b_1,\\,b_2,\\,b_3,\\,...,\\,b_{10})\\in B_{10}.\n\\end{aligned}\n\\]\nIf we multiply the entries and add\n\\[\nX=a_1b_1+a_2b_2+\\cdots +a_{10}b_{10},\n\\]\nwhat is the probability that the sum \\(X\\) is equal to \\(k\\), for each \\(k=0,\\,1,\\,2,\\,3,\\,...,\\,10\\)?\n\nDerive a generic formula for \\(P(X=k)\\) that is a function of \\(k\\), but then plug into your formula and produce an \\(11\\times 2\\) table listing the actual decimal numbers for each \\(k\\);\nWrite a lil’ program in R that simulates 10,000 random trials of this phenomenon. Tally up the number of times you get each value of \\(k\\), and compare the empirical proportions to the actual probabilities in part a. In a large enough number of trials, they should be close by the law of large numbers, which we will study in a month or two."
  },
  {
    "objectID": "problems/bank/counting/spotify-shuffle.html",
    "href": "problems/bank/counting/spotify-shuffle.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "As you may have noticed, Spotify shuffle is not a perfectly random shuffle. If you have a playlist with \\(n\\) songs, Spotify’s shuffle algorithm definitely does not make all \\(n!\\) permutations equally likely. Why not? Because people don’t actually want that. Spotify used to implement a pure random shuffle, but users complained that it was too patternful. For better or worse, the human brain is wired to detect meaningless patterns in random noise. For instance, the picture on the left is a “truly” random scatter of points; the one on the right is manipulated so the points don’t “clump”:\nWhen you survey people that don’t know any better, they often feel like the second picture is “more random” than the first because it lacks clumps, but this is a misunderstanding. A truly random playlist shuffle will often have clumps: clumps of artists, clumps of genres, etc. So when people say that they want “random,” what they really mean is just “variety” in some vague sense, and this is what Spotify shuffle tries to deliver. I could not find a good source for what Spotify shuffle currently does, but whatever it is, I personally dislike it."
  },
  {
    "objectID": "problems/bank/counting/spotify-shuffle.html#footnotes",
    "href": "problems/bank/counting/spotify-shuffle.html#footnotes",
    "title": "STA 240 Fall 2025",
    "section": "Footnotes",
    "text": "Footnotes\n\nMartha Wash is technically heard twice, once in a solo capacity and once as a member of The Weather Girls, but let’s please not quibble;↩︎\nIf you don’t have a Spotify account, collaborate with a classmate who does and please acknowledge them.↩︎"
  },
  {
    "objectID": "problems/bank/counting/mak-cards.html",
    "href": "problems/bank/counting/mak-cards.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Suppose you are dealt six cards from a well-shuffled, standard deck of 52.\n\nWhat is the probability of getting 3 of one rank and 3 of another?\nWhat is the probability of getting 4 of one rank and 2 of another?\nWhat is the probability of getting 3 aces and 3 of another rank?\nWhat is the probability of getting 4 of one rank and 2 aces?\nWhat is the probability that all 6 cards are in the same suit?\nWhat is the probability that all 6 cards are consecutive (ie a hand with 3 hearts, 4 spades, 5 spades, 6 clubs, 7 hearts, and the 8 diamonds)? Assume that the ace can only be the high card;\nWhat is the answer to the previous question if we allow the ace to be either high or low?\nWhat is the probability of receiving the ace, king, queen, jack, 10 and 9 all in hearts?"
  },
  {
    "objectID": "problems/bank/counting/yes-replace-order-no.html",
    "href": "problems/bank/counting/yes-replace-order-no.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Consider selecting \\(k\\) from \\(n\\) with replacement but ignoring order.\n\nLet’s take the specific case of \\(k=2\\) and \\(n=4\\). List out all possible ways you can select \\(k=2\\) items from \\(\\{1, 2, 3, 4\\}\\) in such a scenario.\nLet \\(x_i\\) be the number of times that element \\(\\{i\\}\\) is selected, for each \\(i = 1,\\,2,\\,3,\\, 4\\). For each combination identified in (a), show that it satisfies the following equation:\n\n\\[x_1 + x_2 + x_3 + x_4 = 2, \\quad x_1, x_2, x_3, x_4 \\in \\{0, 1, 2\\}.\\]\n\nState an analogous equation to part b for general \\(n\\) and \\(k\\). Use this to show that the number of ways you can select \\(k\\) from \\(n\\) with replacement and ignoring order is\n\n\\[\n\\binom{n+k-1}{k}=\\frac{(n+k-1)!}{k!(n-1)!}.\n\\]"
  },
  {
    "objectID": "problems/bank/counting/senate-committees.html",
    "href": "problems/bank/counting/senate-committees.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "The United States Senate contains two senators – one senior and one junior – from each of the \\(50\\) states.\n\nIf a committee of eight senators is selected, what is the probability that it will contain at least one of the two senators from a certain specified state?\nIf a committee of twenty senators is selected, what is the probability it will contain \\(15\\) junior senators and \\(5\\) senior senators?\nWhat is the probability that a group of \\(50\\) senators selected at random will contain one senator from each state?"
  },
  {
    "objectID": "problems/bank/counting/exactly-two-suits.html",
    "href": "problems/bank/counting/exactly-two-suits.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "A five-card hand is randomly dealt to you from a well-shuffled deck of 52 standard playing cards. What is the probability that your hand contains exactly two suits?"
  },
  {
    "objectID": "problems/bank/counting/license-plates.html",
    "href": "problems/bank/counting/license-plates.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Suppose NC license plates are issued at random, each having three letters followed by 4 digits. The current distribution of plates includes all those on which the first letter is either H or J.\n\nWhat is the probability that you will get JET-5375? What about HAT-8007?\nWhat is the probability that you get a plate where the three letters are the same and the four digits are the same?"
  },
  {
    "objectID": "labs/lab-6.html",
    "href": "labs/lab-6.html",
    "title": "Lab 6",
    "section": "",
    "text": "In this lab you will get some basic practice with the tools R provides for working with special families of (absolutely) continuous random variables. Fortunately, these are the same tools you saw for discrete random variables in Lab 5: the p-, d-, r- functions.",
    "crumbs": [
      "Labs",
      "Lab 6"
    ]
  },
  {
    "objectID": "labs/lab-6.html#task-1",
    "href": "labs/lab-6.html#task-1",
    "title": "Lab 6",
    "section": "Task 1",
    "text": "Task 1\nWhat is the probability that \\(X\\) will be exactly equal to 0.377?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis is a continuous random variable, so \\(P(X=0.377)=0\\).",
    "crumbs": [
      "Labs",
      "Lab 6"
    ]
  },
  {
    "objectID": "labs/lab-6.html#task-2",
    "href": "labs/lab-6.html#task-2",
    "title": "Lab 6",
    "section": "Task 2",
    "text": "Task 2\nWhat is the probability that \\(X\\) will be somewhere between 0.35 and 0.4?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(P(0.35&lt;X&lt;0.4)=F(0.4)-F(0.35)\\):\n\npnorm(0.4, 0.1, 0.3) - pnorm(0.35, 0.1, 0.3)\n\n[1] 0.04367313\n\n\nThis is the area of the shaded region under the density curve:",
    "crumbs": [
      "Labs",
      "Lab 6"
    ]
  },
  {
    "objectID": "labs/lab-6.html#task-3",
    "href": "labs/lab-6.html#task-3",
    "title": "Lab 6",
    "section": "Task 3",
    "text": "Task 3\nWhat is the probability that \\(X\\) will be negative?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(P(X&lt;0)=F(0)\\):\n\npnorm(0, 0.1, 0.3)\n\n[1] 0.3694413\n\n\nThis is the area of the shaded region under the density curve:",
    "crumbs": [
      "Labs",
      "Lab 6"
    ]
  },
  {
    "objectID": "labs/lab-6.html#task-4",
    "href": "labs/lab-6.html#task-4",
    "title": "Lab 6",
    "section": "Task 4",
    "text": "Task 4\nSimulate n = 5000 realizations of \\(X\\) and then verify that the sample mean, the sample standard deviation, and the sample proportion of negative values agree with the “true,” theoretical numbers.\n\n\n\n\n\n\nSet a seed!\n\n\n\nTo ensure that your numbers are reproducible and you get the same results every time you render your document, set a random number seed of your choosing.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nset.seed(1234)\nn &lt;- 5000\nx &lt;- rnorm(n, 0.1, 0.3)\n\nmean(x) # should be close to 0.1\n\n[1] 0.09809359\n\nsd(x) # should be close to 0.3\n\n[1] 0.2974035\n\nmean(x &lt;= 0) # should be close to answer from Task 3\n\n[1] 0.3684",
    "crumbs": [
      "Labs",
      "Lab 6"
    ]
  },
  {
    "objectID": "labs/lab-6.html#task-5",
    "href": "labs/lab-6.html#task-5",
    "title": "Lab 6",
    "section": "Task 5",
    "text": "Task 5\nPlot a histogram of your random numbers from the previous part, and overlay a line plot of the “true” density.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nhist(x, breaks = \"Scott\", freq = FALSE)\ncurve(dnorm(x, mean = 0.1, sd = 0.3), \n      from = min(x), to = max(x), n = 1000, add = TRUE, col = \"red\")",
    "crumbs": [
      "Labs",
      "Lab 6"
    ]
  },
  {
    "objectID": "labs/lab-6.html#task-6",
    "href": "labs/lab-6.html#task-6",
    "title": "Lab 6",
    "section": "Task 6",
    "text": "Task 6\nThe actuaries at your insurance company know that historically, monthly claims are about $10,000 on average, with a standard deviation of $2,000. If we want to use the model \\(X\\sim\\text{Gamma}(\\alpha,\\,\\beta)\\), how should we set \\(\\alpha\\) and \\(\\beta\\) so that \\(X\\) has the correct location and spread?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe need \\(\\alpha/\\beta = 10000\\) and \\(\\sqrt{\\alpha}/\\beta=2000\\). Alternatively, we need\n\\[\n\\frac{\\alpha/\\beta}{\\sqrt{\\alpha}/\\beta}=\\frac{10000}{2000}\\,\\implies\\,\\sqrt{\\alpha}=5.\n\\]\nSo \\(\\alpha = 5^2=25\\). Consequently:\n\\[\n\\frac{25}{\\beta}=10000\\,\\implies\\,\\beta=\\frac{25}{10000}=0.0025.\n\\]\nFinal answer: \\(\\alpha = 25\\) and \\(\\beta = 0.0025\\).",
    "crumbs": [
      "Labs",
      "Lab 6"
    ]
  },
  {
    "objectID": "labs/lab-6.html#task-7",
    "href": "labs/lab-6.html#task-7",
    "title": "Lab 6",
    "section": "Task 7",
    "text": "Task 7\nSimulate n = 5000 random numbers from the gamma distribution using the appropriate r- function. Plot a histogram of these numbers using the hist function, and overlay a line plot of the density using the curve function and the appropriate d- function.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nset.seed(1234)\nn &lt;- 5000\nx &lt;- rgamma(n, 25, rate = 0.0025)\nhist(x, breaks = \"Scott\", freq = FALSE)\ncurve(dgamma(x, 25, rate = 0.0025), \n      from = min(x), to = max(x), n = 1000, add = TRUE, col = \"red\")",
    "crumbs": [
      "Labs",
      "Lab 6"
    ]
  },
  {
    "objectID": "labs/lab-6.html#task-8",
    "href": "labs/lab-6.html#task-8",
    "title": "Lab 6",
    "section": "Task 8",
    "text": "Task 8\nVerify that the sample mean and sample variance of your random numbers are close to the prescriptions from Task 6. If they are not, revisit Task 6 and adjust your choice of \\(\\alpha\\) and \\(\\beta\\) until this works.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmean(x) # should be close to 10000\n\n[1] 10022.66\n\nsd(x) # should be close to 2000\n\n[1] 1972.818",
    "crumbs": [
      "Labs",
      "Lab 6"
    ]
  },
  {
    "objectID": "labs/lab-6.html#task-9",
    "href": "labs/lab-6.html#task-9",
    "title": "Lab 6",
    "section": "Task 9",
    "text": "Task 9\nWhat is the probability \\(P(X &gt; 15,000)\\)? Calculate this probability exactly using the appropriate p- function, and also approximate it using your random numbers. Verify that the two numbers are close.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n1 - pgamma(15000, 25, rate = 0.0025)\n\n[1] 0.01259674\n\nmean(x &gt; 15000)\n\n[1] 0.011\n\n\nThis is the area of the shaded region under the density curve:",
    "crumbs": [
      "Labs",
      "Lab 6"
    ]
  },
  {
    "objectID": "labs/lab-5.html",
    "href": "labs/lab-5.html",
    "title": "Lab 5",
    "section": "",
    "text": "Dating apps are horrific. Hopefully young people in college do not feel the need to use them. But for us decrepit old hags that were foolish enough to defer adulthood to the autumn of their thirties, they can feel unavoidable. So how do these work? Whelp, I make a profile, someone else on the app reviews it for no more than three seconds, and if I meet enough of their criteria, they swipe right (like my profile). If I fail to meet their criteria, they swipe left. If we both like each other’s profiles, it’s a match. God help us.\nLet’s model this. When the other person reviews my profile, they are deciding if I possess qualities they value. What might those qualities be? Charm, humor, wealth, height, education, hygiene, style, a deep and abiding appreciation for the films of Luis Buñuel, etc. Figure 1 imagines that the set of potentially desirable qualities are balls in the proverbial urn. For the purposes of discussion, let’s say there are a total of one hundred qualities a person could potentially value (100 balls in the jar). Every human has a list of 8 that they care most about. Everyone’s different, so we will model a given person’s preferences as a random selection (without replacement!) of 8 balls from this jar of 100.\nSo, given a person’s preferences, will they like my profile? Well, let’s say that I possess exactly three of the one hundred desirable qualities. I won’t tell you what those three are, but suffice it to say, we have yet to list them in Figure 1. We will treat my three qualities as three red balls in the jar, and the remaining 97 are black. Next, let \\(A\\) be the event that at least two of my qualities are among the eight that a person values. If this happens, then they will swipe right on my profile.",
    "crumbs": [
      "Labs",
      "Lab 5"
    ]
  },
  {
    "objectID": "labs/lab-5.html#task-1",
    "href": "labs/lab-5.html#task-1",
    "title": "Lab 5",
    "section": "Task 1",
    "text": "Task 1\nImagine a random person on the app is presented with my profile. They will either like my profile and swipe right (1) or they won’t like it and they will swipe left (0). Define an indicator random variable \\(I\\) for this event:\n\\[\nI =\\begin{cases}\n    0  & \\text{if $A^c$ happens} \\\\\n    1  & \\text{if $A$ happens}. \\\\\n\\end{cases}\n\\]\nWhat is the probability that a random person on the app swipes right on my profile? In other words, what is \\(P(I=1)\\)?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\[\n\\begin{aligned}\n    P(I=1)=P(A)&=1-P(A^c)\\\\\n    &=1-P(\\text{they value none or only one of my qualities})\\\\\n    &=1-P(\\text{none})-P(\\text{only one})\\\\\n    &=1-\\frac{\\binom{97}{8}}{\\binom{100}{8}}-\\frac{\\binom{3}{1}\\binom{97}{7}}{\\binom{100}{8}}\\\\\n    &\\approx 1 - 0.777 - 0.207\\\\\n    &=0.016.\n\\end{aligned}\n\\]\n\nlike_prob &lt;- 1 - (choose(97, 8) + 3 * choose(97, 7)) / choose(100, 8)\nlike_prob\n\n[1] 0.01627706",
    "crumbs": [
      "Labs",
      "Lab 5"
    ]
  },
  {
    "objectID": "labs/lab-5.html#task-2",
    "href": "labs/lab-5.html#task-2",
    "title": "Lab 5",
    "section": "Task 2",
    "text": "Task 2\nTask 1 gives the probability that one person likes my profile, but as we all know, dating apps are a numbers game. One strategy is to go on there, indiscriminately swipe right on everyone until you run out of people in your area, wait for the matches to roll in, and then filter through only those folks that have already expressed interest in you. The dating app creators know this is a possibility though, and so they cap the number of right swipes you can send in a 24 hour period (unless you buy a premium subscription). Let’s say that number is \\(n=25\\), as I believe it is on Bumble. So on a given day, I send 25 likes out into the world. How many of those \\(n=25\\) people will like me back, resulting in a match? Introduce a binary random variable \\(I_i\\) indicating whether or not person \\(i=1,\\,2,\\, ...,\\,25\\) likes my profile. Assume that these are all identically distributed as in Task 1, and assume that these are independent. We have yet to formally define this, but in context it means that the swiping decisions among these 25 people are not affecting one another. So none of them are sitting next to each other discussing my profile, for example.\nGiven this setup, define a new random variable \\(X\\) that counts the total number of matches I get. How is \\(X\\) related to the \\(I_i\\)? What is the distribution of \\(X\\)? What is \\(E(X)\\), the number of matches I expect to receive that day?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\\(X\\sim\\text{Binom}(25, 0.016)\\);\n\n\\(X=\\sum_{i=1}^{25}I_i\\);\n\n\\(E(X) \\approx 25\\times0.016=0.4\\).",
    "crumbs": [
      "Labs",
      "Lab 5"
    ]
  },
  {
    "objectID": "labs/lab-5.html#task-3",
    "href": "labs/lab-5.html#task-3",
    "title": "Lab 5",
    "section": "Task 3",
    "text": "Task 3\nUse R to compute the probability that I get at least one match that day.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe want \\(P(X\\geq 1)=1-P(X=0)\\):\n\nmatch_prob &lt;- 1 - dbinom(0, 25, like_prob)\nmatch_prob\n\n[1] 0.3365319",
    "crumbs": [
      "Labs",
      "Lab 5"
    ]
  },
  {
    "objectID": "labs/lab-5.html#task-4",
    "href": "labs/lab-5.html#task-4",
    "title": "Lab 5",
    "section": "Task 4",
    "text": "Task 4\nUse R to plot the PMF of \\(X\\).\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nn &lt;- 25\nplot(0:n, dbinom(0:n, n, like_prob), \n     type = \"h\", ylab = \"P(X = k)\", xlab = \"k\")",
    "crumbs": [
      "Labs",
      "Lab 5"
    ]
  },
  {
    "objectID": "labs/lab-5.html#task-5",
    "href": "labs/lab-5.html#task-5",
    "title": "Lab 5",
    "section": "Task 5",
    "text": "Task 5\nLet’s say that every day, I wake up, pour the coffee, do some lil’ granny stretches so that I don’t break a hip, and then swipe on the app until I’ve sent out my 25 likes for the day. Then I wait to see if any matches roll in. I do this day after day after day. Let \\(Y\\) be the number of days I have to wait through until I get at least one match.\nWhat is the distribution of \\(Y\\)? Use R to plot its PMF.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(Y\\sim\\text{Geom}(0.3365)\\).\n\nplot(1:26, dgeom(0:25, match_prob), type = \"h\", ylab = \"P(Y = k)\", xlab = \"k\")",
    "crumbs": [
      "Labs",
      "Lab 5"
    ]
  },
  {
    "objectID": "labs/lab-5.html#task-6",
    "href": "labs/lab-5.html#task-6",
    "title": "Lab 5",
    "section": "Task 6",
    "text": "Task 6\nLet’s say I first downloaded the app on a Sunday. Use R to compute the probability that I get a match in the first week.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe want the probability that one of the first 7 trials is a success. So \\(P(Y\\leq 7)\\):\n\npgeom(7 - 1, match_prob)\n\n[1] 0.9434099",
    "crumbs": [
      "Labs",
      "Lab 5"
    ]
  },
  {
    "objectID": "labs/lab-5.html#task-7-optional",
    "href": "labs/lab-5.html#task-7-optional",
    "title": "Lab 5",
    "section": "Task 7 (optional)",
    "text": "Task 7 (optional)\nWhen you post your profile on a dating app, everyone in your area can see and swipe on it. If someone swipes right on you, the app will usually alert you to the fact that this happened, but they will charge you for the privilege of seeing who exactly it is. So let \\(Z\\) be the number of “likes” you receive in a given twenty-four hour period. We could model this using the Poisson distribution and say that \\(Z\\sim \\text{Poisson}(\\lambda)\\). If we wish to model the likes received by some disgraced undesirable like James Corden, we might pick \\(Z\\sim \\text{Poisson}(0.01)\\), which has \\(E(Z)=0.01\\). Suffice it to say, not a tremendous number of likes rolling in for that guy, on average. If we wish to model the likes received by Taylor Swift, we might pick \\(Z\\sim\\text{Poisson}(20,000,000)\\). You get the idea.\nFor an ordinary person, perhaps \\(Z\\sim\\text{Poisson}(5)\\). Use R to compute the probability that such a person beats their average for the day.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe want \\(P(Z&gt;5)=1-P(Y\\leq 5)\\):\n\n1 - ppois(5, lambda = 5)\n\n[1] 0.3840393",
    "crumbs": [
      "Labs",
      "Lab 5"
    ]
  },
  {
    "objectID": "labs/lab-5.html#task-8-optional",
    "href": "labs/lab-5.html#task-8-optional",
    "title": "Lab 5",
    "section": "Task 8 (optional)",
    "text": "Task 8 (optional)\nUse R to plot the PMF of \\(Z\\).\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nplot(0:25, dpois(0:25, 5), type = \"h\", ylab = \"P(Z = k)\", xlab = \"k\")",
    "crumbs": [
      "Labs",
      "Lab 5"
    ]
  },
  {
    "objectID": "labs/lab-1.html",
    "href": "labs/lab-1.html",
    "title": "Lab 1",
    "section": "",
    "text": "This class will introduce you to the basics of the R programming language, but in contrast to courses like STA 101 or 1991, we will not primarily be using R for data analysis. Instead, we will be using it to run simulations. That is, we will use R to build little computational “laboratories” where we can poke and prod a random system and explore its properties. This skill complements a lot of the math we will be doing. In the modern era, mathematical reasoning and computer simulation are mutually reinforcing; you can check whether you did a calculation correctly by comparing with a simulation, and you can verify that a simulation result is legit by doing a proper calculation. Furthermore, in complex systems where the math is intractable, you can use simulation to explore what’s going on.\nDuring the semester, I will summarize the basic R skills we need in a series of concise “explainers” posted to the course webpage. Here’s what we have so far:\nPlease dip into those as needed.\nIn today’s lab, you will use R to simulate some basic random experiments and approximate probabilities. Along the way, you will get a light workout with if-else statements and for loops.",
    "crumbs": [
      "Labs",
      "Lab 1"
    ]
  },
  {
    "objectID": "labs/lab-1.html#footnotes",
    "href": "labs/lab-1.html#footnotes",
    "title": "Lab 1",
    "section": "Footnotes",
    "text": "Footnotes\n\nThe wise among us know that these are pronounced STAAAWANOWA and STAAAWANANA, respectively.↩︎",
    "crumbs": [
      "Labs",
      "Lab 1"
    ]
  },
  {
    "objectID": "labs/lab-2.html",
    "href": "labs/lab-2.html",
    "title": "Lab 2",
    "section": "",
    "text": "Last week, you used the sample function to simulate random phenomena that had a finite sample space with equally-likely outcomes: coin flips, die rolls, playing cards. Using a loop, you repeated an experiment many many times, calculated the proportion of the time some event happened, and used that proportion as an approximation of the probability1. In class this week, we learned counting techniques for actually doing the math to compute these probabilities exactly.\nIn this lab, you will do both of these things, the math and the simulation, and verify that they agree. The examples today are small potatoes, but these generic skills are important. Being able to whip up computer simulations of random systems is one of the main skills that will empower you to continue self-studying probability and statistics after you leave the classroom. Hardly a day goes by when myself and my colleagues don’t run a simulation of some kind.",
    "crumbs": [
      "Labs",
      "Lab 2"
    ]
  },
  {
    "objectID": "labs/lab-2.html#task-0-review",
    "href": "labs/lab-2.html#task-0-review",
    "title": "Lab 2",
    "section": "Task 0 (review)",
    "text": "Task 0 (review)\nConsider this question from Lab 1:\n\na five-card hand is randomly dealt to you from a well-shuffled deck. What is the probability that the hand contains at least one ace?\n\nLast week, we didn’t have the tools to do the math, but we could program a computer to approximate:\n\ndeck_of_cards &lt;- c(\"AH\", \"2H\", \"3H\", \"4H\", \"5H\", \"6H\", \"7H\", \"8H\", \"9H\", \"10H\", \"JH\", \"QH\", \"KH\",\n                   \"AD\", \"2D\", \"3D\", \"4D\", \"5D\", \"6D\", \"7D\", \"8D\", \"9D\", \"10D\", \"JD\", \"QD\", \"KD\",\n                   \"AC\", \"2C\", \"3C\", \"4C\", \"5C\", \"6C\", \"7C\", \"8C\", \"9C\", \"10C\", \"JC\", \"QC\", \"KC\",\n                   \"AS\", \"2S\", \"3S\", \"4S\", \"5S\", \"6S\", \"7S\", \"8S\", \"9S\", \"10S\", \"JS\", \"QS\", \"KS\")\n\nset.seed(123)\n\noutcomes &lt;- c()\n\nfor (i in 1:10000) {\n  hand &lt;- sample(deck_of_cards, size = 5, replace = FALSE)\n  if (any(hand %in% c(\"AH\",\"AD\",\"AC\",\"AS\"))) {\n    outcomes &lt;- c(outcomes, 1)\n  } else {\n    outcomes &lt;- c(outcomes, 0)\n  }\n}\n\nmean(outcomes)\n\n[1] 0.3473\n\n\nNow let’s see how the math goes. The complement of the target event “at least one ace” is “no aces at all.” This is simpler and may be easier to count, so we will apply:\n\\[\nP(A)=1-P(A^c)=1-\\frac{\\#(A^c)}{\\#(S)}.\n\\]\nThe sample space \\(S\\) is the set of all five-card hands you could be dealt. We are selecting \\(k=5\\) from \\(n=52\\) without replacement and ignoring order, so \\(\\#(S)=\\binom{52}{5}\\). The event \\(A^c\\) is the set of all five-card hands that do not include an ace. There are 48 non-ace cards, and so \\(\\#(A^c)=\\binom{48}{5}\\). As such\n\\[\nP(A)=1-\\frac{\\binom{48}{5}}{\\binom{52}{5}}\\approx 0.34.\n\\]\nTo get the actual number, we can use the choose function in R. Try typing this into the console:\n\n1 - choose(48, 5) / choose(52, 5)\n\n[1] 0.341158\n\n\nThis is close to the number we approximated via simulation. Phew!",
    "crumbs": [
      "Labs",
      "Lab 2"
    ]
  },
  {
    "objectID": "labs/lab-2.html#task-1-i-do-it",
    "href": "labs/lab-2.html#task-1-i-do-it",
    "title": "Lab 2",
    "section": "Task 1 (I do it)",
    "text": "Task 1 (I do it)\n\n\nScenario: roll three fair dice;\n\nEvent: all three dice show different numbers.\n\nThis code simulates 7,500 trials and computes the proportion of the time the event occurs:\n\nset.seed(37367)\n\nnreps &lt;- 7500\ncounter &lt;- 0\n\nfor(i in 1:nreps){\n  \n  rolls &lt;- sample(1:6, 3, replace = TRUE)\n  \n  if( length(unique(rolls)) == 3 ){\n    \n    counter &lt;- counter + 1\n    \n  }\n  \n}\n\ncounter / nreps # proportion\n\n[1] 0.558\n\n\nIn order to do the math, we apply \\(P(A)=\\#(A)/\\#(S)\\) directly. The sample space \\(S\\) is the set of all three-dice rolls. Thought of as three experiments in the sense of the counting principle, the number of ways we could mix and match the rolls is \\(\\#(S)=6\\times6\\times6=6^3=216\\). The target event \\(A\\) is the set of all three-dice rolls where all numbers are different (ie sampling without replacement), and so \\(\\#(A)=6\\times 5\\times 4=120\\). Putting it into R gives:\n\n120 / 216\n\n[1] 0.5555556\n\n\nWe conclude then that\n\\[\nP(A) = \\frac{6\\times 5\\times 4}{6^3} = \\frac{120}{216} \\approx 0.555,\n\\]\nwhich agrees with our simulation.",
    "crumbs": [
      "Labs",
      "Lab 2"
    ]
  },
  {
    "objectID": "labs/lab-2.html#task-2-we-do-it",
    "href": "labs/lab-2.html#task-2-we-do-it",
    "title": "Lab 2",
    "section": "Task 2 (we do it)",
    "text": "Task 2 (we do it)\n\n\nScenario: 6 balls are randomly drawn from a jar containing 5 red, 4 blue, 3 green;\n\nEvent: you draw exactly 3 red or exactly 2 green.\n\nHere is a representation of the contents of the jar that you can sample from:\n\njar &lt;- c(rep(\"R\", 5), rep(\"B\", 4), rep(\"G\", 3))\njar\n\n [1] \"R\" \"R\" \"R\" \"R\" \"R\" \"B\" \"B\" \"B\" \"B\" \"G\" \"G\" \"G\"\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nset.seed(2025)\nnsim &lt;- 20000\ncount_event &lt;- 0\n\njar &lt;- c(rep(\"R\", 5), rep(\"B\", 4), rep(\"G\", 3))\n\nfor (i in 1:nsim) {\n  hand &lt;- sample(jar, 6, replace = FALSE)\n  n_red &lt;- sum(hand == \"R\")\n  n_green &lt;- sum(hand == \"G\")\n  \n  if (n_red == 3 | n_green == 2) {\n    count_event &lt;- count_event + 1\n  }\n}\n\ncount_event / nsim\n\n[1] 0.6606\n\n\n\n\n\\(A\\) = exactly 3 red, \\(B\\) = exactly 2 green\nBoth could happen so \\(A\\cap B\\) is non-empty;\n\n\\(\\#(S)=\\binom{12}{6}\\);\n\n\\(\\#(A) = \\binom{5}{3}\\binom{7}{3}\\);\n\n\\(\\#(B) = \\binom{3}{2}\\binom{9}{4}\\);\n\n\\(\\#(A\\cap B)=\\binom{5}{3}\\binom{3}{2}4\\);\nSo\n\n\\[\nP(A\\cup B)=P(A)+P(B)-P(A\\cap B)=\\frac{\\binom{5}{3}\\binom{7}{3}}{\\binom{12}{6}}+\\frac{\\binom{3}{2}\\binom{9}{4}}{\\binom{12}{6}}-\\frac{\\binom{5}{3}\\binom{3}{2}4}{\\binom{12}{6}}.\n\\]\nCalculate:\n\nPA &lt;- choose(5, 3) * choose(7, 3) / choose(12, 6)\nPB &lt;- choose(3, 2) * choose(9, 4) / choose(12, 6)\nPAB &lt;- choose(5, 3) * choose(3, 2) * 4 / choose(12, 6)\n\nPA + PB - PAB\n\n[1] 0.6580087",
    "crumbs": [
      "Labs",
      "Lab 2"
    ]
  },
  {
    "objectID": "labs/lab-2.html#task-3-you-do-it",
    "href": "labs/lab-2.html#task-3-you-do-it",
    "title": "Lab 2",
    "section": "Task 3 (you do it)",
    "text": "Task 3 (you do it)\n\n\n\n\n\nFigure 1: 8 safe rooks\n\n\n\n\nScenario: you randomly place 8 rooks on distinct squares of a chess board;\n\nEvent: all 8 rooks are safe from one another.\n\nFigure 1 displays an example of a safe position. In chess, rooks are permitted to move up-and-down or side-to-side as far as they want, but not diagonally or anything else.\nThis code creates a matrix representing the chess board, from which you can sample the random positions:\n\nboard &lt;- outer(letters[1:8], 1:8, paste0)\nboard\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n[1,] \"a1\" \"a2\" \"a3\" \"a4\" \"a5\" \"a6\" \"a7\" \"a8\"\n[2,] \"b1\" \"b2\" \"b3\" \"b4\" \"b5\" \"b6\" \"b7\" \"b8\"\n[3,] \"c1\" \"c2\" \"c3\" \"c4\" \"c5\" \"c6\" \"c7\" \"c8\"\n[4,] \"d1\" \"d2\" \"d3\" \"d4\" \"d5\" \"d6\" \"d7\" \"d8\"\n[5,] \"e1\" \"e2\" \"e3\" \"e4\" \"e5\" \"e6\" \"e7\" \"e8\"\n[6,] \"f1\" \"f2\" \"f3\" \"f4\" \"f5\" \"f6\" \"f7\" \"f8\"\n[7,] \"g1\" \"g2\" \"g3\" \"g4\" \"g5\" \"g6\" \"g7\" \"g8\"\n[8,] \"h1\" \"h2\" \"h3\" \"h4\" \"h5\" \"h6\" \"h7\" \"h8\"\n\n\nHere’s an example of 8 random squares chosen for the rooks:\n\nrandom_position\n\n[1] \"g6\" \"h4\" \"h6\" \"f4\" \"d1\" \"c6\" \"d7\" \"d6\"\n\n\nSo that’s where your 8 rooks live. In order to check whether or not they are safe from one another, you might find it helpful to separate out the row information and the column information of the placement using the substring command:\n\nsubstring(random_position, 1, 1)\n\n[1] \"g\" \"h\" \"h\" \"f\" \"d\" \"c\" \"d\" \"d\"\n\nsubstring(random_position, 2, 2)\n\n[1] \"6\" \"4\" \"6\" \"4\" \"1\" \"6\" \"7\" \"6\"\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nboard &lt;- outer(letters[1:8], 1:8, paste0)\n\nset.seed(2025)\nnsim &lt;- 100000\ncount_safe &lt;- 0\n\nfor (i in 1:nsim) {\n  rooks &lt;- sample(board, 8)\n  \n  # split into rows (numbers) and cols (letters)\n  r &lt;- substring(rooks, 2, 2) # row = the number part\n  c &lt;- substring(rooks, 1, 1) # col = the letter part\n  \n  # safe if all 8 rows are different AND all 8 cols are different\n  if (length(unique(r)) == 8 & length(unique(c)) == 8) {\n    count_safe &lt;- count_safe + 1\n  }\n}\n\ncount_safe / nsim\n\n[1] 1e-05\n\n\n\nRecall that rooks can move up and down or side-to-side, but not diagonally. In order to be safe from one another, two rooks therefore need to occupy different rows and columns of the board. For the eight rooks to be mutually safe, they all must be in different columns and different rows from one another;\nThere are \\(n=8\\cdot 8=64\\) squares on the board, and we are selecting \\(k=8\\) of them at which to place the rooks. The rooks are identical, so order doesn’t matter, and it’s without replacement since the squares need to be distinct, and so the total number of possible outcomes is \\(\\#(S)=\\binom{64}{8}\\);\nThe event \\(A\\) we care about is “the rooks are safe from one another,” and this can happen \\(\\#(A)=8!\\) ways.\nWe know that a safe placement will have every row and column occupied by exactly one rook, so let us imagine building up a safe placement row-wise, starting from the bottom and working upward. There are 8 ways to place a single rook on the bottom row. No other rooks can go there, so we move to the next row. There are only 7 ways to place the next rook in the next row, because we must avoid the column occupied by the first rook. Similarly, there are only 6 ways to place the next rook in the next row, because we must avoid the two columns occupied by the first two rooks. And so on. As a consequence of the counting principle, where we regard each row as an experiment, we see that \\(\\#(A)=8\\times7\\times6\\times\\cdots\\times2\\times1=8!\\).\nSo\n\n\\[\nP(A)=\\frac{\\#(A)}{\\#(S)}=\\frac{8!}{\\binom{64}{8}}\\approx0.0000091095.\n\\].\n\nfactorial(8) / choose(64, 8)\n\n[1] 9.109465e-06",
    "crumbs": [
      "Labs",
      "Lab 2"
    ]
  },
  {
    "objectID": "labs/lab-2.html#footnotes",
    "href": "labs/lab-2.html#footnotes",
    "title": "Lab 2",
    "section": "Footnotes",
    "text": "Footnotes\n\nThe proportion will converge to the true probability by the law of large numbers, which we will study later in the course.↩︎",
    "crumbs": [
      "Labs",
      "Lab 2"
    ]
  },
  {
    "objectID": "labs/lab-8.html",
    "href": "labs/lab-8.html",
    "title": "Lab 8",
    "section": "",
    "text": "When we call rnorm or rgamma or rpois to generate random numbers from one of our special distributions, what is the computer actually doing? How do we program a computer to generate random numbers that behave according to some pre-defined distribution?\nAt the end of the day, all simulation methods revolve around a simple principle: “simulate from the standard uniform and then apply a transformation.” We statisticians take from granted that the computer scientists have devised good methods for generating random numbers from Unif(0, 1). The Mersenne Twister is a popular algorithm. Given that we can generate random numbers from the standard uniform, we need to figure out how to transform them into a batch of new random numbers that follow the distribution we want: normal, gamma, Cauchy, whatever.\nTask 0\nThe following theorem is a proof of concept that “simulate Unif(0, 1) and transform” is sufficient, at least in theory, to simulate any distribution we can think of:\n\n\n\n\n\n\nTheorem: inverse transform sampling\n\n\n\nLet \\(U\\sim \\text{Unif}(0,\\, 1)\\), and let \\(F\\) be any continuous cdf we are interested in. If we define a new random variable \\(X=F^{-1}(U)\\), then \\(X\\sim F\\).\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFor simplicity, assume \\(F\\) is smooth and invertible. Things still work if it isn’t, but it makes the math a little cleaner. Furthermore, recall the cdf of the standard uniform \\(U\\sim\\text{Unif}(0,\\, 1)\\): \\[\n    F_U(x)=\\begin{cases}\n        0 & x \\leq 0\\\\\n        x & 0&lt;x&lt;1\\\\\n        1 & 1 \\leq x.\n    \\end{cases}\n\\]\n\\(X\\) is just a transformation of \\(U\\), so we can apply the cdf method. \\[\n    \\begin{align*}\n        F_X(x)\n        &=\n        P(X\\leq x)\n        \\\\\n        &=\n        P(F^{-1}(U)\\leq x)\n        \\\\\n        &=\n        P(U\\leq F(x))\n        \\\\\n        &=\n        F_U\\left(F(x)\\right)\n        \\\\\n        &=\n        F(x)\n        .\n    \\end{align*}\n\\]\nSo, the cdf of \\(X\\) is literally just \\(F\\).\n\n\n\nNow, in order to actually implement the algorithm, we need to be able to compute the inverse cdf \\(F^{-1}\\). In some cases, this may not be possible, and we must use a different transformation to actually get the job done. But the principle stands. Dig down deep enough into any simulation algorithm, and it’s just simulating uniforms and messing with them.\nTask 1\nConsider a random variable \\(X\\) with this density:\n\\[\nf(x) = \\frac{e^{-x}}{(1+e^{-x})^2},\\quad x\\in\\mathbb{R}.\n\\]\n\nCompute the cdf;\nCompute the inverse cdf;\nUse inverse transform sampling to generate n = 5000 random numbers from this distribution. Plot a histogram of the random numbers, and overlay a line plot of the original density function we gave you. If you did everything correctly, the histogram and the density should agree.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis is the standard logistic distribution. The CDF is\n\\[\n\\begin{aligned}\nF(x)\n&=\n\\int_{-\\infty}^x\nf(t)\ndt\n\\\\\n&=\n\\int_{-\\infty}^x\n\\frac{e^{-t}}{(1+e^{-t})^2}\ndt\n\\\\\n&=\n\\int_{\\infty}^{e^{-x}}\n\\frac{-du}{(1+u)^2}\n&&\\begin{matrix}u=e^{-t}\\\\ du = -e^{-t}dt\\end{matrix}\n\\\\\n&=\n\\int_{e^{-x}}^{\\infty}\n\\frac{du}{(1+u)^2}\n\\\\\n&=\n\\left[-\\frac{1}{1+t}\\right]_{e^{-x}}^\\infty\n\\\\\n&=\n0 - \\left(-\\frac{1}{1+e^{-x}}\\right)\n\\\\\n&=\n\\frac{1}{1+e^{-x}},&&x\\in\\mathbb{R}.\n\\end{aligned}\n\\]\nInverting it gives\n\\[\n\\begin{aligned}\nu &= \\frac{1}{1+e^{-x}}\\\\\n1+e^{-x} &= \\frac{1}{u}\\\\\ne^{-x} & = \\frac{1}{u} - 1\\\\\n-x &= \\ln\\left(\\frac{1}{u} - 1\\right)\\\\\nx &= -\\ln\\left(\\frac{1}{u} - 1\\right).\n\\end{aligned}\n\\]\nSo the inverse CDF is\n\\[\nF^{-1}(u)=-\\ln\\left(\\frac{1}{u}-1\\right),\\quad 0&lt;u&lt;1.\n\\]\n\nset.seed(152638)\nU &lt;- runif(5000)\nX &lt;- -log((1 / U) - 1)\nhist(X, breaks = \"Scott\", freq = FALSE)\ncurve(exp(-x) / (1 + exp(-x))^2, from = min(X), to = max(X), n = 1000, \n      col = \"red\", lwd = 2, add = TRUE)\n\n\n\n\n\n\n\n\n\n\nTask 2\nConsider a random variable \\(X\\) with this density:\n\\[\nf(x) = xe^{-\\frac{1}{2}x^2},\\quad x\\geq 0.\n\\]\n\nCompute the cdf;\nCompute the inverse cdf;\nUse inverse transform sampling to generate n = 5000 random numbers from this distribution. Plot a histogram of the random numbers, and overlay a line plot of the original density function we gave you. If you did everything correctly, the histogram and the density should agree.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis is the Rayleigh distribution.\n\\[\n\\begin{aligned}\nF(x)\n&=\n\\int_{-\\infty}^x\nf(t)\ndt\n\\\\\n&=\n\\int_{0}^x\nte^{-\\frac{1}{2}t^2}\ndt\n\\\\\n&=\n\\int_{0}^{-x^2/2}\n-e^u\\,du\n&&\n\\begin{matrix}\nu=-t^2/2\\\\\ndu=-t\\,dt\n\\end{matrix}\n\\\\\n&=\n\\int_{-x^2/2}^0\ne^u\\,du\n\\\\\n&=\n[e^u]_{-x^2/2}^0\n\\\\\n&=\n1-e^{-x^2/2},&&x\\geq 0.\n\\end{aligned}\n\\]\nInvert it:\n\\[\n\\begin{aligned}\nu &= 1-e^{-x^2/2}\\\\\ne^{-x^2/2} &= 1 - u\\\\\n-x^2/2&=\\ln(1-u)\\\\\nx^2&=-2\\ln(1-u)\\\\\nx&=\\sqrt{-2\\ln(1-u)}\n\\end{aligned}\n\\]\nSo\n\\[\nF^{-1}(u)=\\sqrt{-2\\ln(1-u)}, \\quad 0&lt;u&lt;1.\n\\]\n\nset.seed(152638)\nU &lt;- runif(5000)\nX &lt;- sqrt(-2*log(1-U))\nhist(X, breaks = \"Scott\", freq = FALSE)\ncurve(x * exp(-0.5 * x^2), from = min(X), to = max(X), n = 1000, \n      col = \"red\", lwd = 2, add = TRUE)\n\n\n\n\n\n\n\n\n\n\nTask 3\nConsider a random variable \\(X\\) with this density:\n\\[\nf(x) = e^{-(x+e^{-x})},\\quad x\\in\\mathbb{R}.\n\\]\n\nCompute the cdf;\nCompute the inverse cdf;\nUse inverse transform sampling to generate n = 5000 random numbers from this distribution. Plot a histogram of the random numbers, and overlay a line plot of the original density function we gave you. If you did everything correctly, the histogram and the density should agree.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis is the standard Gumbel distribution.\n\\[\n\\begin{aligned}\nF(x)\n&=\n\\int_{-\\infty}^x\nf(t)\ndt\n\\\\\n&=\n\\int_{-\\infty}^x\ne^{-(t+e^{-t})}\ndt\n\\\\\n&=\n\\int_{\\infty}^{e^{-x}}\n-e^{-(-\\ln u+u)}\n\\frac{du}{u}\n&&\n\\begin{matrix}\nu=e^{-t}\\\\\ndu=-e^{-t}\\,dt\n\\end{matrix}\n\\\\\n&=\n\\int_{e^{-x}}^\\infty e^{-u}du\n\\\\\n&=\n[-e^{-u}]_{e^{-x}}^\\infty\n\\\\\n&=\n0-(-e^{-e^{-x}})\n\\\\\n&=\ne^{-e^{-x}},&& x\\in\\mathbb{R}.\n\\end{aligned}\n\\]\nInvert it:\n\\[\n\\begin{aligned}\nu &= e^{-e^{-x}}\\\\\n\\ln u&=-e^{-x}\\\\\n-\\ln u &=e^{-x}\\\\\n\\ln(-\\ln u)&=-x\\\\\n-\\ln(-\\ln u)&=x.\n\\end{aligned}\n\\]\nSo\n\\[\nF^{-1}(u)=-\\ln(-\\ln u),\\quad 0&lt;u&lt;1.\n\\]\n\nset.seed(152638)\nU &lt;- runif(5000)\nX &lt;- -log(-log(U))\nhist(X, breaks = \"Scott\", freq = FALSE)\ncurve(exp(-x-exp(-x)), from = min(X), to = max(X), n = 1000, \n      col = \"red\", lwd = 2, add = TRUE)\n\n\n\n\n\n\n\n\n\n\nTask 4\nConsider a random variable \\(X\\) with this density:\n\\[\nf(x) = \\frac{1}{2}e^{-|x|},\\quad x\\in\\mathbb{R}.\n\\]\n\nCompute the cdf;\nCompute the inverse cdf;\nUse inverse transform sampling to generate n = 5000 random numbers from this distribution. Plot a histogram of the random numbers, and overlay a line plot of the original density function we gave you. If you did everything correctly, the histogram and the density should agree.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis is the standard Laplace distribution. For \\(x&lt;0\\),\n\\[\n\\begin{aligned}\nF(x)\n&=\n\\int_{-\\infty}^x\n\\frac{1}{2}e^{-|t|}\\,\\text{d}t\n\\\\\n&=\n\\frac{1}{2}\n\\int_{-\\infty}^x\ne^{-(-t)}\\,\\text{d}t\n\\\\\n&=\n\\frac{1}{2}\n\\int_{-\\infty}^x\ne^{t}\\,\\text{d}t\n\\\\\n&=\n\\frac{1}{2}\n[e^t]_{-\\infty}^x\n\\\\\n&=\n\\frac{1}{2} e^x.\n\\end{aligned}\n\\]\nFor \\(x\\geq 0\\),\n\\[\n\\begin{aligned}\nF(x)&=P(X\\leq x)\n\\\\\n&=1-P(X&gt;x)\n\\\\\n&=1-\\int_x^\\infty\n\\frac{1}{2}e^{-|t|}\\,\\text{d}t\n\\\\\n&=\n1-\\frac{1}{2}\\int_x^\\infty\ne^{-t}\\,\\text{d}t\n\\\\\n&=\n1-\\frac{1}{2}[-e^{-t}]_x^\\infty\n\\\\\n&=\n1-\\frac{1}{2}e^{-x}.\n\\end{aligned}\n\\]\nSo\n\\[\n\\begin{aligned}\nF(x)\n&=\n\\begin{cases}\n\\frac{1}{2} e^x & x&lt;0\\\\\n1-\\frac{1}{2} e^{-x} & x\\geq 0.\n\\end{cases}\n\\end{aligned}\n\\]\nIf you invert that darn thing, you get:\n\\[\n\\begin{aligned}\nF^{-1}(u)=\\text{sign}(u-0.5)\\ln(1-2|u-0.5|),\\quad 0&lt;u&lt;1.\n\\end{aligned}\n\\]\n\nset.seed(152638)\nU &lt;- runif(5000)\nX &lt;- sign(U - 0.5) * log(1 - 2 * abs(U - 0.5))\nhist(X, breaks = \"Scott\", freq = FALSE)\ncurve(0.5 * exp(-abs(x)), from = min(X), to = max(X), n = 1000, \n      col = \"red\", lwd = 2, add = TRUE)\n\n\n\n\n\n\n\n\n\n\nFurther reading\nI love computer simulation. It’s gorgeous. Here are two great books:\n\nDevroye, Luc (1986): Non-uniform Random Variate Generation (free pdf from the author);\nRobert, Christian and George Casella (2004): Monte Carlo Statistical Methods (free pdf through Duke).",
    "crumbs": [
      "Labs",
      "Lab 8"
    ]
  },
  {
    "objectID": "labs/lab-10.html",
    "href": "labs/lab-10.html",
    "title": "Lab 10",
    "section": "",
    "text": "This lab provides extra practice with the kind of maximum likehood problem that you will encounter on the last problem set and the final exam.\nTask 1\nIf a non-negative random variable \\(X\\) has the Rayleigh distribution with parameter \\(\\theta&gt;0\\), then its CDF is\n\\[\nF(x;\\,\\theta)=1-\\exp\\left(-\\frac{x^2}{2\\theta}\\right)\\quad x\\geq 0,\n\\]\nand we say \\(X\\sim\\text{Rayleigh}(\\theta)\\). What is the PDF in this parametric family?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\[\n\\begin{aligned}\nf(x;\\,\\theta)\n&=\nF'(x;\\,\\theta)\n\\\\\n&=\n\\frac{\\text{d}}{\\text{d}x}\n\\left[\n1-\\exp\\left(-\\frac{x^2}{2\\theta}\\right)\n\\right]\n\\\\\n&=\n0-\\frac{-x}{\\theta}\n\\exp\\left(-\\frac{x^2}{2\\theta}\\right)\n\\\\\n&=\n\\frac{x}{\\theta}\n\\exp\\left(-\\frac{x^2}{2\\theta}\\right),&&x\\geq 0.\n\\end{aligned}\n\\]\n\n\n\nTask 2\nImagine we observe data\n\\[\nX_1,\\,X_2,\\,...,\\,X_n\\overset{\\text{iid}}{\\sim}\\text{Rayleigh}(\\theta),\n\\]\nwhere the true value of the parameter \\(\\theta&gt;0\\) is unknown. Use the method of maximum likelihood to propose an estimator for \\(\\theta\\).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe likelihood function is\n\\[\n\\begin{aligned}\nL(\\theta;\\,X_{1:n})\n&=\n\\prod_{i=1}^n\nf(X_i;\\,\\theta)\n\\\\\n&=\n\\prod_{i=1}^n\n\\frac{X_i}{\\theta}\n\\exp\\left(-\\frac{X_i^2}{2\\theta}\\right)\n\\\\\n&=\n\\theta^{-n}\\left(\\prod_{i=1}^nX_i\\right)\\left[\\prod_{i=1}^n\\exp\\left(-\\frac{X_i^2}{2\\theta}\\right)\\right]\n\\\\\n&=\n\\theta^{-n}\\left(\\prod_{i=1}^nX_i\\right)\\exp\\left(-\\frac{1}{2\\theta}\\sum\\limits_{i=1}^nX_i^2\\right).\n\\end{aligned}\n\\]\nThe log-likelihood function is\n\\[\n\\begin{aligned}\n\\ell(\\theta;\\,X_{1:n})\n&=\n\\ln L(\\theta;\\,X_{1:n})\n\\\\\n&=\n\\ln\\left[\\theta^{-n}\\left(\\prod_{i=1}^nX_i\\right)\\exp\\left(-\\frac{1}{2\\theta}\\sum\\limits_{i=1}^nX_i^2\\right)\\right]\n\\\\\n&=\n\\ln(\\theta^{-n})+\\ln\\left(\\prod_{i=1}^nX_i\\right)+\\ln\\left[\\exp\\left(-\\frac{1}{2\\theta}\\sum\\limits_{i=1}^nX_i^2\\right)\\right]\n\\\\\n&=\n-n\\ln \\theta+\\sum\\limits_{i=1}^n\\ln X_i-\\frac{1}{2\\theta}\\sum\\limits_{i=1}^nX_i^2\n.\n\\end{aligned}\n\\]\nTo maximize this, we take the second derivative, set it equal to zero, and solve:\n\\[\n\\begin{aligned}\n\\frac{\\text{d}}{\\text{d}\\theta}\\ell(\\theta;\\,X_{1:n})\n=\n-\n\\frac{n}{\\theta}\n+\n\\frac{1}{2\\theta^2}\n\\sum\\limits_{i=1}^nX_i^2\n&=\n0\n\\\\\n\\frac{1}{2\\theta^2}\n\\sum\\limits_{i=1}^nX_i^2\n&=\n\\frac{n}{\\theta}\n\\\\\n\\frac{1}{2n}\n\\sum\\limits_{i=1}^nX_i^2\n&=\n\\theta.\n\\end{aligned}\n\\]\nSo the maximum likelihood estimator for the Rayleigh distribution is\n\\[\n\\hat{\\theta}_n=\\frac{1}{2n}\n\\sum\\limits_{i=1}^nX_i^2.\n\\]\n\n\n\nTask 3\nYour estimator in the previous part is a random variable that inherits its randomness from the \\(X_i\\). Derive the full sampling distribution of your estimator.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFor these problems, I work “from the inside out.” What’s the distribution of \\(X_i\\)? What’s the distribution of \\(X_i^2\\)? What’s the distribution when you add those up? What’s the distribution when you scale the sum by \\(1/2n\\)? To answer those questions, you need tools like the change-of-variables formulas and our MGF updating rules for linear combinations.\nWe already know that \\(X_1\\sim\\text{Rayleigh}(\\theta)\\), so we derive the distribution of \\(Y_1=g(X_1)=X_1^2\\) using change of variables. Since \\(\\text{Range}(X_1)=(0,\\,\\infty)\\), the transformation is strictly increasing, so we apply the change of variables formula:\n\\[\n\\begin{aligned}\nf_{Y_1}(y)\n&=\nf\\left(g^{-1}(y);\\,\\theta\\right)\\left|\\frac{\\text{d}}{\\text{d}y}g^{-1}(y)\\right|\n\\\\\n&=\nf\\left(y^{1/2};\\,\\theta\\right)\\left|\\frac{\\text{d}}{\\text{d}y}y^{1/2}\\right|\n\\\\\n&=\n\\frac{y^{1/2}}{\\theta}\n\\exp\\left(-\\frac{(y^{1/2})^2}{2\\theta}\\right)\n\\left|\\frac{1}{2}y^{-1/2}\\right|\n\\\\\n&=\n\\frac{y^{1/2}y^{-1/2}}{2\\theta}\n\\exp\\left(-\\frac{y}{2\\theta}\\right)\n\\\\\n&=\n\\frac{1}{2\\theta}\n\\exp\\left(-\\frac{y}{2\\theta}\\right)\n,&&y&gt;0.\n\\end{aligned}\n\\]\nThis is the density of \\(\\text{Exponential}(1/2\\theta)\\), otherwise known as \\(\\text{Gamma}(1,\\,1/2\\theta)\\). Given this, we have the following:\n\\[\n\\begin{aligned}\nX_i\\overset{\\text{iid}}{\\sim}\\text{Rayleigh}(\\theta)\n\\quad\n&\\implies\n\\quad\nX_i^2\\overset{\\text{iid}}{\\sim}\\text{Gamma}(1,\\,1/2\\theta)\n\\\\\n&\\implies\n\\sum\\limits_{i=1}^nX_i^2\\sim\\text{Gamma}(n,\\,1/2\\theta)\n\\\\\n&\\implies\n\\frac{1}{2n}\\sum\\limits_{i=1}^nX_i^2\\sim\\text{Gamma}(n,\\,n/\\theta).\n\\end{aligned}\n\\] The last implication follows from Problem Set 5 #10d. So, the full sampling distribution of the estimator is\n\\[\n\\hat{\\theta}_n\\sim \\text{Gamma}(n,\\,n/\\theta).\n\\]\n\n\n\nTask 4\nNow that you know the sampling distribution of the estimator, compute its mean squared error (MSE) using the bias-variance decomposition. Is the estimator unbiased? Is it consistent?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUsing the mean and variance formulas for the gamma distribution, we see that\n\\[\n\\begin{aligned}\nE(\\hat{\\theta}_n)&=\\frac{n}{n/\\theta}=\\theta\\\\\n\\text{var}(\\hat{\\theta}_n)&=\\frac{n}{(n/\\theta)^2}=\\frac{\\theta^2}{n}.\n\\end{aligned}\n\\]\nSince \\(E(\\hat{\\theta}_n)=\\theta\\), the bias of the estimator is zero, and so the mean-squared-error is simply \\(\\text{var}(\\hat{\\theta}_n)=\\theta^2/n\\). As \\(n\\to\\infty\\), this goes to zero. As such, the estimator is unbiased and consistent.\n\n\n\nTask 5\nCompute the quantile function of the Rayleigh distribution and use it to generate 100 draws from the distribution that has \\(\\theta=4\\).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe quantile function is the inverse cdf, so we solve:\n\\[\n\\begin{aligned}\ny\n&=\n1-\\exp\\left(-\\frac{x^2}{2\\theta}\\right)\n\\\\\n\\exp\\left(-\\frac{x^2}{2\\theta}\\right)\n&=\n1-y\n\\\\\n-\\frac{x^2}{2\\theta}\n&=\n\\ln(1-y)\n\\\\\nx^2\n&=\n-2\\theta\\ln(1-y)\n\\\\\nx\n&=\n\\sqrt{-2\\theta\\ln(1-y)}\n.\n\\end{aligned}\n\\]\nSo \\(F^{-1}(y;\\,\\theta)=\\sqrt{-2\\theta\\ln(1-y)}\\). As such, we can implement an inverse transform sampler to simulate from the distribution:\n\nset.seed(2345)\nn &lt;- 100\ntheta &lt;- 4\nX &lt;- sqrt(-2 * theta * log(1 - runif(n)))\nhist(X, breaks = \"Scott\", freq = FALSE)\ncurve(exp(-x^2/(2*theta)) * x / theta, \n      from = 0, to = max(X), n = 1000, col = \"red\", add = TRUE)\n\n\n\n\n\n\n\n\n\n\nTask 6\nWrite a for loop that simulates the sampling distribution for your estimator. Here is a schematic of what you are doing:\n\\[\n\\begin{matrix}\n\\text{0. True parameter value} &&& \\theta_0 && \\\\\n&&& \\downarrow && \\\\\n\\text{1. True distribution} &&& \\text{Rayleigh}(\\theta_0) && \\\\\n&\\swarrow &\\swarrow& \\cdots &\\searrow&\\searrow \\\\\n\\text{3. Simulated data}&x_{1:n}^{(1)} &x_{1:n}^{(2)}& \\cdots &x_{1:n}^{(M-1)}&x_{1:n}^{(M)} \\\\\n&\\downarrow &\\downarrow& \\cdots &\\downarrow&\\downarrow \\\\\n\\text{4. Different estimates}&\\hat{\\theta}_n^{(1)} &\\hat{\\theta}_n^{(2)}& \\cdots &\\hat{\\theta}_n^{(M-1)}&\\hat{\\theta}_n^{(M)} \\\\\n&\\searrow &\\searrow& \\cdots &\\swarrow&\\swarrow \\\\\n& && \\text{Histogram} && \\\\\n\\end{matrix}\n\\]\nAt the end, add on top of the histogram a curve of the density that you derived in Task 3. They ought to match.\n\n\n\n\n\n\nTemplate\n\n\n\nHere is some template code you can fill in:\n\ntrue_theta &lt;- 3.14      # true value of parameter\nn &lt;- 25                 # sample size\nM &lt;- 2500               # how many repetitions\nestimates &lt;- numeric(M) # preallocate storage for the sample of estimates\n\nfor(m in 1:M){\n  # Step 1: simulate a new fake dataset of size n\n  # Step 2: apply your formula from Task 2 to compute new estimate\n  # Step 3: store it\n}\n\n# Step 4: plot histogram of estimates\n# Step 5: add density curve of exact sampling distribution\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nset.seed(123)\ntrue_theta &lt;- 3.14      # true value of parameter\nn &lt;- 25                 # sample size\nM &lt;- 2500               # how many repetitions\nestimates &lt;- numeric(M) # preallocate storage for the sample of estimates\n\nfor(m in 1:M){\n  X &lt;- sqrt(-2 * true_theta * log(1 - runif(n)))\n  estimates[m] &lt;- sum(X^2) / (2*n)\n}\n\nhist(estimates, breaks = \"Scott\", freq = FALSE)\ncurve(dgamma(x, shape = n, rate = n / true_theta), \n      from = 0, to = max(estimates), n = 1000, col = \"red\", \n      lwd = 2, add = TRUE)",
    "crumbs": [
      "Labs",
      "Lab 10"
    ]
  },
  {
    "objectID": "lecture-notes/sta240-notes-18-mle-examples.html",
    "href": "lecture-notes/sta240-notes-18-mle-examples.html",
    "title": "Maximum likelihood estimation",
    "section": "",
    "text": "Below are two complete, worked examples of maximum likelihood problems. You also have the solutions to Lab 10. The final exam will feature a problem with exactly the same format as these three examples, so study them carefully. The steps are as follows:\n\nCompute the (log-)likelihood function;\nCompute the maximum likelihood estimator;\nDerive the exact sampling distribution of the estimator;\nDerive the mean-squared-error (MSE) of the estimator and identify its statistical properties (biased? consistent?).\n\nStep 1 requires you to be fluent with algebraic manipulations from pre-calculus, especially your log and exponent properties. Step 2 requires you to be fluent in your derivative rules. Step 3 requires you to to be fluent with change-of-variables and sums and averages of iid random variables. As such, these problems synthesize a lot of the technical skills that you will need in future statistical theory courses. Furthermore, the steps are cumulative. You can’t do Step 3 correctly if you muck up Steps 1 and 2. So, to ace the final, you need to get your arms around the entire process.\n\nExample: exponential distribution\nConsider\n\\[\nX_1,\\,X_2,\\,...,\\,X_n\\overset{\\text{iid}}{\\sim}\\text{Exponential}(\\theta).\n\\]\nRecall that \\(\\text{Exponential}(\\theta)\\) is the same as \\(\\text{Gamma}(1,\\,\\theta)\\) and the density of the distribution is\n\\[\nf(x;\\,\\theta)=\\theta e^{-\\theta x},\\quad x&gt;0.\n\\]\nSo the likelihood function is\n\\[\n\\begin{aligned}\nL(\\theta;\\,X_{1:n})\n&=\n\\prod_{i=1}^n\nf(X_i;\\,\\theta)\n\\\\\n&=\n\\prod_{i=1}^n\n\\theta e^{-\\theta X_i}\n\\\\\n&=\n\\theta^n\n\\prod_{i=1}^n\ne^{-\\theta X_i}\n\\\\\n&=\n\\theta^n\ne^{-\\theta\\sum\\limits_{i=1}^nX_i},\n\\end{aligned}\n\\]\nand the log-likelihood function is\n\\[\n\\begin{aligned}\n\\ell(\\theta;\\,X_{1:n})\n&=\n\\ln L(\\theta;\\,X_{1:n})\n\\\\\n&=\n\\ln\n\\left[\n\\theta^n\ne^{-\\theta\\sum\\limits_{i=1}^nX_i}\n\\right]\n\\\\\n&=\n\\ln \\theta^n\n+\n\\ln\ne^{-\\theta\\sum\\limits_{i=1}^nX_i}\n\\\\\n&=\nn\\ln \\theta\n-\\theta\\sum\\limits_{i=1}^nX_i.\n\\end{aligned}\n\\]\nWe optimize this by taking the derivative with respect to \\(\\theta\\), setting it equal to zero, and solving:\n\\[\n\\begin{aligned}\n\\frac{\\text{d}\\ell}{\\text{d}\\theta}=\\frac{n}{\\theta}-\\sum\\limits_{i=1}^nX_i&=0\\\\\n\\frac{n}{\\theta}&=\\sum\\limits_{i=1}^nX_i\\\\\n\\frac{n}{\\sum\\limits_{i=1}^nX_i}&=\\theta.\n\\end{aligned}\n\\]\nSo the maximum likelihood estimator for the exponential distribution is\n\\[\n\\hat{\\theta}_n=\\frac{n}{\\sum\\limits_{i=1}^nX_i}=\\frac{1}{\\bar{X}_n}.\n\\]\nThis is a random variable because it depends on the \\(X_i\\), which are random. We know that \\(X_i\\overset{\\text{iid}}{\\sim}\\text{Gamma}(1,\\,\\theta)\\) and so we know from lecture that \\(\\bar{X}_n\\sim\\text{Gamma}(n,\\,n\\theta)\\). Consequently, the distribution of \\(\\hat{\\theta}_n=1/\\bar{X}_n\\) is inverse gamma, which you derived on Problem Set 6 #6d.\nSince the exact sampling distribution of the estimator is \\(\\hat{\\theta}_n\\sim\\text{IG}(n,\\,n\\theta)\\), we know that the mean and variance are\n\\[\n\\begin{aligned}\nE(\\hat{\\theta}_n)\n&=\n\\frac{n\\theta}{n-1}\n\\\\\n\\text{var}(\\hat{\\theta}_n)\n&=\n\\frac{n^2\\theta^2}{(n-1)^2(n-2)}.\n\\end{aligned}\n\\]\nThis implies that the bias of the estimator is\n\\[\nE(\\hat{\\theta}_n)-\\theta=\\frac{n\\theta}{n-1}-\\theta=\\frac{n\\theta-(n-1)\\theta}{n-1}=\\frac{n\\theta-n\\theta+\\theta}{n-1}=\\frac{\\theta}{n-1}&gt;0.\n\\]\nSo this estimator is biased upward. On average, it is an overestimate. Finally, the full MSE is\n\\[\n\\begin{aligned}\n\\text{MSE}\n&=\n\\text{bias}^2+\\text{variance}\n\\\\\n&=\n\\left[\nE(\\hat{\\theta}_n)-\\theta\n\\right]^2\n+\n\\text{var}(\\hat{\\theta}_n)\n\\\\\n&=\n\\frac{\\theta^2}{(n-1)^2}\n+\n\\frac{n^2\\theta^2}{(n-1)^2(n-2)}.\n\\end{aligned}\n\\]\nAs \\(n\\to\\infty\\), this goes to zero, so the estimator is consistent.\n\n\nExample: Midterm 2 strikes again!\nConsider an iid collection of random variables \\(X_1\\), \\(X_2\\), …, \\(X_n\\) belonging to the family that has this density:\n\\[\nf(x;\\,\\theta)=\\theta(x+1)^{-(\\theta+1)},\\quad x&gt;0.\n\\]\nThis is a special case of the family you studied on Problem Set 6 #1, and you also worked with it on Midterm 2.\nThe likelihood function is\n\\[\n\\begin{aligned}\nL\\left(\\theta; X_{1:n}\\right) & = \\prod_{i = 1}^{n}{f\\left(X_{i} ;\\, \\theta\\right)} \\\\ & = \\prod_{i = 1}^{n}{\\theta\\left(X_{i} + 1\\right)^{-(\\theta + 1)}} \\\\ & = \\theta^{n}\\prod_{i = 1}^{n}{\\left(X_{i} + 1\\right)^{-(\\theta + 1)}} \\\\ & = \\theta^{n}\\left(\\prod_{i = 1}^{n}{\\left(X_{i} + 1\\right)}\\right)^{-(\\theta + 1)}.\n\\end{aligned}\n\\]\nThe log-likelihood function is\n\\[\n\\begin{aligned}\n\\ell\\left(\\theta;\\, X_{1:n}\\right) & = \\ln L\\left(\\theta; X_{1:n}\\right) \\\\ & = \\ln\\left(\\theta^{n}\\left(\\prod_{i = 1}^{n}{\\left(X_{i} + 1\\right)}\\right)^{-(\\theta + 1)}\\right) \\\\ & = \\ln\\left(\\theta^{n}\\right) + \\ln\\left(\\left(\\prod_{i = 1}^{n}{\\left(X_{i} + 1\\right)}\\right)^{-(\\theta + 1)}\\right) \\\\ & = n  \\ln\\theta - (\\theta + 1)\\sum_{i = 1}^{n}{\\ln\\left(X_{i} + 1\\right)}.\n\\end{aligned}\n\\]\nTo compute the estimator, we take the derivative with respect to \\(\\theta\\), set it equal to zero, and solve:\n\\[\n\\begin{aligned}\n\\frac{\\text{d}\\ell}{\\text{d}\\theta}  &= \\frac{\\text{d}}{\\text{d}\\theta}\\left[n \\ln\\theta - (\\theta + 1)\\sum_{i = 1}^{n}{\\ln\\left(X_{i} + 1\\right)}\\right]  = \\frac{n}{\\theta} - \\sum_{i = 1}^{n}{\\ln\\left(X_{i} + 1\\right)}\n=0\n\\\\\n\\\\\n&\\implies \\quad \\hat{\\theta}_n=\\frac{n}{\\sum\\limits_{i=1}^n\\ln(X_i+1)}.\n\\end{aligned}\n\\]\nWe know from Midterm 2 that \\(\\ln(X_i+1)\\overset{\\text{iid}}{\\sim}\\text{Gamma}(1,\\,\\theta)\\), so \\(\\sum_{i=1}^n\\ln(X_i+1)\\sim\\text{Gamma}(n,\\,\\theta)\\), and \\((1/n)\\sum_{i=1}^n\\ln(X_i+1)\\sim\\text{Gamma}(n,\\,n\\theta)\\). As such, \\(\\hat{\\theta}_n\\sim\\text{IG}(n,\\,n\\theta)\\). This is the same sampling distribution we got in the previous example, so from here the analysis is the same; the estimator is biased upward but consistent."
  },
  {
    "objectID": "lecture-notes/sta240-notes-16-stats.html",
    "href": "lecture-notes/sta240-notes-16-stats.html",
    "title": "Introduction to the theory of statistics",
    "section": "",
    "text": "Imagine you work at a hospital. Dozens of babies are delivered in the hospital each week. A lot of information is collected about each birth, and hiding inside of these data is a lot of useful information about the health and future of the community. How can we extract it? If you take classes like STA 101 or 199, you learn various methods:\n\n\n\n\n\n\n\n\nQuestion\nMethod\n\n\n\n\nHow are birth weights distributed?\nhistogram\n\n\nWhat’s the typical birth weight?\nsample average\n\n\nHow is birth weight related to gestational age?\nlinear regression\n\n\n\n\nThe goal of data science in general is to convert data into knowledge, and we perform this “conversion” by applying statistical methods: histogram, sample average, line of best fit, etc.\nA mathematical statistician looks at these methods and wonders: do they actually work? Do they behave the way we expect? Do they deliver what they promise? Are they reliable? How reliable? Under what conditions? And before we can answer these questions, we have to back up and ask a more fundamental one: what does it even mean for a statistical method to “work”? These are all theoretical questions, and mathematical statisticians answer them using the tools of probability theory that we have studied for the last twelve weeks.\n\nBaby’s first dataset\nFor us, a data set will be a spreadsheet with one column of numbers:\nXXX\nOf course, in the modern era, datasets are huge. There could be millions of columns. There could be so many columns and rows that you can’t fit the entire dataset on a single computer. Welcome to “big data.” Furthermore, modern datasets are weird. It’s not just a box of numbers anymore. Text is data. Images are data. Video is data. Sometimes all at once.\nIf you continue studying statistics, you’ll get there eventually. But for now, let’s keep it simple.\n\n\nThe implicit assumption of all statistics\n\nObsered data are the result of a random process.\n\nBy the time the data arrive in our spreadsheet, they are a fixed set of numbers. But how did those numbers get there? Plenty of seemingly random forces might have had their way before you ever got to observe anything:\n\nnature’s randomness:\nhuman error:\nmeasurement error:\ndeliberate error\n\n\nStatisticians have found that it is convenient to use the tolls and language of probability to get a handle on the variation that we observe in real-world data.\nThis assumption is so foundational to statistics that it often goes unremarked upon and recedes into the background, but it is indeed an assumption. Not every set of numbers you would seek to extract patterns from is random, but this is the first assumption a statistician makes.\nBy the time data arrive in our spreadsheet, they are a fixed set of numbers. But how did they get there?\nWe use the language and tools of probability to model variation. Even if the variation in our datasets is not literally the result of random forces, it may be convenient to proceed as if it is.\n\n\nOur mathematical model of data analysis\nagain, iid is an assumption, and it will often be bogus. But we’re just starting out. If you can’t get a handle on the simple case,\n\n\nClassical statistical inference\n\n\nParametric statistics\nIn this class, our data will be numerical.\nYada yada data science yada yada extract patterns and learn about the world.\ngoal: typical behavior - mean goal: distribution - plot histogram goal: learn relationships - line of best fit\nThese are all examples of statistical methods. A theoretical statistician\nDo these methods actually work? Do they do what we want them to do? Do they deliver what they promise to deliver? Under what conditions? And before we can answer those questions, we have to back up and ask a more fundamental one: what does it even mean for a statistical method to “work”? These are all theoretical questions, and mathematical statisticians use the tools of probability theory to answer them.\nIn American government, there are legislative, judicial, and executive branches. In music, there is melody, harmony, and rhythm. Similarly, we organize statistical theory into three related areas of inquiry:\n\n(point estimation)\n(interval estimation)\n(hypothesis testing) can the data distinguish between\n\n\n\nPoint estimation\n\n\n\n\n\n\nDefinition: risk of an estimator\n\n\n\n\\[\nE[L(\\hat{\\theta}_n,\\,\\theta_0)]\n\\]\n\n\n\n\n\n\n\n\nTheorem: bias-variance trade-off"
  },
  {
    "objectID": "lecture-notes/sta240-notes-15-clt.html",
    "href": "lecture-notes/sta240-notes-15-clt.html",
    "title": "Central limit theorem",
    "section": "",
    "text": "In the last lecture, we studied iid collections of several random variables:\n\\[\nX_1,\\,X_2,\\,X_3,\\,X_4,\\,...\\overset{\\text{iid}}{\\sim}P.\n\\]\nBecause these are identically distributed, we introduced notation for the common mean \\(\\mu=E(X_1)\\), common variance \\(\\sigma^2=\\text{var}(X_1)\\), and common moment-generating function \\(M(t)=E(e^{tX_1})\\) shared by all \\(X_i\\) in the collection. Obviously, we are assuming these objects exist to begin with, which is not always the case (think Cauchy).\nBased on the iid collection, we defined the running sum and the running average of the first \\(n\\) variables:\n\\[\n\\begin{aligned}\nS_n&=\\sum\\limits_{i=1}^n X_i\\\\\n\\bar{X}_n&=\\frac{1}{n}\\sum\\limits_{i=1}^n X_i=\\frac{1}{n}S_n.\n\\end{aligned}\n\\]\nBecause these depend on the \\(X_i\\), \\(S_n\\) and \\(\\bar{X}_n\\) are themselves random variables with their own distributions, and we sought to itemize their properties: mean, variance, full distribution, etc. Here’s what we came up with:\n\\[\n\\begin{aligned}\nE(S_n)&=n\\mu\\\\\nE(\\bar{X}_n)&=\\mu\\\\\n\\\\\n\\text{var}(S_n)&=n\\sigma^2\\\\\n\\text{var}(\\bar{X}_n)&=\\sigma^2/n\\\\\n\\\\\nM_{S_n}(t)&=M(t)^n\\\\\nM_{\\bar{X}_n}(t)&=M(t/n)^n.\\\\\n\\end{aligned}\n\\]\nIn the case of the mean and the variance, we have generic formulas that work for any \\(P\\). If we want to say what the exact distribution is, then we need to know what the original \\(P\\) was. Using those updating formulas for the MGF, we worked several special cases:\n\\[\n\\begin{aligned}\nP=\\text{Bern}(p) \\quad & \\implies &&S_n\\sim\\text{Binom}(n,\\,p)\\\\\n\\\\\nP=\\text{Poisson}(\\lambda) \\quad & \\implies &&S_n\\sim\\text{Poisson}(n\\lambda)\\\\\n\\\\\nP=\\text{Gamma}(\\alpha,\\,\\beta) \\quad & \\implies &&S_n\\sim\\text{Gamma}(n\\alpha,\\,\\beta)\\\\\n\\quad & \\implies &&\\bar{X}_n\\sim\\text{Gamma}(n\\alpha,\\,n\\beta)\\\\\n\\\\\nP=\\text{N}(\\mu,\\,\\sigma^2) \\quad & \\implies &&S_n\\sim\\text{N}(n\\mu,\\,n\\sigma^2)\\\\\n\\quad & \\implies &&\\bar{X}_n\\sim\\text{N}(\\mu,\\,\\sigma^2/n)\\\\\n\\end{aligned}\n\\]\nNeat! Now, here’s the $\\(10^6\\) question. What happens as \\(n\\to\\infty\\)? As you average more and more of these iid random variables, what happens to the distribution? Let’s see:\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 500\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  titlePanel(\"What happens as you average more and more iid random variables?\"),\n  \n  sidebarLayout(\n    sidebarPanel(\n      selectInput(\"dist\", \"Choose an underlying distribution P:\",\n                  choices = c(\"Bernoulli\", \"Poisson\", \"Gamma\")),\n      \n      # The sliders will appear here:\n      uiOutput(\"param_ui\")\n    ),\n    \n    mainPanel(\n      plotOutput(\"distPlot\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n  \n  # Dynamically choose which sliders to show\n  output$param_ui &lt;- renderUI({\n    req(input$dist)\n    \n    if (input$dist == \"Bernoulli\") {\n      tagList(\n        sliderInput(\"n\", \"Sample size (n):\", min = 1, max = 200, value = 1, step = 1),\n        sliderInput(\"p\", \"Probability of success (p):\", min = 0, max = 1, value = 0.5, step = 0.01)\n      )\n    } else if (input$dist == \"Poisson\") {\n      tagList(\n        sliderInput(\"n\", \"Sample size (n):\", min = 1, max = 200, value = 1, step = 1),\n        sliderInput(\"rate\", \"Rate parameter (λ):\", min = 0, max = 5, value = 1, step = 0.1)\n      )\n    } else if (input$dist == \"Gamma\") {\n      tagList(\n        sliderInput(\"n\", \"Sample size (n):\", min = 1, max = 200, value = 1, step = 1),\n        sliderInput(\"a\", \"Shape parameter (α):\", min = 0, max = 5, value = 1, step = 0.1),\n        sliderInput(\"b\", \"Rate parameter (β):\", min = 0, max = 5, value = 1, step = 0.1)\n      )\n    }\n  })\n  \n  # Generate a plot from the chosen distribution\n  output$distPlot &lt;- renderPlot({\n    if (input$dist == \"Bernoulli\") {\n      n &lt;- input$n\n      p &lt;- input$p\n      #par(mfrow = c(1, 2))\n      #plot(0:n, dbinom(0:n, n, p), type = \"h\")\n      plot((0:n) / n, dbinom(0:n, n, p), type = \"h\", xlab = expression(bar(x)),\n           ylab = expression(\"P(\" ~ bar(X)[n] ~ \" = \" ~ bar(x) ~ \")\"),\n           main = expression(\"Average of iid\" ~ X[i] ~ \"~ Bernoulli(p)\"))\n      mtext(\"μ\", side = 1, at = p, col = \"red\", line = 2)\n    } else if (input$dist == \"Poisson\") {\n      n &lt;- input$n\n      rate &lt;- input$rate\n      Kmax &lt;- ceiling(qpois(0.99, 200 * rate))\n      #par(mfrow = c(1, 2))\n      plot(0:Kmax / n, dpois(0:Kmax, n * rate), type = \"h\", xlab = expression(bar(x)),\n           ylab = expression(\"P(\" ~ bar(X)[n] ~ \" = \" ~ bar(x) ~ \")\"),\n           xlim = c(0, qpois(0.99, rate)),\n           main = expression(\"Average of iid\" ~ X[i] ~ \"~ Poisson(λ)\"))\n      mtext(\"μ\", side = 1, at = rate, col = \"red\", line = 2)\n      #plot((0:n) / n, dbinom(0:n, n, p), type = \"h\")\n      \n    } else if (input$dist == \"Gamma\") {\n      n &lt;- input$n\n      a &lt;- input$a\n      b &lt;- input$b\n      #par(mfrow = c(1, 2))\n      #curve(dgamma(x, shape = n * a, rate = b), from = 0, to = 10, n = 1000)\n      curve(dgamma(x, shape = n * a, rate = n * b), from = 0, to = qgamma(0.95, shape = a, rate = b), \n            n = 1000, xlab = expression(bar(x)), ylab = \"density\",\n            main = expression(\"Average of iid\" ~ X[i] ~ \"~ Gamma(α, β)\"))\n      mtext(\"μ\", side = 1, at = a / b, col = \"red\", line = 2)\n    }\n    \n  })\n}\n\nshinyApp(ui, server)\n\n\nThese three examples are based on very different underlying distributions, and yet the final result would seem to be the same:\n\nas you average more and more iid random variables, the distribution looks more and more like a bell curve.\n\nIs this always true? Can this high-level, qualitative observation be restated as a precise mathematical fact? It turns out that the answer is yes, and we call it the central limit theorem. In order to state this theorem, we need to rigorously define what we mean by “the distribution looks more and more like a bell curve.” In other words, we need to define a notion of convergence for random variables and their distributions."
  },
  {
    "objectID": "lecture-notes/sta240-notes-15-clt.html#partial-proof-of-the-clt",
    "href": "lecture-notes/sta240-notes-15-clt.html#partial-proof-of-the-clt",
    "title": "Central limit theorem",
    "section": "Partial proof of the CLT",
    "text": "Partial proof of the CLT\nAssume the \\(X_i\\) all share a common moment-generating function \\(M(t)=E(e^{tX_1})\\). To show that \\(Z_n\\) converges in distribution to N(0, 1), we can appeal to the continuity theorem above and show that the MGF of \\(Z_n\\) converges:\n\\[\n\\lim_{n\\to\\infty}M_{Z_n}(t)=\\underbrace{\\exp\\left(\\frac{1}{2}t^2\\right)}_{\\text{mgf of N(0, 1)}}.\n\\]\nSince the log and exp functions are continuous, this is equivalent to showing that\n\\[\n\\lim_{n\\to\\infty}\\ln M_{Z_n}(t)=\\frac{1}{2}t^2.\n\\]\nWithout loss of generality, we can clean up the calculations by assuming that \\(\\mu=0\\) and \\(\\sigma=1\\). With this, the standardized average becomes\n\\[\nZ_n\n=\n\\frac{\\frac{1}{n}\\sum\\limits_{i=1}^nX_i}{1/\\sqrt{n}}\n=\n\\frac{1}{\\sqrt{n}}\\sum\\limits_{i=1}^nX_i\n=\n\\sum\\limits_{i=1}^n\\frac{1}{\\sqrt{n}}X_i.\n\\] In other words, \\(Z_n\\) is a linear combination of iid random variables, and we can use what we know about iid sums to compute the MGF of \\(Z_n\\):\n\\[\n\\begin{aligned}\nM_{Z_n}(t)\n&=\nE(e^{tZ_n})\n\\\\\n&=\nE\\left(e^{t\\sum\\limits_{i=1}^n\\frac{1}{\\sqrt{n}}X_i}\\right)\n\\\\\n&=\nE\\left(e^{\\sum\\limits_{i=1}^n\\frac{t}{\\sqrt{n}}X_i}\\right)\n\\\\\n&=\nE\\left(\\prod_{i=1}^ne^{\\frac{t}{\\sqrt{n}}X_i}\\right)\n&&e^{a+b}=e^ae^b\n\\\\\n&=\n\\prod_{i=1}^nE\\left(e^{\\frac{t}{\\sqrt{n}}X_i}\\right) && \\text{independence}\n\\\\\n&=\n\\prod_{i=1}^nM_{X_i}(t/\\sqrt{n})\n\\\\\n&=\n\\prod_{i=1}^nM(t/\\sqrt{n}) && \\text{identically distributed}\n\\\\\n&=\n[M(t/\\sqrt{n})]^n\n.\n\\end{aligned}\n\\]\nGreat! Taking the log, we have that\n\\[\n\\ln M_{Z_n}(t)=n\\ln M(t/\\sqrt{n}),\n\\]\nand so the proof is complete if we can show that\n\\[\n\\lim_{n\\to\\infty}n\\ln M\\left(\\frac{t}{\\sqrt{n}}\\right)=\\frac{t^2}{2}.\n\\]\nTo do that, we need to haul out L’Hôpital’s rule a few times. But first, a few reminders:\n\\[\n\\begin{aligned}\nM(0)&=E\\left(e^{0\\cdot X_1}\\right)=E(e^0)=E(1)=1 && \\text{always true}\\\\\nM'(0)&=E(X_1)=0 && \\text{assumption from above}\\\\\nM''(0)&=E(X_1^2)=\\text{var}(X_1)+E(X_1)^2=1+0=1 && \\text{assumption from above}.\n\\end{aligned}\n\\]\nWith that, we’re ready to roll. We start by re-writing the limit using a change-of-variables: \\(y=1/\\sqrt{n}\\), or \\(n=1/y^2\\). If \\(n\\to\\infty\\), that’s the same as \\(y\\to 0\\), so we have:\n\\[\n\\lim_{n\\to\\infty}\\ln M_{Z_n}(t)\n=\n\\lim_{n\\to\\infty}n\\ln M\\left(\\frac{t}{\\sqrt{n}}\\right)\n=\n\\lim_{y\\to0}\\frac{\\ln M\\left(ty\\right)}{y^2}\n=\n\\frac{\\ln M(0)}{0}\n=\n\\frac{\\ln 1}{0}\n=\n\\frac{0}{0}.\n\\]\nSince a naive attempt to simplify the limit by “plugging in” gives an indeterminate form, we apply L’Hôpital’s rule:\n\\[\n\\begin{aligned}\n\\lim_{n\\to\\infty}\\ln M_{Z_n}(t)\n&=\n\\lim_{n\\to\\infty}n\\ln M\\left(\\frac{t}{\\sqrt{n}}\\right)\n\\\\\n&=\n\\lim_{y\\to0}\\frac{\\ln M\\left(ty\\right)}{y^2}\n&& y=\\frac{1}{\\sqrt{n}}\n\\\\\n&\\overset{\\text{L'H}}{=}\n\\lim_{y\\to0}\n\\frac{\\frac{t}{M(ty)}M'(ty)}{2y}\n\\\\\n&=\n\\frac{t}{2}\n\\lim_{y\\to0}\n\\frac{M'(ty)}{yM(ty)}\n&&\\left(\\underbrace{=\n\\frac{t}{2}\n\\frac{M'(0)}{0\\cdot M(0)}\n=\n\\frac{t}{2}\n\\frac{0}{0}}_{\\text{darn!}}\n\\right)\n\\\\\n&\\overset{\\text{L'H}}{=}\n\\frac{t}{2}\n\\lim_{y\\to 0}\n\\frac{tM''(ty)}{tyM'(ty)+M(yt)}\n&&\\text{need L'Hôpital again}\n\\\\\n&=\n\\frac{t^2}{2}\n\\frac{M''(0)}{t\\cdot 0\\cdot M'(0)+M(0)}\n\\\\\n&=\n\\frac{t^2}{2}\n\\frac{1}{0+1}\n\\\\\n&=\n\\frac{t^2}{2}\n.\n\\end{aligned}\n\\]\nDone!\n\n\n\n\n\n\nProblem Set 0 strikes again!\n\n\n\nThe limit we took to finish the proof is basically the same limit you took on Problem Set 0 problem 6. Just take \\(x=n\\) and \\(f=\\ln M\\)."
  },
  {
    "objectID": "lecture-notes/sta240-notes-15-clt.html#footnotes",
    "href": "lecture-notes/sta240-notes-15-clt.html#footnotes",
    "title": "Central limit theorem",
    "section": "Footnotes",
    "text": "Footnotes\n\nWhy is the normal the maximum entropy distribution? Why is the limit actually obtained instead of stopping short somewhere before? Because the proof says so! It’s just turtles all the way down.↩︎"
  },
  {
    "objectID": "lecture-notes/sta240-notes-17-mle-intro.html",
    "href": "lecture-notes/sta240-notes-17-mle-intro.html",
    "title": "Maximum likelihood estimation",
    "section": "",
    "text": "In parametric statistics, we observe iid data from some parametric distribution\n\\[\nX_1,\\,X_2,\\,...,\\,X_n\\overset{\\text{iid}}{\\sim}f_{\\theta}.\n\\]\nThe problem is that we have no clue what the true value of \\(\\theta\\) actually is, so we want to use the observed data to construct a best guess. How do we do this? What do we even mean by “best”? To be concrete, consider data from a normal distribution with unknown mean:\n\\[\nX_1,\\,X_2,\\,...,\\,X_n\\overset{\\text{iid}}{\\sim}\\text{N}(\\theta,\\,1).\n\\]\n\\(\\theta\\) could be any real number, so suffice it to say, there are a lot of possible guesses to choose from. Which is best, and how do we measure that? Every possible value of \\(\\theta\\) gives us a different density curve. Here are three possible guesses:\n\n\n\n\n\n\n\n\nWe can probably agree that \\(\\theta = 2\\) is best guess among these options. Why? Because that curve assigns the highest density to the data we actually saw (the red dots on the number line). Notice that in each picture, the data are the same. The observations are the observations. The numbers in the your spreadsheet don’t change. What changes is our guess at \\(\\theta\\) and the associated curve. The goal of estimation is to search through all possible values of \\(\\theta\\) and find the one that gives the best fitting curve. How do we measure “best fitting”? We look at the density that the curve assigns to all the data points. Revisiting the cartoons above, for a given choice of \\(\\theta\\), each observation \\(X_i\\) has a density value \\(f(X_i;\\,\\theta)\\) associated with it, and this corresponds to the vertical gap on the picture:\n\n\n\n\n\n\n\n\nWe want to pick the parameter and associated curve that makes these vertical gaps as large as possible. To boil everything down to a single number measure of the goodness-of-fit, we look at the product of the vertical gaps:\n\\[\n\\prod_{i=1}^nf(X_i;\\,\\theta)=f(X_1;\\,\\theta)\\times f(X_2;\\,\\theta)\\times...\\times f(X_n;\\,\\theta)\n\\]\nWe look at the product instead of the sum or the max for two reasons. One, for the product to be large, we need the fit for each observation to be decent. If any one of them is zero or very close to zero, it zeros out the whole product, no matter how good the fit is for the others (zero times one hundred and zero times one thousand are both zero). So the product rewards good overall fit for all observations instead of just super good fit for one or two. Second, the product has an important probabilistic interpretation. Because we assume that the data are iid, the joint distribution of all \\(n\\) of the observations is the product of marginals by independence, and the marginals all have the same form by identical distribution:\n\\[\nf(x_1,\\,x_2,\\,...,\\,x_n;\\,\\theta)=\\prod_{i=1}^nf_i(x_i;\\,\\theta)=\\prod_{i=1}^nf(x_i;\\,\\theta).\n\\]\nSo our goodness-of-fit criteria is the joint density of the observed data.\nAs we saw in the cartoons above, if you change your guess of \\(\\theta\\), you change the curve, you change the vertical gaps, and you change the goodness-of-fit. The observed data are not changing, but the guess for \\(\\theta\\) is. For every possible value of \\(\\theta\\), you get a different goodness-of-fit measure (the product of vertical gaps), and you can summarize this with the likelihood function:\n\\[\nL(\\theta;\\,X_{1:n})=\\prod_{i=1}^nf(X_i;\\,\\theta).\n\\]\nIn goes a choice of \\(\\theta\\), and out comes a numerical measure of how well that choice fits the observed data. Play around here to see how the choice of \\(\\theta\\) changes the fit, and how this traces out a curve measuring the quality of the guess:\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 500\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  titlePanel(\"Maximum likelihood by eye\"),\n  \n  sidebarLayout(\n    sidebarPanel(\n      selectInput(\"dist\", \"Choose a distribution family:\",\n                  choices = c(\"Normal (unknown mean)\", \n                              \"Normal (unknown variance)\", \n                              \"Exponential\")),\n      \n      # The sliders will appear here:\n      uiOutput(\"param_ui\")\n    ),\n    \n    mainPanel(\n      plotOutput(\"distPlot\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n  \n  # Dynamically choose which sliders to show\n  output$param_ui &lt;- renderUI({\n    req(input$dist)\n    \n    if (input$dist == \"Normal (unknown mean)\") {\n      tagList(\n        sliderInput(\"theta\", \"Take a guess at θ\", min = -5, max = 5, value = 0, step = 0.01)\n      )\n    } else if (input$dist == \"Normal (unknown variance)\") {\n      tagList(\n        sliderInput(\"theta\", \"Take a guess at θ\", min = 0, max = 10, value = 1, step = 0.01)\n      )\n    } else if (input$dist == \"Exponential\") {\n      tagList(\n        sliderInput(\"theta\", \"Take a guess at θ\", min = 0, max = 6, value = 1, step = 0.01)\n      )\n    }\n  })\n  \n  # Generate a plot from the chosen distribution\n  output$distPlot &lt;- renderPlot({\n    if (input$dist == \"Normal (unknown mean)\") {\n      \n      # =============================================\n      # get the guess\n      # =============================================\n      \n      theta &lt;- input$theta\n      \n      # =============================================\n      # create some fake data\n      # =============================================\n      \n      n &lt;- 10\n      set.seed(123)\n      X &lt;- rnorm(n, mean = sqrt(pi) * 3 / 5, sd = 1.5)\n      \n      # =============================================\n      # plot data density\n      # =============================================\n      \n      par(mfrow = c(1, 2))\n      \n      curve(dnorm(x, mean = theta, sd = 1), \n            from = -6, \n            to = 6, \n            n = 2000, \n            ylab = \"f(x | θ)\", \n            main = \"Density of N(θ, 1)\",\n            bty = \"n\",\n            yaxs = \"i\", \n            xaxt = \"n\",\n            yaxt = \"n\",\n            lwd = 2,\n            xlim = c(-5, 5),\n            ylim = c(-0.05, 1.25 * dnorm(theta, mean = theta, sd = 1)))\n      axis(1, pos = 0)\n      axis(2, at = seq(-0.5, 0.5, by = 0.1))\n      mtext(\"θ\", side = 1, at = theta, line = 1, col = \"blue\")\n      points(theta, dnorm(theta, mean = theta, sd = 1), type = \"h\", col = \"blue\", lwd = 2, lty = 2)\n      points(X, numeric(n), col = \"red\", pch = 19, cex = 0.75)\n      points(X, dnorm(X, mean = theta, sd = 1), type = \"h\", lty = 3, col = \"darkgrey\")\n      points(X, dnorm(X, mean = theta, sd = 1), col = \"red\", pch = 19, cex = 0.75)\n      \n      # =============================================\n      # plot the likelihood function\n      # =============================================\n      \n      M &lt;- 2000\n      theta_grid &lt;- seq(-5, 5, length.out = M)\n      L_grid &lt;- numeric(M)\n      for(i in 1:M){\n        L_grid[i] &lt;- prod(dnorm(X, mean = theta_grid[i], sd = 1))\n      }\n      plot(theta_grid, L_grid, \n           type = \"l\", \n           main = \"Likelihood function\", \n           xlab = expression(theta), \n           ylab = \"L(θ)\",\n           col = \"salmon\",\n           bty = \"n\", lwd = 2)\n      points(theta, prod(dnorm(X, mean = theta, sd = 1)), col = \"red\", pch = 19)\n      points(theta, prod(dnorm(X, mean = theta, sd = 1)), type = \"h\", lty = 3, \"darkgrey\")\n      \n    } else if (input$dist == \"Normal (unknown variance)\"){\n      \n      # =============================================\n      # get the guess\n      # =============================================\n      \n      theta &lt;- input$theta\n      \n      # =============================================\n      # create some fake data\n      # =============================================\n      \n      n &lt;- 10\n      set.seed(123)\n      X &lt;- rnorm(n, mean = 0, sd = sqrt(pi))\n      \n      # =============================================\n      # plot data density\n      # =============================================\n      \n      par(mfrow = c(1, 2))\n      \n      curve(dnorm(x, mean = 0, sd = sqrt(theta)), \n            from = -6, \n            to = 6, \n            n = 2000, \n            ylab = \"f(x | θ)\", \n            main = \"Density of N(0, θ)\",\n            bty = \"n\",\n            yaxs = \"i\", \n            xaxt = \"n\",\n            yaxt = \"n\",\n            lwd = 2,\n            xlim = c(-5, 5),\n            ylim = c(-0.05, 1))\n      axis(1, pos = 0)\n      axis(2, at = seq(-0.5, 1, by = 0.1))\n      points(X, numeric(n), col = \"red\", pch = 19, cex = 0.75)\n      points(X, dnorm(X, mean = 0, sd = sqrt(theta)), type = \"h\", lty = 3, col = \"darkgrey\")\n      points(X, dnorm(X, mean = 0, sd = sqrt(theta)), col = \"red\", pch = 19, cex = 0.75)\n      \n      # =============================================\n      # plot the likelihood function\n      # =============================================\n      \n      M &lt;- 3000\n      theta_grid &lt;- seq(0, 10, length.out = M)\n      L_grid &lt;- numeric(M)\n      for(i in 1:M){\n        L_grid[i] &lt;- prod(dnorm(X, mean = 0, sd = sqrt(theta_grid[i])))\n      }\n      plot(theta_grid, L_grid, \n           type = \"l\", \n           main = \"Likelihood function\", \n           xlab = expression(theta), \n           ylab = \"L(θ)\",\n           col = \"salmon\",\n           bty = \"n\", lwd = 2)\n      points(theta, prod(dnorm(X, mean = 0, sd = sqrt(theta))), col = \"red\", pch = 19)\n      points(theta, prod(dnorm(X, mean = 0, sd = sqrt(theta))), type = \"h\", lty = 3, \"darkgrey\")\n\n    } else if (input$dist == \"Exponential\"){\n      \n      # =============================================\n      # get the guess\n      # =============================================\n      \n      theta &lt;- input$theta\n      \n      # =============================================\n      # create some fake data\n      # =============================================\n      \n      n &lt;- 10\n      set.seed(123)\n      X &lt;- rexp(n, rate = sqrt(pi))\n      \n      # =============================================\n      # plot data density\n      # =============================================\n      \n      par(mfrow = c(1, 2))\n      \n      curve(dexp(x, rate = theta), \n            from = 0, \n            to = 6, \n            n = 2000, \n            ylab = \"f(x | θ)\", \n            main = \"Density of Exponential(θ)\",\n            bty = \"n\",\n            yaxs = \"i\", \n            xaxt = \"n\",\n            yaxt = \"n\",\n            lwd = 2,\n            xlim = c(0, 4),\n            ylim = c(-0.05, 6))\n      axis(1, pos = 0)\n      axis(2, at = seq(0, 6, by = 1))\n      mtext(\"θ\", side = 2, at = theta, line = 2, col = \"blue\")\n      #points(theta, dnorm(theta, mean = theta, sd = 1), type = \"h\", col = \"blue\", lwd = 2, lty = 2)\n      points(X, numeric(n), col = \"red\", pch = 19, cex = 0.75)\n      points(X, dexp(X, rate = theta), type = \"h\", lty = 3, col = \"darkgrey\")\n      points(X, dexp(X, rate = theta), col = \"red\", pch = 19, cex = 0.75)\n      \n      # =============================================\n      # plot the likelihood function\n      # =============================================\n      \n      M &lt;- 2000\n      theta_grid &lt;- seq(0, 6, length.out = M)\n      L_grid &lt;- numeric(M)\n      for(i in 1:M){\n        L_grid[i] &lt;- prod(dexp(X, rate = theta_grid[i]))\n      }\n      plot(theta_grid, L_grid, \n           type = \"l\", \n           main = \"Likelihood function\", \n           xlab = expression(theta), \n           ylab = \"L(θ)\",\n           col = \"salmon\",\n           bty = \"n\", lwd = 2)\n      points(theta, prod(dexp(X, rate = theta)), col = \"red\", pch = 19)\n      points(theta, prod(dexp(X, rate = theta)), type = \"h\", lty = 3, \"darkgrey\")\n      \n    }\n    \n  })\n}\n\nshinyApp(ui, server)\nSo to summarize: we seek the value of \\(\\theta\\) that gives the best fitting density curve, and we can measure fit with the product of density values. Each value of \\(\\theta\\) gives a new product, and this relationship can be summarized by the likelihood function. If our goal is to find the value of the \\(\\theta\\) that best fits the data, we can do that by finding the value of \\(\\theta\\) that maximizes the likelihood function:\n\\[\n\\hat{\\theta}_n=\\underset{\\theta}{\\arg\\max}\\,L(\\theta;\\,X_{1:n})=\\underset{\\theta}{\\arg\\max}\\,\\prod_{i=1}^nf(X_i;\\,\\theta).\n\\]\nIn the app above you can perform the maximization by eye, but in general we do it mathematically. In other words, we reduce the problem of estimation to a Calc 1 problem: take the derivative, set it equal to zero, solve for the critical points, classify them using the second derivative test, all that jazz. See here if you want a refresher on how that goes.\nOn a technical level, it is important to recognize that maximizing the likelihood function will give you the same result as maximizing the log-likelihood function because the natural-log is an order-preserving function:\n\\[\n\\ell(\\theta;\\,X_{1:n})=\\ln L(\\theta;\\,X_{1:n})=\\ln\\prod_{i=1}^nf(X_i;\\,\\theta)=\\sum\\limits_{i=1}^n\\ln f(X_i;\\,\\theta).\n\\]\nSo, when performing maximum likelihood calculations, it is often easier to work with the log-likelihood, and so we do that instead:\n\\[\n\\hat{\\theta}_n=\\underset{\\theta}{\\arg\\max}\\,L(\\theta;\\,X_{1:n})=\\underset{\\theta}{\\arg\\max}\\,\\sum\\limits_{i=1}^n\\ln f(X_i;\\,\\theta).\n\\]\nYou get the same estimate either way, but applying the log turns the big product into a big sum, which is generally easier to differentiate. Lastly, notice that we write “argmax” and not “max.” This is because we don’t care what the biggest value of \\(L\\) actually is. We just care where it occurs. So we want “the argument that does the maximizing.”\n\n\n\n\n\n\nMLE summary\n\n\n\nWe have data from some parametric family of probability distributions:\n\\[\nX_1,\\,X_2,\\,...,\\,X_n\\overset{\\text{iid}}{\\sim}f(x;\\,\\theta).\n\\]\nSo \\(f\\) is the PDF or PMF for the distribution that all of the \\(X_i\\) share, and \\(\\theta\\) is the value of the parameter, which we do not know. The likelihood function measures how well a guess of \\(\\theta\\) fits the observed data:\n\\[\nL(\\theta;\\,X_{1:n})=\\prod_{i=1}^nf(X_i;\\,\\theta)\n\\]\nTo get the best-fitting estimate, we find the value that maximizes the likelihood function:\n\\[\n\\hat{\\theta}_n=\\underset{\\theta}{\\arg\\max}\\,L(\\theta;\\,X_{1:n}).\n\\]\nThis is the maximum likelihood estimator (MLE). You get the same solution if you maximize the log-likelihood function, and this is often easier to accomplish mathematically.\nIf you take a course in statistical theory like STA 332, you learn that the maximum likelihood estimator has good statistical properties in general. So no matter what distribution family you’re working with (assuming certain technical regularity conditions are satisfied), you can count on the following to be true:\n\nThe MLE is consistent:\nThe MLE is asymptotically normal, which allows you to compute approximate confidence intervals.\n\nThe MLE may or may not be biased. It depends on the distribution family you are working with, because the estimator is consistet, whatever bias there is will eventually go away as you collect more data. How fast? It depends. Welcome to statistical theory!"
  },
  {
    "objectID": "lecture-notes/sta240-notes-ex-stats-summary.html",
    "href": "lecture-notes/sta240-notes-ex-stats-summary.html",
    "title": "Guide to the stats problems",
    "section": "",
    "text": "We ended our course with a preview of some statistical topics that you are guaranteed to encounter early on in STA 332 and STA 402. The last two problems on our final exam will cover these topics, and as I have promised several times, you can expect these problems to be identical in format to the ones you have seen in lecture, lab, problem sets, and study guides. To that end, I have summarized below everything that you have seen. You should pay attention to the similarities and the differences.\n\nMaximum likelihood estimation\nProblem 9 on the final exam will present you with a parametric distribution family that you may or may not have seen before, and you will be asked to do the following:\n\nDerive the likelihood function and the log-likelihood function. To do this successfully, you need to be comfortable with pre-calc stuff: algebra, PEMDAS, especially your log and exponent properties;\nDerive the maximum likelihood estimator (MLE). So, we’re back in Calc I: take a derivative, set it equal to zero, and solve. Within a timed exam, I don’t expect you to check the second derivative;\nThe estimator you derive is a function of the \\(X_i\\), which are random, and so the estimator is also random. What is its distribution? To find it, you may have to apply the change-of-variables formula and recall the behavior of sums and averages;\nOnce you have the sampling distribution of the estimator, you can compute the estimator’s mean, variance, bias, mean-squared-error (MSE) and observe what the statistical properties are. If the sampling distribution belongs to a familiar family, you can lift the formulas for mean and variance off the shelf without any new derivations.\n\n\n\n\nsource\ndistribution\n\\(\\hat{\\theta}_n\\)\nsampling distribution\nproperties\n\n\n\n\nLecture\n\\(\\text{Exponential}(\\theta)\\)\n\\(\\frac{n}{\\sum_{i=1}^nX_i}\\)\n\\(\\text{IG}(n,\\,n\\theta)\\)\nbiased, consistent\n\n\nLecture\n\\(\\theta(x+1)^{-(\\theta+1)}\\)\n\\(\\frac{n}{\\sum_{i=1}^n\\ln(X_i+1)}\\)\n\\(\\text{IG}(n,\\,n\\theta)\\)\nbiased, consistent\n\n\nLab 10\n\\(\\text{Rayleigh}(\\theta)\\)\n\\(\\frac{1}{2n}\\sum\\limits_{i=1}^nX_i^2\\)\n\\(\\text{Gamma}(n,\\,n/\\theta)\\)\nunbiased, consistent\n\n\nPSET 7\n\\(\\text{N}(0,\\,\\theta)\\)\n\\(\\frac{1}{n}\\sum\\limits_{i=1}^nX_i^2\\)\n\\(\\text{Gamma}(n/2,\\,n/2\\theta)\\)\nunbiased, consistent\n\n\nPSET 7\n\\(\\frac{1}{2\\theta}\\exp\\left(-\\frac{|x|}{\\theta}\\right)\\)\n\\(\\frac{1}{n}\\sum\\limits_{i=1}^n|X_i|\\)\n\\(\\text{Gamma}(n,\\,n/\\theta)\\)\nunbiased, consistent\n\n\nStudy Guide\n\\(\\text{N}(\\theta,\\,1)\\)\n\\(\\frac{1}{n}\\sum\\limits_{i=1}^nX_i\\)\n\\(\\text{N}(\\theta,\\,1/n)\\)\nunbiased, consistent\n\n\nStudy Guide\n\\(\\frac{k}{\\theta}x^{k-1}\\exp\\left(-\\frac{x^k}{\\theta}\\right)\\)\n\\(\\frac{1}{n}\\sum\\limits_{i=1}^nX_i^k\\)\n\\(\\text{Gamma}(n,\\,n/\\theta)\\)\nunbiased, consistent\n\n\n\n\n\nBayesian inference\nProblem 10 on the final exam will present you with a Bayesian model: a prior for the parameter and a distribution for the data. Then, you will do a few things:\n\nDerive the posterior distribution. To do this, you massage the posterior kernel until you recognize that it belongs to a familiar family of distributions;\nCompute the posterior mean and show that it can be rewritten as a convex combination of the prior mean and the maximum likelihood estimator. If you do not already know the MLE from a previous result, then you may have to compute it;\nThe posterior mean is a new estimator that could be used as an alternative to the pure MLE. This new estmator has its own mean, variance, bias, and MSE which you can compute using what you know about the sampling distribution of the MLE and the linearity of expectation. The solutions to Lab 11 give you an example where this is fully worked out.\n\n\n\n\nsource\nprior\nlikelihood\nposterior\nhyperparameters\n\n\n\n\nLecture\n\\(\\text{Beta}(a_0,\\,b_0)\\)\n\\(\\text{Bernoulli}(\\theta)\\)\n\\(\\text{Beta}(a_n,\\,b_n)\\)\n\\[\\begin{aligned}a_n&=a_0+\\sum_{i=1}^nx_i\\\\b_n&=b_0+n-\\sum_{i=1}^nx_i\\end{aligned}\\]\n\n\nLab 11\n\\(\\text{Gamma}(a_0,\\,b_0)\\)\n\\(\\text{Poisson}(\\theta)\\)\n\\(\\text{Gamma}(a_n,\\,b_n)\\)\n\\[\\begin{aligned}a_n&=a_0+\\sum_{i=1}^nx_i\\\\b_n&=b_0+n\\end{aligned}\\]\n\n\nPSET 7\n\\(\\text{Gamma}(a_0,\\,b_0)\\)\n\\(\\text{Exponential}(\\theta)\\)\n\\(\\text{Gamma}(a_n,\\,b_n)\\)\n\\[\\begin{aligned}a_n&=a_0+n\\\\b_n&=b_0+\\sum_{i=1}^nx_i\\end{aligned}\\]\n\n\nPSET 7\n\\(\\text{Beta}(a_0,\\,b_0)\\)\n\\(\\text{Geometric}(\\theta)\\)\n\\(\\text{Beta}(a_n,\\,b_n)\\)\n\\[\\begin{aligned}a_n&=a_0+n\\\\b_n&=b_0-n+\\sum_{i=1}^nx_i\\end{aligned}\\]\n\n\nStudy Guide\n\\(\\text{N}(m_0,\\,\\tau_0^2)\\)\n\\(\\text{N}(\\theta,\\,1)\\)\n\\(\\text{N}(m_n,\\,\\tau_n^2)\\)\n\\[\\begin{aligned}\\tau_n^2&=(n+1/\\tau_0^2)^{-1}\\\\m_n&=\\tau_n^2\\left(\\frac{m_0}{\\tau_0^2}+\\sum_{i=1}^nx_i\\right)\\end{aligned}\\]\n\n\nStudy Guide\n\\(\\text{IG}(a_0,\\,b_0)\\)\n\\(\\text{N}(0,\\,\\theta)\\)\n\\(\\text{IG}(a_n,\\,b_n)\\)\n\\[\\begin{aligned}a_n&=a_0+n/2\\\\b_n&=b_0+\\frac{1}{2}\\sum_{i=1}^nx_i^2\\end{aligned}\\]\n\n\nStudy Guide\n\\(\\text{Gamma}(a_0,\\,b_0)\\)\n\\(\\theta(x+1)^{-(\\theta+1)}\\)\n\\(\\text{Gamma}(a_n,\\,b_n)\\)\n\\[\\begin{aligned}a_n&=a_0+n\\\\b_n&=b_0+\\sum_{i=1}^n\\ln(1+x_i)\\end{aligned}\\]"
  },
  {
    "objectID": "labs/lab-11.html",
    "href": "labs/lab-11.html",
    "title": "Lab 11",
    "section": "",
    "text": "Let’s say you work at an insurance company. Your boss asks you “how many insurance claims do we typically receive in a month?” So you pull up some data. Here’s a preview of what it might look like:\n\n\n\n\nMonth\nYear\nNumber of claims\n\n\n\n\n…\n…\n…\n\n\nMarch\n2005\n80\n\n\nApril\n2005\n61\n\n\nMay\n2005\n98\n\n\nJune\n2005\n73\n\n\n…\n…\n…\n\n\n\n\nTo answer your boss’s question, you could take the numbers and just compute the average. That’s probably a pretty good guess. But imagine your boss follows up with questions like “how certain are we of that guess?” or “how reliable is that estimate?” Now we’re talking about statistics.\nAs soon as we start worrying about uncertainty quantification, we enter the subtle realm of modeling assumptions and interpretive philosophies. It’s not like probability theory where we were just proving theorems and doing calculations. Now, the theory of statistics is in some ways just another branch of probability theory, and so in truth were are still proving theorems and doing calculations. But we wring our hands about the interpretation of the results in ways that we didn’t necessarily do before.\nAnyway, I’m rambling. Data don’t speak for themselves, and so we require an interpretive lens to make sense of them. The strongest form of this is a concrete statistical model, so let’s pose one. The number of insurance claims in a month is going to be a non-negative integer, and so we could choose for our model any parametric family of probability distributions supported on \\(\\mathbb{N}\\). Let’s go with the simplest one:\n\\[\nX_1,\\,X_2,\\,...,\\,X_n\\overset{\\text{iid}}{\\sim}\\text{Poisson}(\\theta).\n\\]\nThat’s our model for the claims data. The rate parameter \\(\\theta&gt;0\\) is unknown, and we want to use the data to learn what it is. Note that this model may or may not be a good choice, and entire books have been written on model building and model selection and model diagnostics and model criticism. If you keep studying statistics, you’ll learn about all that, but this class is about one thing and one thing only: can you do the math or not? So we will just take the model as given and use it as an opportunity to test our technical skills.\n\nTask 0\nCompute the likelihood function and the log-likelihood function for the Poisson distribution. You will need both in what follows.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe likelihood function is\n\\[\n\\begin{aligned}\nL(\\theta\\mid X_{1:n})\n&=\n\\prod_{i=1}^nf(X_i\\mid \\theta)\n\\\\\n&=\n\\prod_{i=1}^ne^{-\\theta}\\frac{\\theta^{X_i}}{X_i!}\n\\\\\n&=\n\\left(\\prod_{i=1}^ne^{-\\theta}\\right)\\left(\\prod_{i=1}^n\\theta^{X_i}\\right)\\prod_{i=1}^n\\frac{1}{X_i!}\n\\\\\n&=\ne^{-n\\theta}\\theta^{\\sum_{i=1}^nX_i}\\prod_{i=1}^n\\frac{1}{X_i!}.\n\\end{aligned}\n\\]\nThe log-likelihood function is\n\\[\n\\begin{aligned}\n\\ell(\\theta\\mid X_{1:n})\n&=\n\\ln L(\\theta\\mid X_{1:n})\n\\\\\n&=\n\\ln\\left(\ne^{-n\\theta}\\theta^{\\sum_{i=1}^nX_i}\\prod_{i=1}^n\\frac{1}{X_i!}\n\\right)\n\\\\\n&=\n-n\\theta+\\left(\\sum_{i=1}^nX_i\\right)\\ln\\theta+\\sum\\limits_{i=1}^n\\ln(1/X_i!).\n\\end{aligned}\n\\]\n\n\n\n\n\nTask 1\nCompute the maximum likelihood estimator for \\(\\theta&gt;0\\) in the Poisson distribution.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\ell'(\\theta\\mid X_{1:n})=-n+\\frac{\\sum_{i=1}^nX_i}{\\theta}&=0\n\\\\\n\\frac{\\sum_{i=1}^nX_i}{\\theta}&=n\n\\\\\n\\frac{\\sum_{i=1}^nX_i}{n}&=\\theta\n\\\\\n\\\\\n\\implies\\quad\\hat{\\theta}_n&=\\bar{X}_n\n\\end{aligned}\n\\]\n\n\n\n\n\nTask 2\nCompute the mean, bias, variance, and MSE of the estimator and comment on its statistical properties.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis is the average of iid random variables, so we know that\n\\[\n\\begin{aligned}\nE(\\hat{\\theta}_n)&=E(X_1)=\\theta\\\\\n\\text{var}(\\hat{\\theta}_n)&=\\frac{\\text{var}(X_1)}{n}=\\frac{\\theta}{n}.\n\\end{aligned}\n\\]\nAs such, the estimator is unbiased, the MSE is equal to the variance, and this goes to zero as \\(n\\to\\infty\\), so the estimator is consistent.\n\n\n\n\n\nTask 3\nThe rate parameter \\(\\theta&gt;0\\) is a positive real number, so if we want to perform a Bayesian analysis, we need a continuous probability distribution on positive reals. The gamma family is a convenient choice for this, so consider the Bayesian model:\n\\[\n\\begin{aligned}\n\\theta&\\sim\\text{Gamma}(a_0,\\,b_0)\n\\\\\nX_1,\\,X_2,\\,...,\\,X_n\\mid\\theta&\\overset{\\text{iid}}{\\sim}\\text{Poisson}(\\theta).\n\\end{aligned}\n\\]\nDerive the posterior distribution.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe posterior kernel is recognizeable:\n\\[\n\\begin{aligned}\nf(\\theta\\mid x_{1:n})\n&\\propto\nL(\\theta\\mid x_{1:n})f(\\theta)\n\\\\\n&=\ne^{-n\\theta}\\theta^{\\sum_{i=1}^nx_i}\\underbrace{\\left(\\prod_{i=1}^n\\frac{1}{x_i!}\\right)\\frac{b_0^{a_0}}{\\Gamma(a_0)}}_{\\text{no }\\theta}\\theta^{a_0-1}e^{-b_0\\theta}\n\\\\\n&\\propto\ne^{-n\\theta}\\theta^{\\sum_{i=1}^nx_i}\\theta^{a_0-1}e^{-b_0\\theta}\n\\\\\n&=\n\\underbrace{\\theta^{a_0+\\sum_{i=1}^nx_i-1}e^{-(b_0+n)\\theta}}_{\\text{kernel of Gamma density}}.\n\\end{aligned}\n\\]\nSo the posterior is\n\\[\n\\begin{aligned}\n\\theta\\mid X_{1:n}=x_{1:n}&\\sim\\text{Gamma}(a_n,\\,b_n)\\\\\n\\\\\na_n&=a_0+\\sum_{i=1}^nx_i\\\\\nb_n&=b_0+n.\n\\end{aligned}\n\\]\n\n\n\n\n\nTask 4\nBased on your answer from the previous task, show that the posterior mean is a convex combination of the prior mean and the maximum likelihood estimator that you derived earlier.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe posterior mean is\n\\[\n\\begin{aligned}\nE(\\theta\\mid x_{1:n})\n&=\n\\frac{a_n}{b_n}\n\\\\\n&=\n\\frac{a_0+\\sum_{i=1}^nx_i}{b_0+n}\n\\\\\n&=\n\\frac{1}{b_0+n}a_0+\\frac{1}{b_0+n}\\sum_{i=1}^nx_i\n\\\\\n&=\n\\frac{1}{b_0+n}\\frac{b_0}{b_0}a_0+\\frac{1}{b_0+n}\\frac{n}{n}\\sum_{i=1}^nx_i\n\\\\\n&=\n\\frac{b_0}{b_0+n}\\frac{a_0}{b_0}+\\frac{n}{b_0+n}\\frac{1}{n}\\sum_{i=1}^nx_i\n\\\\\n&=\n\\frac{b_0}{b_0+n}E(\\theta)+\\frac{n}{b_0+n}\\bar{x}_n.\n\\end{aligned}\n\\]\n\n\n\n\n\nTask 5\nIgnoring that we ever did a Bayesian analysis, treat the posterior mean formula from the previous task as a new classical estimator for \\(\\theta\\). It’s just a function of the data at the end of the day.\nWhat are the mean, bias, variance, and MSE of the posterior mean?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOur new estimator is\n\\[\n\\begin{aligned}\n\\tilde{\\theta}_n&=\\frac{b_0}{b_0+n}\\frac{a_0}{b_0}+\\frac{n}{b_0+n}\\bar{X}_n\\\\\n&=(1-w_n)g+w_n\\bar{X}_n.\n\\end{aligned}\n\\]\nThis is just a linear transformation of one random variable \\(\\bar{X}_n\\), so the mean and variance are\n\\[\n\\begin{aligned}\nE(\\tilde{\\theta}_n)&=(1-w_n)g+w_n\\theta\\\\\n\\text{var}(\\tilde{\\theta}_n)&=w_n^2\\frac{\\theta}{n}.\n\\end{aligned}\n\\]\nSo the bias is\n\\[\n\\begin{aligned}\nE(\\tilde{\\theta}_n)-\\theta&=(1-w_n)g+w_n\\theta-\\theta\\\\\n&=\n(1-w_n)g-(1-w_n)\\theta\n\\\\\n&=\n(1-w_n)(g-\\theta).\n\\end{aligned}\n\\]\nUnless the prior mean is exactly equal to the true value (unlikely) or we set \\(w_n=1\\) (back to pure MLE), this is a biased estimator.\nThe overall MSE is\n\\[\n\\begin{aligned}\n\\text{MSE}\n&=\n\\left[E(\\tilde{\\theta}_n)-\\theta\\right]^2+\\text{var}(\\tilde{\\theta}_n)\n\\\\\n&=\n(1-w_n)^2(g-\\theta)^2+w_n^2\\frac{\\theta}{n}\n.\n\\end{aligned}\n\\]\nAs \\(n\\to\\infty\\), \\(w_n\\to 1\\), so both terms go to zero, and the estimator is consistent.\n\n\n\n\n\nTask 6\nAre there circumstances under which this new estimator would be better (in the sense of lower MSE) than the pure MLE?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIf we set \\(w_n=1\\), the estimator is back to pure MLE, which has an MSE of \\(\\theta/n\\). It is possible to pick \\(w_n\\) such that the biased estimator has even lower MSE. Behold\n\\[\n\\begin{aligned}\n\\frac{\\text{d}}{\\text{d}w_n}[(1-w_n)^2(g-\\theta)^2+w_n^2\\frac{\\theta}{n}]\n=\n-2(1-w_n)(g-\\theta)^2+2w_n\\frac{\\theta}{n}\n&=\n0\n\\\\\n2w_n\\frac{\\theta}{n}\n&=\n2(1-w_n)(g-\\theta)^2\n\\\\\nw_n\\frac{\\theta}{n}\n&=\n(g-\\theta)^2-w_n(g-\\theta)^2\n\\\\\nw_n\\frac{\\theta}{n}+w_n(g-\\theta)^2\n&=\n(g-\\theta)^2\n\\\\\nw_n\\left(\\frac{\\theta}{n}+(g-\\theta)^2\\right)\n&=\n(g-\\theta)^2\n\\\\\nw_n\n&=\n\\frac{(g-\\theta)^2}{\\frac{\\theta}{n}+(g-\\theta)^2}.\n\\end{aligned}\n\\]\nIt’s 1 AM, so I am not checking the second derivative, but this is a global minimizer, and it is between zero and one as it should be. Here, we don’t just want the location of the minimum, we want the actual minimum value so we can compare the MSEs:\n\\[\n\\begin{aligned}\n\\underset{w_n}{\\min}\\,\\text{MSE}\n&=\n\\left(\\frac{\\theta/n}{\\frac{\\theta}{n}+(g-\\theta)^2}\\right)^2(g-\\theta)^2+\\left(\\frac{(g-\\theta)^2}{\\frac{\\theta}{n}+(g-\\theta)^2}\\right)^2\\frac{\\theta}{n}\n\\\\\n&=\n\\frac{(\\theta/n)^2(g-\\theta)^2+(g-\\theta)^4(\\theta/n)}{\\left[\\frac{\\theta}{n}+(g-\\theta)^2\\right]^2}\n\\\\\n&=\n\\frac{(g-\\theta)^2(\\theta/n)[\\theta/n+(g-\\theta)^2]}{\\left[\\frac{\\theta}{n}+(g-\\theta)^2\\right]^2}\n\\\\\n&=\n\\frac{(g-\\theta)^2(\\theta/n)}{\\frac{\\theta}{n}+(g-\\theta)^2}\n\\\\\n&=\nw_n^*\\frac{\\theta}{n}\n\\\\\n&&lt;\n\\frac{\\theta}{n}\n.\n\\end{aligned}\n\\]\nSo, the minimum MSE is equal to the minimizing weight times the MSE of the pure MLE. Since the minimizing weight is strictly less than 1, this is indeed an improvement.\nNow, the MSE-minimizing weight depends on the true but unknown \\(\\theta\\), so you cannot immediately operationalize this result. But nevertheless it demonstrates that in theory shrinkage can improve the MLE.\n\n\n\n\n\nPlay around!\nHere’s a cute lil’ app you can play with to get a sense of how these statistical procedures behave. Start by keeping all the default setting the same but increasing the sample size. What happens to the posterior distribution as you accumulate more and more data?\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\nlibrary(shiny)\n\n# true value\n# prior hyperparameter\n# sample size\n# show the data\n# show the summary statistics\n\nui &lt;- fluidPage(\n  titlePanel(\"Inference for the Poisson distribution\"),\n  \n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"lambda\", \"True value of θ:\",\n                   min = 0.1, max = 10, value = 1, step = 0.1),\n      sliderInput(\"a\", \"Prior shape (a₀):\",\n                  min = 0.1, max = 100, value = 70, step = 0.1),\n      sliderInput(\"b\", \"Prior rate (b₀):\",\n                  min = 0.1, max = 100, value = 10, step = 0.1),\n      sliderInput(\"n\", \"Sample size (n):\",\n                  min = 1, max = 300, value = 1, step = 1),\n      hr(),\n      verbatimTextOutput(\"moments\")\n    ),\n    \n    mainPanel(\n      plotOutput(\"betaPlot\", height = \"600px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n  \n  output$moments &lt;- renderText({\n    set.seed(1234)\n  \n    n &lt;- input$n\n    lambda &lt;- input$lambda\n    a &lt;- input$a\n    b  &lt;- input$b\n    X &lt;- rpois(n, lambda)\n    \n    xbar &lt;- mean(X)\n    a_new &lt;- a + sum(X)\n    b_new &lt;- b + n\n    paste0(\"Prior mean: \", round(a / b, 3),\n           \"\\nMLE: \", round(xbar, 3),\n           \"\\nPosterior mean: \", round(a_new / b_new, 3))\n  })\n  \n  output$betaPlot &lt;- renderPlot({\n    set.seed(1234)\n  \n    n &lt;- input$n\n    lambda &lt;- input$lambda\n    a &lt;- input$a\n    b  &lt;- input$b\n    X &lt;- rpois(n, lambda)\n    \n    xbar &lt;- mean(X)\n    a_new &lt;- a + sum(X)\n    b_new &lt;- b + n\n    \n    curve(dgamma(x, shape = a, rate = b), from = 0, to = 10, n = 1000, col = \"blue\",\n          ylim = c(0, 5), lwd = 2, xlab = \"θ\",\n          ylab = \"density\")\n    curve(dgamma(x, shape = a_new, rate = b_new), from = 0, to = 10, n = 1000, add = TRUE,\n          col = \"purple\", lwd = 2)\n    abline(v = lambda, lwd = 2, col = \"red\")\n      legend(\"topright\", c(\"prior\", \"posterior\", \"true value\"), \n         lty = 1, lwd = 2,\n         col = c(\"blue\", \"purple\", \"red\"), bty = \"n\")\n  })\n}\n\nshinyApp(ui, server)",
    "crumbs": [
      "Labs",
      "Lab 11"
    ]
  },
  {
    "objectID": "labs/lab-9.html",
    "href": "labs/lab-9.html",
    "title": "Lab 9",
    "section": "",
    "text": "An insurance company sells policies in the hope that most customers will not actually file a claim. In that case, the company keeps the premiums, and never actually pays out. Cha-ching. If on the other hand, every customer files a claim, then the company is ruined. They simply don’t keep enough cash on hand to literally pay out all the customers they sell policies to.\nThis is obviously a risky business. The insurance company is making a bet that most of their customers won’t need the policy that they bought, but there could be surprises. The company is fundamentally uncertain about how many people will file claims, and how big those claims will be. Faced with this uncertainty, they must make decisions about who to sell to, what to charge, and how much cash to keep on hand. To manage the uncertainty and make good decisions, insurance companies hire armies of actuaries to model the insurance market using the basic tools of probability and statistics that we are learning. Let’s get a taste of that.\nImagine the Pacific All-Risk Insurance Company makes its decisions on a monthly basis. Let \\(N\\in\\mathbb{N}\\) denote the number of insurance claims the company receives in a given month. Each of those claims will be for some positive dollar amount, call it \\(X_i&gt;0\\). So each month, customers apply for \\(S=X_1+X_2+...+X_N\\) in reimbursement from the insurance company. The company is not sure what \\(N\\) will be, they are not sure how big each \\(X_i\\) will be, and so they are not sure how big the total \\(S\\) will be. But if it gets too big, the company is finished. We shall model the number of claims \\(N\\) and the size of each claim \\(X_i\\) as random variables using our familiar families:\n\\[\n\\begin{aligned}\nN&\\sim\\text{Poisson}(\\lambda)\n\\\\\nX_1,\\,X_2,\\,X_3,\\,X_4,\\,...&\\overset{\\text{iid}}{\\sim} \\text{Gamma}(\\alpha,\\,\\beta)\\\\\nS&=\\sum\\limits_{i=1}^N X_i\n\\end{aligned}\n\\]\nWe see that \\(S\\) is a random sum of random variables. Each \\(X_i\\) is random, and the number of terms in the sum is also random. So randomness from both the \\(X_i\\) and \\(N\\) is feeding forward into the randomness in \\(S\\), making it more challenging to model.\nIn what follows, take \\(\\lambda=100\\) and \\((\\alpha,\\,\\beta)=(25,\\,0.0025)\\).\nTask 1\nWhat is the probability that \\(N\\) is strictly within one standard deviation of its mean? In other words, what is the probability that \\(N\\) lies in the interval \\((E(N)-\\text{sd}(N),\\,E(N)+\\text{sd}(N))\\). Use the appropriate p- function to compute this, and pay attention to the endpoints!\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(E(N)=100\\) and \\(\\text{sd}(N)=\\sqrt{\\text{var}(N)}=\\sqrt{100}=10\\), so we want \\(P(90 &lt; N &lt; 110)\\). We can use the cdf, but we have to pay attention to the endpoints. In general, we know:\n\\[\nP(90 &lt; N \\leq  110) = F(110) - F(90).\n\\]\nEither of these works:\n\nppois(110, 100) - ppois(90, 100) - dpois(110, 100)\n\n[1] 0.658055\n\nppois(109, 100) - ppois(90, 100)\n\n[1] 0.658055\n\n\n\n\n\nTask 2\nWhat is the probability that \\(N\\) is at or above its mean? So, \\(P(N\\geq E(N))\\)?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nSince \\(E(N)=100\\), we want \\(P(N\\geq 100) = 1 - P(N &lt; 100) = 1 - P(N\\leq 99)\\):\n\n1 - ppois(99, 100)\n\n[1] 0.5132988\n\n\n\n\n\nTask 3\nWrite some R code that simulates m = 5000 draws from the joint distribution of \\((N,\\,S)\\). One way to do this is with a for loop that cycles through the following steps:\n\nSimulate \\(N\\) and store it;\nGiven the result of Step 1, simulate iid \\(X_1\\), \\(X_2\\), …, \\(X_N\\);\nGiven the result of Step 2, compute \\(S=X_1+X_2+...+X_N\\) and store it.\n\nAfter the loop is complete, you should have a vector N storing all of the draws of \\(N\\), and a vector S storing all of the draws of \\(S\\).\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nset.seed(556643)\nm = 5000\nN = numeric(m)\nS = numeric(m)\nfor(i in 1:m){\n  N[i] = rpois(1, 100)\n  S[i] = sum(rgamma(N[i], 25, rate = 0.0025))\n}\n\n\n\n\nTask 4\nCreate a well-labeled scatterplot of the pairs \\((N,\\,S)\\) that you just simulated.\n\n\n\n\n\n\nAdjust the point size\n\n\n\nYou might want to use the cex argument to the plot function in order to decrease the size of the points. By default cex = 1.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nplot(N, S, pch = 19, cex = 0.25)\n\n\n\n\n\n\n\n\n\n\nTask 5\nPlot a histogram of the draws of \\(S\\) by themselves.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nhist(S, freq = FALSE, breaks = \"Scott\")\n\n\n\n\n\n\n\n\n\n\nTask 6\nUse your draws of \\(N\\) to estimate the probabilities from Tasks 1 and 2, and verify that you get estimates that are close to the actual probabilities you calculated.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmean(90 &lt; N & N &lt; 110)\n\n[1] 0.6536\n\nmean(N &gt;= 100)\n\n[1] 0.5194\n\n\n\n\n\nTask 7\nUse the draws of \\(S\\) to estimate the following quantities:\n\nMean of \\(S\\);\nVariance of \\(S\\);\n\n\\(P(S &gt; 1200000)\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmean(S)\n\n[1] 1002005\n\nvar(S)\n\n[1] 10598722666\n\nmean(S &gt; 1200000)\n\n[1] 0.0294\n\n\n\n\n\nTask 8\nIn our setup, \\(N\\) and \\(S\\) are a random pair that is jointly distributed according to the following hierarchy:\n\\[\n\\begin{aligned}\nN&\\sim\\text{Poisson}(\\lambda = 100)\\\\\nS\\mid N=n &\\sim \\,???\n\\end{aligned}\n\\]\nConditional on \\(n\\), what is the distribution of \\(S\\)? You may find results from yesterday’s lecture helpful.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIf \\(n\\) is fixed, then \\(S\\) is the sum of \\(n\\) iid gamma random variables, so we know\n\\[\nS\\mid N = n \\sim \\text{Gamma}(n\\cdot 25,\\,0.0025)\n\\]\n\n\n\nTask 9\nUse the tower property to compute the following marginal expected values of \\(S\\):\n\n\n\\(E(S)\\);\n\n\\(E(S^2)\\);\n\n\\(M_S(t)=E(e^{tS})\\).\n\n\n\n\n\n\n\nNote\n\n\n\nThe marginal distribution of \\(S\\) is not recognizable. It’s some funky new thing. By deriving its MGF, you have completely characterized the entire distribution, but it turns out that we don’t have nice clean formulas for the CDF or the PDF. Bummer! But as you saw in previous tasks, we can still simulate the distribution of \\(S\\) and approximate its properties. Neat!\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nPart a\n\\[\n\\begin{aligned}\nE(S)&=E(E(S\\mid N))\n\\\\\n&=\nE\\left(\\frac{N\\cdot 25}{0.0025}\\right)\n\\\\\n&=\n\\frac{25}{0.0025}E\\left(N\\right)\n\\\\\n&=\nE(X_1)E(N)\n\\\\\n&=\n10000\\times 100\n\\\\\n&=\n1000000.\n\\end{aligned}\n\\]\nPart b\nWe know \\(S\\mid N=n\\sim\\text{Gamma}(n\\cdot 25,\\,0.0025)\\), and we know that \\(\\text{var}(S|N=n)=n\\cdot 25/0.0025^2\\), so\n\\[\nE(S^2|N=n)=\\text{var}(S|N=n)+E(S|N=n)^2=\\frac{n\\cdot 25}{0.0025^2}+\\frac{n^2\\cdot 25^2}{0.0025^2}\n\\]\nTower property again:\n\\[\n\\begin{aligned}\nE(S^2)\n&=E(E(S^2|N))\\\\\n&=\nE\\left(\\frac{N\\cdot 25}{0.0025^2}+\\frac{N^2\\cdot 25^2}{0.0025^2}\\right)\n\\\\\n&=\n\\frac{25}{0.0025^2}E(N)+\\frac{25^2}{0.0025^2}E(N^2)\n\\\\\n&=\n\\frac{25}{0.0025^2}100+\\frac{25^2}{0.0025^2}(100+100^2)\n\\end{aligned}\n\\]\n\n(25 / 0.0025^2) * (100 + 25 * (100 + 100^2))\n\n[1] 1.0104e+12\n\n\nPart c\n\\[\n\\begin{aligned}\nE(e^{tS})\n&=\nE(E(e^{tS}|N))\n\\\\\n&=E\\left(\\left(\\frac{0.0025}{0.0025 - t}\\right)^{N25}\\right)&&\\text{gamma mgf}\n\\\\\n&=\n\\sum\\limits_{n=0}^\\infty\n\\left(\\frac{0.0025}{0.0025 - t}\\right)^{n25}\\frac{100^n}{n!}e^{-100} && \\text{LOTUS}\n\\\\\n&=\ne^{-100}\n\\sum\\limits_{n=0}^\\infty\n\\frac{\\left(100\\left(\\frac{0.0025}{0.0025 - t}\\right)^{25}\\right)^n}{n!} && \\text{taylor series of exponential}\n\\\\\n&=\ne^{-100}\ne^{100\\left(\\frac{0.0025}{0.0025 - t}\\right)^{25}},&& t&lt;0.0025.\n\\end{aligned}\n\\]\n\n\n\n\nTask 10\nVerify that the theoretical mean and variance of \\(S\\) match the sample estimates you computed from your simulation in Task 7.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe means are close.\n\nmean(S) # should be close to one million\n\n[1] 1002005\n\n\nThe variance should be close to\n\nES2 = (25 / 0.0025^2) * (100 + 25 * (100 + 100^2))\nES = 1000000\nvarS = ES2 - ES^2\nvarS\n\n[1] 1.04e+10\n\n\n\nvar(S)\n\n[1] 10598722666\n\n\nI guess it’s close.\n\n\n\nTask 11\nIn our setup, we assumed that the \\(X_i\\) were independent.\n\nWhat was the purely mathematical benefit of making this assumption? How did it simplify our life?\nIn the context of our application, is independence a reasonable assumption?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nWe needed independence to say that the sum of gammas was also gamma. If we didn’t know that the exact conditional distribution was gamma, Task 9 would have been a real pain;\nInsurance claims are definitely not independent. Imagine this insurance company sells home insurance policies in southern California. If a wild fire rips through town, the claims being filed by homes in the same neighborhood are definitely dependent.\n\n\n\n\nTask \\(\\infty\\)\n\nGo watch Double Indemnity. It is so entertaining it will make your head spin.",
    "crumbs": [
      "Labs",
      "Lab 9"
    ]
  },
  {
    "objectID": "labs/lab-3.html",
    "href": "labs/lab-3.html",
    "title": "Lab 3",
    "section": "",
    "text": "Note\n\n\n\nThis assignment was adapted from:\n\nMeng, Xiao-Li (2023): “Double your variance, dirtify your Bayes, devour your pufferfish, and draw your kidstrogram,” The New England Journal of Statistics in Data Science, vol 1 no 1.\nCar Talk (1977 - 2012) was a popular show on NPR. It featured a segment called the “Puzzler,” where the hosts would read a brain teaser and invite listeners to send in solutions for the chance to win a prize. Here is the text of one such Puzzler (original audio at around 19:19 here):\nIn what follows, you may find some notation useful. Let \\(D\\) denote your true disease status, and \\(T\\) denote the result of your test. Then\n\\[\n\\begin{align*}\n    p &= P(D=+) && \\text{(prevalence)}\n    \\\\\n    f_{-}&=P(T=-\\,|\\, D=+) && \\text{(false negative rate)}\n    \\\\\n    f_{+}&=P(T=+\\,|\\, D=-) && \\text{(false positive rate)}\n    \\\\\n    1-f_{-}&=P(T=+\\,|\\, D = +) && \\text{(sensitivity)}\n    \\\\\n    1-f_{+}&=P(T=-\\,|\\, D = -) && \\text{(specificity)}.\n\\end{align*}\n\\]",
    "crumbs": [
      "Labs",
      "Lab 3"
    ]
  },
  {
    "objectID": "labs/lab-3.html#task-1",
    "href": "labs/lab-3.html#task-1",
    "title": "Lab 3",
    "section": "Task 1",
    "text": "Task 1\nBefore you read anything else below, try to solve the puzzler yourself. What do you come up with?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nObviously you will struggle to come up with anything, and a lot depends on how you interpret “It’s 95% accurate….5% of the people who take the test will test positive but they don’t really have the disease.” My solution below interprets this as the false positive rate. The puzzler says nothing about the false negative rate, and so you can’t actually answer the question because you do not have enough information.",
    "crumbs": [
      "Labs",
      "Lab 3"
    ]
  },
  {
    "objectID": "labs/lab-3.html#task-2",
    "href": "labs/lab-3.html#task-2",
    "title": "Lab 3",
    "section": "Task 2",
    "text": "Task 2\nExplain the correct way to solve this problem. Exactly what probability are we asked to compute, and what formula should we apply to do it? Does the prompt actually provide enough information to ultimately get the job done?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe Puzzler asks us to compute \\(P(D=+\\,|\\, T=+)\\), which in principle we can do with Bayes’ theorem:\n\\[\n\\begin{aligned}\n    P(D=+\\,|\\, T=+)\n    &=\n    \\frac{P(T=+\\,|\\, D=+)P(D=+)}{P(T=+)}\n    \\\\\n    &=\n    \\frac{P(T=+\\,|\\, D=+)P(D=+)}{P(T=+\\,|\\, D=+)P(D=+)+P(T=+\\,|\\, D=-)P(D=-)}\n    \\\\\n    &=\n    \\frac{(1-f_-)p}{(1-f_-)p+f_+(1-p)}\n    .\n\\end{aligned}\n\\]\nUnfortunately, the show doesn’t tell us \\(f_-\\), so you cannot actually compute the answer.",
    "crumbs": [
      "Labs",
      "Lab 3"
    ]
  },
  {
    "objectID": "labs/lab-3.html#task-3",
    "href": "labs/lab-3.html#task-3",
    "title": "Lab 3",
    "section": "Task 3",
    "text": "Task 3\nBeneath the fold is the solution that the hosts ultimately revealed.\n\n\n\n\n\n\nThe show’s solution\n\n\n\n\n\nOriginal audio at around 20:30 here:\n\nLet’s say 1000 people take the test. Fifty people will test positive and yet they will not have it. One will test positive and have it. So your chances of actually having it, even though you tested positive, are one in 51, or a little less than 2%.\n\n\n\n\nSo, what do you think? Did they get it right? Explain how the show interpreted the information given in the Puzzler, and explain what arithmetic formula they implicitly applied in order to compute their answer.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWhen they say “0.1% of the people actually contract the disease,” this refers to the prevalence, and when they say “5% of the people who take the test will test positive but they don’t really have the disease,” this refers to the false positive rate. So \\(p=0.001\\), and \\(f_+=0.05\\). Based only on these, they somehow came up with:\n\\[\n\\frac{p}{p+f_+}=\\frac{0.001}{.001 + 0.05} = \\frac{1}{51}\\approx 0.0196.\n\\]",
    "crumbs": [
      "Labs",
      "Lab 3"
    ]
  },
  {
    "objectID": "labs/lab-3.html#task-4",
    "href": "labs/lab-3.html#task-4",
    "title": "Lab 3",
    "section": "Task 4",
    "text": "Task 4\nExplain two things:\n\nUnder what conditions is the formula that the show used actually an upper bound on the correct answer?\nEven though their number is not exactly correct, why might an upper bound on the true probability still be a useful thing to calculate?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nBy factoring \\(1-f_-\\) out of the numerator and denominator, we can rewrite the correct formula as\n\\[\nP(D=+\\,|\\, T = +)=\\frac{(1-f_-)p}{(1-f_-)p+f_+(1-p)}=\\frac{p}{p+f_+\\left(\\frac{1-p}{1-f_-}\\right)}.\n\\]\nIf \\(p\\leq f_-\\), then \\(1-f_-\\leq 1-p\\), which implies\n\\[\np+f_+\\leq p+f_+\\left(\\frac{1-p}{1-f_-}\\right).\n\\]\nAs a consequence, we see that\n\\[\nP(D=+\\,|\\, T = +)\n=\n\\frac{p}{p+f_+\\left(\\frac{1-p}{1-f_-}\\right)}\n\\leq \\frac{p}{p+f_+},\n\\quad \\text{when }p\\leq f_-\n.\n\\]\nSo the formula that the show applied is an upper bound for the true probability.\nTwo types of errors are possible in a situation like this: treating a disease that is not there, or failing to treat a disease that is there. On balance, the second error is probably the worse of the two, so I would rather overestimate the probability that I have the disease than underestimate it.",
    "crumbs": [
      "Labs",
      "Lab 3"
    ]
  },
  {
    "objectID": "labs/lab-3.html#task-5",
    "href": "labs/lab-3.html#task-5",
    "title": "Lab 3",
    "section": "Task 5",
    "text": "Task 5\nSo, the show’s answer, while wrong, could still be potentially useful upper bound on the true probability. Neat! But how wrong is it, even? To get a sense of this, use R to create some line plots with the prevalence \\(p\\in[0,\\, 1]\\) on the horizontal axis and the true and approximate probabilities for different values of \\(f_-\\) and \\(f_+\\) on the vertical axis. Mix-and-match \\(f_-,\\, f_+\\in\\{0.1,\\, 0.2\\}\\), and comment on the difference between the two curves.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nCodeFPR = c(0.1, 0.2)\nFNR = c(0.1, 0.2)\npar(mfrow = c(2, 2))\nfor (i in 1:2){\n  for (j in 1:2){\n    f_plus = FPR[i]\n    f_minus = FNR[j]\n    curve(x / (x + f_plus),\n          col = \"blue\",\n          xlab = \"p\", \n          ylab = \"\", \n          xlim = c(0, 1),\n          ylim = c(0, 1),\n          main = paste(\"FNR = \", f_minus, \"; FPR = \", f_plus))\n    curve(x / (x + f_plus * ((1 - x) / (1 - f_minus))),\n          col = \"red\", add = TRUE)\n    legend(\"bottomright\",\n           c(\"True Probability\", \"Car Talk Answer\"),\n           col = c(\"red\", \"blue\"),\n           bty = \"n\",\n           lty = c(1, 1)\n           )\n  }\n}\n\n\n\n\n\n\n\nWhen the prevalence is low (ie rare disease) the Car Talk curve is a very close upper bound on the true probability. For higher prevalence, the Car Talk curve is no longer an upper bound, and the curves get farther apart.",
    "crumbs": [
      "Labs",
      "Lab 3"
    ]
  },
  {
    "objectID": "labs/lab-4.html",
    "href": "labs/lab-4.html",
    "title": "Lab 4 - How does ChatGPT generate the next word?",
    "section": "",
    "text": "If you’ve ever asked ChatGPT a question, you may have noticed something; it generates its response one word at a time right before your eyes:\nWhat exactly is it doing? Well, ask it! If you do, the explanation will go something like this…\nChatGPT is an example of a large language model (LLM). Under the hood, there’s this fancy pants deep neural network machine learning mumbo jumbo that has been trained on a massive corpus of documents (books, newspaper articles, academic publications, reddit, all of Wikipedia, etc). When you ask it a question like “What color is the apple?”, the model asks itself “conditional on the text you just gave me, what is the likely next word?” Based on the text it was trained on and the underlying ML model, ChatGPT constructs a conditional probability distribution over the set of “next words” and simulates from it, like with the sample function. The new word that it simulates gets added to the end of the text you provided, and it repeats the process over again: given where we are now, what is the next word likely to be? And on and on…\nSo say you ask it “What color is the apple?” Based on that, the model constructs a probability distribution for \\(P(\\text{next word}\\,|\\,\\texttt{\"What color is the apple?\"})\\). The sample space \\(S\\) here is (basically) the set of all words that the model has seen. That set could be pretty big, but nevertheless it is finite, so we just need to list out all of the possible words together with their individual probabilities:\nOnce this is constructed, the model simulates from the distribution. You could imagine that the code might look something like sample(words, size = 1, prob = conditional_probs). So say that happens and the next word we simulate is “the.” That gets added to the end, and now we want to know \\(P(\\text{next word}\\,|\\,\\texttt{\"What color is the apple? the\"})\\). So we query the underlying model, and it constructs a new distribution on words, conditional on this revised context:\nNotice that the information we are conditioning on has changed, so the probabilities change. Based on the new probabilities, we simulate the next word. Let’s say it ends up being “apple.” So we add that to the end, and ask the model to construct a new distribution for \\(P(\\text{next word}\\,|\\,\\texttt{\"What color is the apple? the apple\"})\\):\nNow we simulate “is,” add that to the end, and query the model for \\(P(\\text{next word}\\,|\\,\\texttt{\"What color is the apple? the apple is\"})\\):\nWe see at this point that “red” is the most likely next word, but we are not searching the table for the most likely next word and slavishly returning it. We are simulating. So there’s a 35% chance we’ll come up with “red,” but also a non-trivial possibility that we’ll come up with “green.” You will notice that if you ask ChatGPT the exact same question multiple times (literally the identical text prompt), it returns different answers. That is because of this element of randomness.\nObviously, the elephant in the room in the above discussion is “where did the probabilities come from?” This is the magic, such as it is, of LLMs. In order to implement this scheme, we need to be able to construct a conditional probability distribution on words given any context. So no matter if you give it “What color is the apple” or “Explain string theory like I’m a genius” or “Why did my wife leave me” or “que se passe t’il,” it needs to come up with a meaningful and useful distribution for the next word, and it has to repeat this thousands if not millions of times. There are so many possibilities that it’s kind of bonkers to expect to be able to do this, but with enough data and computing power and a big enough model, apparently we can pull it off, at least some of the time.\nSo, the question of “where did the probabilities come from?” is sadly beyond the scope of our course. However, the question of “what do you do with the probabilities once you have them?” is perfectly accessible to us, even at this early stage, so let’s have some fun.",
    "crumbs": [
      "Labs",
      "Lab 4"
    ]
  },
  {
    "objectID": "labs/lab-4.html#task-1",
    "href": "labs/lab-4.html#task-1",
    "title": "Lab 4 - How does ChatGPT generate the next word?",
    "section": "Task 1",
    "text": "Task 1\nWe have provided you with a pre-trained language model kn, a vector midsummer_words containing the 3,063 unique words in the play, and a function next_word_probs:\n\ncond_dist &lt;- next_word_probs(context, midsummer_words, kn)\ntail(cond_dist)\n\n         words         prob\n3058  unearned 3.375323e-05\n3059    'scape 3.375323e-05\n3060 serpent's 3.375323e-05\n3061    amends 6.751470e-05\n3062      liar 3.375323e-05\n3063   restore 3.375323e-05\n\n\nSo, given the context, the set of possible words, and the model, it returns a table (a data frame) listing out all the words together with their conditional probabilities given the context.\nRun this function and use the output together with the sample function to simulate a new word.",
    "crumbs": [
      "Labs",
      "Lab 4"
    ]
  },
  {
    "objectID": "labs/lab-4.html#task-2",
    "href": "labs/lab-4.html#task-2",
    "title": "Lab 4 - How does ChatGPT generate the next word?",
    "section": "Task 2",
    "text": "Task 2\nUse the paste function to append the word you simulated in Task 1 to the end of context. Then, starting from this updated context, go back to the beginning, generate a new conditional distribution, and use it to simulate a new word.",
    "crumbs": [
      "Labs",
      "Lab 4"
    ]
  },
  {
    "objectID": "labs/lab-4.html#task-3",
    "href": "labs/lab-4.html#task-3",
    "title": "Lab 4 - How does ChatGPT generate the next word?",
    "section": "Task 3",
    "text": "Task 3\nWrite a for loop that iterates the process in Tasks 1 and 2 in order to add six new words to the context.\n\n\n\n\n\n\nThis might be a tad slow.\n\n\n\nThe function next_word_probs is doing a lot of work under the hood, so in the upcoming tasks, do not be alarmed if it takes a few seconds to run.",
    "crumbs": [
      "Labs",
      "Lab 4"
    ]
  },
  {
    "objectID": "labs/lab-4.html#task-4",
    "href": "labs/lab-4.html#task-4",
    "title": "Lab 4 - How does ChatGPT generate the next word?",
    "section": "Task 4",
    "text": "Task 4\nRerun your code five times to generate five different responses. How many of them would you consider approximately coherent?",
    "crumbs": [
      "Labs",
      "Lab 4"
    ]
  },
  {
    "objectID": "labs/lab-4.html#task-5",
    "href": "labs/lab-4.html#task-5",
    "title": "Lab 4 - How does ChatGPT generate the next word?",
    "section": "Task 5",
    "text": "Task 5\nNow that you have the basic code, play around with different starting contexts besides “now fair hippolyta.” Bear in mind that our world is small here. We’ve trained the model on a single, old text, so if you want coherent responses, you should probably give it something in the ballpark of what it has seen before. I doubt “which banger slaps hardest my dude” will yield anything meaningful (though feel free to try!). For inspiration, here is the full text of the original play.",
    "crumbs": [
      "Labs",
      "Lab 4"
    ]
  },
  {
    "objectID": "labs/lab-4.html#task-6",
    "href": "labs/lab-4.html#task-6",
    "title": "Lab 4 - How does ChatGPT generate the next word?",
    "section": "Task 6",
    "text": "Task 6\nUp to now we’ve been worrying a lot about individual words, but individual words don’t create meaning. It’s how those words interact across phrases and sentences and paragraphs. Unfortunately, we do not have methods for inducing a probability distribution over the set of all possible sentences, or the set of all possible thoughts. But in a sense, language models are attempting to approximate such a thing. And on the evidence to date, maybe they succeed sometimes. Anyway, how do we go from a method that gets things right on the word level to a method that gets things right on the sentence level? Well, therein lies the art of the engineers that work on these things, and one method they have found helpful is called temperature sampling.\nSo far, given the current context, the model returns a (conditional) probability \\(p(i)\\) for each word \\(i\\in \\text{corpus}\\). So far we’ve used these probabilities unmodified. With temperature sampling, before we draw the next word, we first adjust the probabilities based on a user-defined tuning parameter \\(t&gt;0\\) called the temperature:\n\\[\np_t(i) = \\frac{p(i)^{1/t}}{\\sum\\limits_{j\\in\\text{corpus}}p(j)^{1/t}},\\quad i\\in\\text{corpus}.\n\\]\nIf \\(t=1\\), we’re back to the original probabilities: \\(p_1(i)=p(i)\\). Otherwise, to quote the \\(k\\)-grams people, “higher and lower temperatures make the original probability distribution smoother and rougher, respectively. By making a physical analogy, we can think of less probable words as states with higher energies, and the effect of higher (lower) temperatures is to make more (less) likely to excite these high energy states.” Why do people do this? To the best of my knowledge, it is no deeper than “it seems to work well.” In practice, adjusting the temperature seems to have a macro effect on the overall character of the sentences.\nThe function next_word_probs that we gave you has a fourth argument temp where you can give it a number and it will give you the tempered probabilities. Play around with big and small values, and report any effect that it has on the overall quality of the sentences.\n\n\n\n\n\n\nThink about it\n\n\n\nAs you increase the temperature and \\(t\\to\\infty\\), what does that do to the probabilities?",
    "crumbs": [
      "Labs",
      "Lab 4"
    ]
  },
  {
    "objectID": "labs/lab-4.html#task-7-optional",
    "href": "labs/lab-4.html#task-7-optional",
    "title": "Lab 4 - How does ChatGPT generate the next word?",
    "section": "Task 7 (optional)",
    "text": "Task 7 (optional)\nCode temperature sampling yourself. Take the probabilities that probs &lt;- next_word_probs(...) gives you when temp = 1.0, and then use basic R commands to apply the formula above and compute a vector temp_probs with the revised numbers for temp = 0.5. Check that your numbers agree with the ones that our function next_word_probs gives when you supply 0.5 as the fourth argument.",
    "crumbs": [
      "Labs",
      "Lab 4"
    ]
  },
  {
    "objectID": "labs/lab-7.html",
    "href": "labs/lab-7.html",
    "title": "Lab 7",
    "section": "",
    "text": "Let \\(X\\) be a random variable, let \\(g\\) be a transformation, and consider the new random variable \\(Y=g(X)\\). If all we cared about was the mean of \\(Y\\), then LOTUS provides a nice shortcut for computing \\(E(Y)=E[g(X)]\\) directly. But often, we want to know the entire distribution of the new random variable \\(Y\\), not just the mean. Today we will learn how to calculate it.\n\n\n\n\n\n\nThe template\n\n\n\nIn all that follows, \\(X\\) is continuous with pdf \\(f_X\\) and cdf \\(F_X\\). The goal in each case is to compute the density \\(f_Y\\) of the new random variable \\(Y=g(X)\\). The process for doing this is always the same:\n\nPlot the transformation \\(g\\) so you understand what’s going on;\nFigure out what \\(\\text{Range}(Y)\\) is;\nFind the cdf of \\(Y\\): \\(F_Y(y)=P(Y\\leq y)\\);\nDifferentiate it to get the pdf: \\(f_Y(y)=F'_Y(y)\\).\n\nThis is the so-called “cdf method.”\n\n\nTask 1\nConsider \\(X\\sim \\text{Gamma}(1,\\,1)\\). This is the same as saying \\(X\\sim\\text{Exponential}(1)\\), which we saw in lecture on Monday.\n\nWhat are the range and pdf of \\(Y=\\ln X\\)?\nSimulate n = 5000 draws of \\(Y\\). Plot a histogram, and add a line plot of the density you derived in part b. They should match!\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nTo begin, we know that \\(\\text{Range}(X)=[0,\\,\\infty)\\) and\n\\[\n\\begin{aligned}\nF_X(x)&=1-e^{-x},&&x\\geq0\\\\\nf_X(x) &= e^{-x}, &&x\\geq0.\n\\end{aligned}\n\\]\nOne of our math helpers displays the graph of the natural log and its inverse, the exponential. Consulting these, we note a few things:\n\nthe natural log takes in strictly positive numbers and spits out both positive and negative numbers;\nboth the natural log and the exponential are strictly increasing. In other words, they are order-preserving.\n\nThe first property implies that \\(\\text{Range}(Y)=\\mathbb{R}\\). So fix arbitrary \\(y\\in \\mathbb{R}\\). The cdf of \\(Y\\) is\n\\[\n\\begin{aligned}\nF_Y(y)&=P(Y\\leq y)\\\\\n&=P(\\ln X\\leq y)\\\\\n&=P(e^{\\ln X}\\leq e^y)\\\\\n&=P\\left(X\\leq e^y\\right)\\\\\n&=F_X\\left(e^y\\right)\\\\\n&=1-e^{-e^y}.\n\\end{aligned}\n\\]\nSince the exponential function is order-preserving, we didn’t have to chage the inequality when we applied it to both sides. To get the pdf, take a derivative (using the chain rule!):\n\\[\nf_Y(y)=F_Y'(y)=\\frac{\\text{d}}{\\text{d} y}\\left(1-e^{-e^y}\\right)=0-(-e^y)e^{-e^y}=e^{y-e^y},\\quad y\\in\\mathbb{R}.\n\\]\nIf we compare this density to the histogram from a simulation, they agree:\n\nset.seed(20)\nX &lt;- rgamma(5000, shape = 1, rate = 1)\nY &lt;- log(X)\nhist(Y, breaks = \"Scott\", freq = FALSE)\ncurve(exp(x - exp(x)), from = min(Y), to = max(Y), n = 1000, \n      col = \"red\", lwd = 3, add = TRUE)\n\n\n\n\n\n\n\nPhew!\n\n\n\nTask 2\nConsider \\(X\\sim\\text{Unif}(-\\pi/2,\\,\\pi/2)\\).\n\nWhat are the range and pdf of \\(Y=\\tan X\\)?\nSimulate n = 5000 draws of \\(Y\\). Plot a histogram, and add a line plot of the density you derived in part b. Can you get them to match?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nBy construction, \\(\\text{Range}(X)=(-\\pi/2,\\,\\pi/2)\\). You can read more about the continuous uniform here, and we see see that the cdf is\n\\[\nF_X(x)\n=\n\\begin{cases}\n0 & x\\leq -\\frac{\\pi}{2}\\\\\n\\frac{x-\\left(-\\frac{\\pi}{2}\\right)}{\\frac{\\pi}{2}-\\left(-\\frac{\\pi}{2}\\right)} & -\\frac{\\pi}{2}&lt;x&lt;\\frac{\\pi}{2}\\\\\n1 & \\frac{\\pi}{2}\\leq x.\n\\end{cases}\n\\]\nSo when \\(-\\frac{\\pi}{2}&lt;x&lt;\\frac{\\pi}{2}\\), we have\n\\[\nF_X(x)=\\frac{x+\\frac{\\pi}{2}}{\\pi}=\\frac{x}{\\pi}+\\frac{1}{2}.\n\\]\nThe transformation is\n\n\n\n\n\n\n\n\nAgain, strictly-increasing, and it maps to all real numbers. So \\(\\text{Range}(Y)=\\mathbb{R}\\). Now let’s fix arbitrary \\(y\\in\\mathbb{R}\\) and find the cdf:\n\\[\n\\begin{aligned}\n    F_Y(y)&=P(Y\\leq y)\\\\\n    &=P(\\tan X\\leq y)\\\\\n    &=P(X\\leq \\tan^{-1}y)\\\\\n    &=F_X(\\tan^{-1}y)\\\\\n    &=\\frac{\\tan^{-1}y}{\\pi}+\\frac{1}{2}.\n\\end{aligned}\n\\]\nTake a derivative to get the pdf:\n\\[\nf_Y(y)\n=\nF'_Y(y)\n=\n\\frac{\\textrm{d}}{\\textrm{d}y}\n\\left(\n\\frac{\\tan^{-1}y}{\\pi}+\\frac{1}{2}\n\\right)\n=\n\\frac{1}{\\pi(1+y^2)},\\quad y\\in\\mathbb{R}.\n\\]\nRest assured that we did the math right, but you’d have a hard time verifying it with a simulation:\n\nset.seed(20)\nX &lt;- runif(5000, -pi / 2, pi / 2)\nY &lt;- tan(X)\nhist(Y, breaks = \"Scott\", freq = FALSE)\n\n\n\n\n\n\n\nHere is the graph of the density we derived, and for reference I added the pdf of the standard normal distribution (bell curve):\n\nCodecurve(dnorm(x), from = -7, to = 7, n = 1000, ylab = \"\", bty = \"n\")\ncurve(dcauchy(x), from = -7, to = 7, n = 1000, add = TRUE, col = \"red\")\nlegend(\"topright\", c(\"N(0, 1) density\", \"Cauchy density\"), bty = \"n\",\n       lty = 1, col = c(\"black\", \"red\"))\n\n\n\n\n\n\n\nCompared to the bell curve, this new density has very heavy tails. They are so heavy in fact that this distribution does not have a finite mean or variance or any other moment. It is very ill-behaved, and it frequently surprises with extreme values far out in the tails. That’s why the histogram looks awful and always will.\nThis distribution we just derived is quite famous. It’s called the Cauchy distribution.\n\n\n\nTask 3\nConsider \\(X\\sim \\text{N}(0,\\,1)\\).\n\nWhat are the range and pdf of \\(Y=e^X\\)?\nSimulate n = 5000 draws of \\(Y\\). Plot a histogram, and add a line plot of the density you derived in part b. They should match!\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe know that \\(\\text{Range}(X)=\\mathbb{R}\\) and that\n\\[\n\\begin{aligned}\nf_X(x)&=\\frac{1}{\\sqrt{2\\pi}}e^{-x^2/2},&&x\\in\\mathbb{R}\\\\\nF_X(x)&=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^xe^{-t^2/2}\\,\\text{d}t,&&x\\in\\mathbb{R}.\n\\end{aligned}\n\\]\nNewsflash: the cdf of the normal distribution is unknown. We can approximate it arbitrarily well using numerical methods, but we don’t actually have a clean formula for it. The integral above is simply intractable.\nAnyway, the transformations involved here are again the exponential and the natural log, which we already saw in Task 1. Both are strictly-increasing. The exponential takes any real number and maps it to something strictly positive, so we see then that \\(\\text{Range}(Y)=(0,\\,\\infty)\\). Fixing \\(y&gt;0\\), we compute:\n\\[\n  \\begin{align*}\n      F_Y(y)&=P(Y\\leq y)\\\\\n      &=P(e^X\\leq y)\\\\\n      &=P\\left(X\\leq \\ln y\\right)\\\\\n      &=F_X\\left(\\ln y\\right).\n  \\end{align*}\n\\]\nAs such, the pdf is:\n\\[\n\\begin{aligned}\nf_Y(y)\n=\nF'_Y(y)&=\\frac{\\text{d}}{\\text{d} y}F_X(\\ln y)\\\\\n&=F_X'(\\ln y)\\frac{\\text{d}}{\\text{d}y}\\ln y\\\\\n&=f_X(\\ln y)\\frac{1}{y}\\\\\n&=\\frac{1}{\\sqrt{2\\pi}}e^{-(\\ln y)^2/2}\\frac{1}{y},&&y&gt;0.\n\\end{aligned}\n\\]\nThis matches with a simulation:\n\nset.seed(20)\nX &lt;- rnorm(5000)\nY &lt;- exp(X)\nhist(Y, breaks = \"Scott\", freq = FALSE)\ncurve(dnorm(log(x)) / x, from = min(Y), to = max(Y), n = 1000, \n      col = \"red\", lwd = 3, add = TRUE)\n\n\n\n\n\n\n\nThe distribution we derived here is an example of the log-normal distribution.\n\n\n\nInterlude\nGiven \\(X\\sim f_X\\), we want the density of \\(Y=g(X)\\). In the three examples, you followed the same basic steps:\n\nwrite the generic cdf of \\(Y\\): \\(F_Y(y)=P(Y\\leq y)=P(g(X)\\leq y)\\);\nrewrite the event inside until \\(X\\) is by itself (you probably have to undo or invert \\(g\\) to do this);\nsince \\(X\\) is by itself, rewrite the probability using the cdf of \\(X\\);\ndifferentiate (you probably have to use the chain rule to do this).\n\nThe basic template can be neatly summarized with the so-called change-of-variables formula, which tells you how to compute the density of \\(Y\\) using the density of \\(X\\) and the transformation \\(g\\):\n\\[\nf_Y(y)=f_X\\left(g^{-1}(y)\\right)\n    \\left|\\frac{\\text{d}}{\\text{d} y}g^{-1}(y)\\right|,\\quad y\\in\\text{Range}(Y).\n\\]\n\n\n\n\n\n\nProof\n\n\n\n\n\nAssume…\n\n\n\\(X\\) is absolutely continuous with pdf \\(f_X\\);\n\n\\(g\\) is defined for all \\(x\\in\\text{Range}(X)\\);\n\n\\(g\\) is differentiable and either strictly increasing or strictly decreasing (hence, invertible).\n\nThere are two cases:\n\n\nStrictly increasing\nIf \\(g\\) is strictly increasing, then \\(g^{-1}\\) is strictly increasing also. This means that \\(\\frac{\\text{d}}{\\text{d} y}g^{-1}(y)\\) must be positive for all \\(y\\)! It also means that \\(g^{-1}\\) is order-preserving. If \\(a\\leq b\\), then \\(g^{-1}(a)\\leq g^{-1}(b)\\).\nFix arbitrary \\(y\\in\\text{Range}(Y)\\). Then:\n\\[\n\\begin{aligned}\n    F_Y(y)\n    &=\n    P(Y\\leq y)\n    \\\\\n    &=\n    P\\left(g(X)\\leq y\\right)\n    \\\\\n    &=\n    P\\left(X\\leq g^{-1}(y)\\right)\n    \\\\\n    &=\n    F_X\\left(g^{-1}(y)\\right)\n    .\n\\end{aligned}\n\\]\nTaking a derivative and applying the chain rule gives\n\\[\n\\begin{aligned}\n    f_Y(y)\n    &=\n    \\frac{\\text{d}}{\\text{d} y}F_X\\left(g^{-1}(y)\\right)\n    \\\\\n    &=\n    f_X\\left(g^{-1}(y)\\right)\\underbrace{\\frac{\\text{d}}{\\text{d} y}g^{-1}(y)}_{\\text{positive!}}\n    \\\\\n    &=\n    f_X\\left(g^{-1}(y)\\right)\n    \\left|\\frac{\\text{d}}{\\text{d} y}g^{-1}(y)\\right|.\n\\end{aligned}\n\\]\n\nStrictly decreasing\nIf \\(g\\) is strictly decreasing, then \\(g^{-1}\\) is strictly decreasing also. This means that \\(\\frac{\\text{d}}{\\text{d} y}g^{-1}(y)\\) must be negative for all \\(y\\)! It also means that \\(g^{-1}\\) is order-reversing. If \\(a\\leq b\\), then \\(g^{-1}(a)\\geq g^{-1}(b)\\).\nFix arbitrary \\(y\\in\\text{Range}(Y)\\). Then:\n\\[\n\\begin{aligned}\n    F_Y(y)\n    &=\n    P(Y\\leq y)\n    \\\\\n    &=\n    P\\left(g(X)\\leq y\\right)\n    \\\\\n    &=\n    P\\left(X&gt; g^{-1}(y)\\right)\n    \\\\\n    &=\n    1-P\\left(X\\leq g^{-1}(y)\\right)\n    \\\\\n    &=\n    1-F_X\\left(g^{-1}(y)\\right)\n    .\n\\end{aligned}\n\\] Taking a derivative and applying the chain rule gives\n\\[\n\\begin{align*}\n    f_Y(y)\n    &=\n    \\frac{\\text{d}}{\\text{d} y}[1-F_X\\left(g^{-1}(y)\\right)]\n    \\\\\n    &=\n    {\\color{red}-}f_X\\left(g^{-1}(y)\\right)\\underbrace{\\frac{\\text{d}}{\\text{d} y}g^{-1}(y)}_{\\text{negative!}}\n    \\\\\n    &=\n    f_X\\left(g^{-1}(y)\\right)\n    \\left|\\frac{\\text{d}}{\\text{d} y}g^{-1}(y)\\right|.\n\\end{align*}\n\\]\n\n\nEither way, we got to the same place.\n\n\n\nTask 4\nLet \\(X\\) be an absolutely continuous random variable with pdf \\(f_X\\), let \\(a,\\,b\\in\\mathbb{R}\\) be non-zero constants, and consider the new random variable \\(Y=aX+b\\).\n\nWhat is \\(E(Y)\\)?\nWhat is \\(\\text{var}(Y)\\)?\nUse the change-of-variables formula to find the density of \\(Y\\)?\nIn the special case where \\(X\\sim\\text{N}(0,\\,1)\\), what are \\(E(Y)\\), \\(\\text{var}(Y)\\), and the density of \\(Y\\)? There’s no need to re-derive anything here. Just plug what you know about N(0, 1) into the formulas from parts a, b, and c.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe know from lecture that \\(E(Y)=E(aX+b)=aE(X)+b\\) and \\(\\text{var}(Y)=\\text{var}(aX+b)=a^2\\text{var}(X)\\).\nIf \\(g(x)=ax+b\\) is the linear transformation, then the inverse transformation and its derivative are\n\\[\n\\begin{align*}\n        g^{-1}(y)&=\\frac{y-b}{a}\\\\\n        \\frac{\\text{d}}{\\text{d} y}g^{-1}(y)&=\\frac{1}{a},\n    \\end{align*}\n\\]\nand the change-of-variables formula gives:\n\\[\nf_Y(y)=f_X\\left(\\frac{y-b}{a}\\right)\\frac{1}{|a|}.\n\\]\nIf \\(X\\sim\\text{N}(0,\\,1)\\), then we know from lecture that \\(E(X)=0\\), \\(\\text{var}(X)=1\\), and\n\\[\nf_X(x)=\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}x^2\\right),\\quad x\\in\\mathbb{R}.\n\\]\nA new random variable \\(Y=aX+b\\) has\n\\[\nE(Y)=aE(X)+b=a\\cdot 0+b=b,\n\\]\nand\n\\[\n\\text{var}(Y)=a^2\\text{var}(X)=a^2\\cdot 1=a^2,\n\\]\nand\n\\[\nf_Y(y)=\\frac{1}{|a|}\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}\\left(\\frac{x-b}{a}\\right)^2\\right)=\\frac{1}{\\sqrt{2\\pi a^2}}\\exp\\left(-\\frac{1}{2}\\frac{\\left(x-b\\right)^2}{a^2}\\right).\n\\]\nSo \\(Y\\sim \\text{N}(b,\\, a^2)\\). That’s where the general normal comes from: you apply a linear transformation to the standard normal.",
    "crumbs": [
      "Labs",
      "Lab 7"
    ]
  },
  {
    "objectID": "problems/pset-7.html",
    "href": "problems/pset-7.html",
    "title": "Problem Set 7",
    "section": "",
    "text": "Final exam prep\n\n\n\nThe final exam includes one problem on maximum likelihood estimation, and you can expect it to look similar to Problems 3 and 4 below. The final exam closes with one problem on Bayesian statistics, and you can expect it to look similar to Problems 5 and 6 below.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 7"
    ]
  },
  {
    "objectID": "problems/pset-7.html#problem-0",
    "href": "problems/pset-7.html#problem-0",
    "title": "Problem Set 7",
    "section": "Problem 0",
    "text": "Problem 0\nDoodle a cute character that will cheer you on during this assignment.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 7"
    ]
  },
  {
    "objectID": "problems/pset-7.html#problem-1",
    "href": "problems/pset-7.html#problem-1",
    "title": "Problem Set 7",
    "section": "Problem 1",
    "text": "Problem 1\nLet \\(Z\\sim\\text{N}(0,\\,1)\\), and recall what that means:\n\\[\n\\begin{aligned}\nf_Z(z)&=\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}z^2\\right), && -\\infty&lt;z&lt;\\infty\\\\\nM_Z(t)&=\\exp\\left(\\frac{1}{2}t^2\\right), && -\\infty&lt;t&lt;\\infty.\n\\end{aligned}\n\\]\nNext, let \\(X=\\mu+\\sigma Z\\) for some constants \\(\\mu\\in\\mathbb{R}\\) and \\(\\sigma&gt;0\\).\n\nUse the change-of-variables formula to derive density of \\(X\\). What is its distribution?\nWhat is the moment-generating function of \\(X\\)?\nWhat are the mean and variance of \\(X\\)? Make sure you justify your answer with some type of derivation.\nConsider \\(X_1,\\,X_2,\\,...,\\,X_n\\overset{\\text{iid}}{\\sim}\\text{N}(\\mu,\\,\\sigma^2)\\) and derive the distribution of their sum and their average.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 7"
    ]
  },
  {
    "objectID": "problems/pset-7.html#problem-2",
    "href": "problems/pset-7.html#problem-2",
    "title": "Problem Set 7",
    "section": "Problem 2",
    "text": "Problem 2\nIf the discrete random variable \\(X\\) has the funky-ass distribution (FAS), then its range is \\(\\text{Range}(X)=\\mathbb{N}\\) and its pmf is\n\\[\nP(X=k)=\\binom{k+r-1}{k}(1-p)^kp^r\\quad k=0,\\,1,\\,2,\\,3,\\,...,\n\\]\nwhere the parameters \\(r\\in\\mathbb{N}\\) and \\(0&lt;p&lt;1\\) are constants. We denote this \\(X\\sim\\text{FAS}(r,\\,p)\\).\n\nUse what you know about probability to find the value of this infinite series:\n\n\\[\n\\sum\\limits_{k=0}^\\infty\n\\binom{k+r-1}{k}(1-p)^k.\n\\]\n\nCompute the MGF of \\(X\\);\nUse the definition of the expected value to compute the mean of \\(X\\);\nUse the MGF to compute the mean of \\(X\\) and verify that you get the same answer you got before;\nConsider an iid collection \\(X_i\\overset{\\text{iid}}{\\sim}\\text{FAS}(r,\\,p)\\) and derive the distribution of the sum \\(S_n=\\sum_{i=1}^nX_i\\).",
    "crumbs": [
      "Problem Sets",
      "Problem Set 7"
    ]
  },
  {
    "objectID": "problems/pset-7.html#problem-3",
    "href": "problems/pset-7.html#problem-3",
    "title": "Problem Set 7",
    "section": "Problem 3",
    "text": "Problem 3\nConsider these data:\n\\[\nX_1,\\,X_2,\\,...,\\,X_n\\overset{\\text{iid}}{\\sim}\\text{N}(0,\\,\\theta).\n\\]\n\nWhat is the maximum likelihood estimator of \\(\\theta&gt;0\\)?\nWhat is the sampling distribution of the estimator?\nWhat is the MSE of the estimator?\nBased on the MSE, what are the statistical properties of this estimator?",
    "crumbs": [
      "Problem Sets",
      "Problem Set 7"
    ]
  },
  {
    "objectID": "problems/pset-7.html#problem-4",
    "href": "problems/pset-7.html#problem-4",
    "title": "Problem Set 7",
    "section": "Problem 4",
    "text": "Problem 4\nLet \\(X_1\\), \\(X_2\\), …, \\(X_n\\) be iid from some member of this parametric family:\n\\[\nf(x\\,|\\,\\theta)\n=\n\\frac{1}{2\\theta}\\exp\\left(-\\frac{|x|}{\\theta}\\right), \\quad -\\infty&lt;x&lt;\\infty.\n\\]\n\nWhat is the maximum likelihood estimator of \\(\\theta&gt;0\\)?\nWhat is the sampling distribution of the estimator?\nWhat is the MSE of the estimator?\nBased on the MSE, what are the statistical properties of this estimator?",
    "crumbs": [
      "Problem Sets",
      "Problem Set 7"
    ]
  },
  {
    "objectID": "problems/pset-7.html#problem-5",
    "href": "problems/pset-7.html#problem-5",
    "title": "Problem Set 7",
    "section": "Problem 5",
    "text": "Problem 5\nImagine that I quit my job and open a factory that manufactures bow ties and light bulbs (Zito’s Bows and Bulbs). The ties are alright, but the bulbs suck. They burn out real quick. Each bulb is slightly different, and you can’t perfectly predict how long they will last, so the time (in hours) until the bulb dies is a random variable \\(X\\), and let’s assume \\(X\\sim\\text{Exponential}(\\lambda)\\), where \\(\\lambda&gt;0\\) is unknown. Recall that \\(E(X)=1/\\lambda\\), so the larger the rate, the sooner the burnout time.\nI want to estimate \\(\\lambda\\) to get a sense of how bad my light bulbs are, so I sample \\(n\\) bulbs and record their burnout times:\n\\[\nX_1,\\,X_2,\\,...,\\,X_n\\overset{\\text{iid}}{\\sim}\\text{Exponential}(\\lambda).\n\\]\nAt this point, I could just use the method of maximum likelihood to estimate \\(\\lambda\\) like we did in lecture, but before I do, I go and consult Great Grandma Zito. She’s been making bad light bulbs for decades and taught me everything that I know. She says that in her experience, \\(\\lambda\\) is in the ballpark of 1 (meaning our bulbs burn out in an hour, on average), but there’s uncertainty about that. In her opinion, the probability that \\(\\lambda&gt;3\\) is about 1.7%.\nI want to incorporate my grandmother’s prior knowledge into my analysis, so I decide to be Bayesian:\n\\[\n\\begin{aligned}\n\\lambda &\\sim\\text{Gamma}(\\alpha_0,\\,\\beta_0) && \\text{(prior)}\\\\\nX_1,\\,X_2,\\,...,\\,X_n\\,|\\,\\lambda&\\overset{\\text{iid}}{\\sim}\\text{Exponential}(\\lambda) && \\text{(data model)}.\n\\end{aligned}\n\\]\n\\(\\lambda \\sim\\text{Gamma}(\\alpha_0,\\,\\beta_0)\\) is my prior distribution for the unknown parameter, and \\(\\alpha_0,\\,\\beta_0&gt;0\\) are hyperparameters that I will tune in order to encode the prior knowledge about \\(\\lambda\\) that my grandmother described. I chose the gamma family simply because it is convenient and familiar to me, and I know that \\(\\lambda\\) is a continuous numerical quantity that must be positive.\n\nShow that the posterior distribution for \\(\\lambda\\) in this model is\n\\[\n\\lambda\\,|\\,X_{1:n}=x_{1:n} \\sim \\text{Gamma}(\\alpha_n,\\,\\beta_n).\n\\]\nAfter we see some data, what are the revised hyperparameters \\(\\alpha_n,\\,\\beta_n\\) equal to?\n\n\n\n\n\n\n\nPay attention to the notation here\n\n\n\nBefore I see any data, \\(\\text{Gamma}(\\alpha_0,\\,\\beta_0)\\) summarizes my beliefs about \\(\\lambda\\). After I see some data, \\(\\text{Gamma}(\\alpha_n,\\,\\beta_n)\\) summarizes my beliefs about \\(\\lambda\\). \\(\\alpha_0\\) and \\(\\beta_0\\) are the prior hyperparameters, and \\(\\alpha_n\\) and \\(\\beta_n\\) are the posterior hyperparameters. The subscript indicates how much data my beliefs are based on.\n\n\n\nShow that the posterior mean has the form\n\\[\nE(\\lambda\\,|\\,X_1,\\,X_2,\\,...,\\,X_n)=w_n\\hat{\\lambda}_n^{(MLE)} + (1-w_n)\\underbrace{E(\\lambda)}_{\\text{prior mean}},\n\\]\nwhere \\(w_n\\in(0,\\,1)\\) might depend on the data. This means that the posterior mean is a shrinkage estimator. We shrink the MLE toward our prior estimate of the parameter.\nHow should the prior hyperparameters \\(\\alpha_0\\) and \\(\\beta_0\\) be set so that the prior distribution captures my grandmother’s beliefs about \\(\\lambda\\)? There’s not a tremendous amount of math here. It’s just trial-and-error until you find numbers that work.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 7"
    ]
  },
  {
    "objectID": "problems/pset-7.html#problem-6",
    "href": "problems/pset-7.html#problem-6",
    "title": "Problem Set 7",
    "section": "Problem 6",
    "text": "Problem 6\nConsider the following Bayesian model:\n\\[\n\\begin{aligned}\np&\\sim\\text{Beta}(a_0,\\,b_0)&& \\text{(prior)}\\\\\nX_1,\\,X_2,\\,...,\\,X_n\\,|\\,p&\\overset{\\text{iid}}{\\sim}\\text{Geometric}(p).&& \\text{(likelihood)}\n\\end{aligned}\n\\]\n\nWhat is the posterior distribution?\nCompute the posterior mean and show that it is a weighted average of the prior mean and the maximum likelihood estimator.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 7"
    ]
  },
  {
    "objectID": "problems/pset-7.html#submission",
    "href": "problems/pset-7.html#submission",
    "title": "Problem Set 7",
    "section": "Submission",
    "text": "Submission\nYou are free to compose your solutions for this problem set however you wish (scan or photograph written work, handwriting capture on a tablet device, LaTeX, Quarto, whatever) as long as the final product is a single PDF file. You must upload this to Gradescope and mark the pages associated with each problem.\nDo not forget to include the following:\n\nFor each problem, please acknowledge your collaborators;\nIf a problem required you to code something, please include both the code and the output. “Including the code” can be as crude as a screenshot, but you might also use Quarto to get a nice lil’ pdf that you can merge with the rest of your submission.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 7"
    ]
  },
  {
    "objectID": "problems/bank/counting/bridge.html",
    "href": "problems/bank/counting/bridge.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Four people are going to play bridge, which begins with an entire 52-card deck being dealt at random, 13 cards per player. What is the probability that each player is dealt exactly one ace?\n\n\n\n\n\n\nStill confused about ordered versus unordered sampling?\n\n\n\nThere are several ways to do this calculation. You should try it on your own, but then, go read Example 1.8.9 and the subsequent discussion on pages 37 - 39 of DeGroot & Schervish."
  },
  {
    "objectID": "problems/bank/counting/library-card-PIN.html",
    "href": "problems/bank/counting/library-card-PIN.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Congratulations! You and two of your friends are now proud owners of brand-new library cards. In order to use your new cards, all you and your friends need to do is create four-digit PINs, with numbers \\(0\\) - \\(9\\) available (in theory) at each position. Predictably, all of you have different ideas of what an acceptable PIN is (odd or even, leading zeros or not, etc.).\n\nYou are fairly chill about what you want in a PIN – all you want is an even PIN, and you’re totally fine if it starts with a zero. How many possible PINs fit your preference?\nOne of your friends is a bit more persnickety – they want a PIN divisible by \\(5\\), and they cannot stand PINs starting with a zero. How many possible PINs fit this friend’s preferences?\nYour other friend has some more…particular preferences – they have an affinity for the number \\(16\\) and thus want the digits \\(1\\) and \\(6\\) to appear in that exact order at some point in the PIN. Further, they also despise PINs starting with a zero. How many possible PINs fit this friend’s preferences?"
  },
  {
    "objectID": "problems/bank/counting/poker-dice.html",
    "href": "problems/bank/counting/poker-dice.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Poker dice is played by simultaneously rolling five dice. The dice are fair, six-sided, and distinct (imagine that each one is a different color). Find the probabilities of the following dice rolls:\n\nfive different numbers;\nthree of a kind (three dice show the same number, while the other two dice are singles);\nfull house (three dice show the same number, while the other two show a separate pair)."
  },
  {
    "objectID": "problems/bank/counting/streaks.html",
    "href": "problems/bank/counting/streaks.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Consider a sequence of \\(n\\) independent coin flips of a fair coin. Define a streak of heads as an uninterrupted run of heads, and efine a streak of tails analogously. We are interested in the number of streaks in a sequence of flips. For example, consider the sequence below:\n\\[\nhhhttthhttthhhhhthtthhhh.\n\\]\nIt contains 9 streaks. Here is the sequence again with the streaks separated:\n\\[\nhhh\\quad ttt\\quad hh\\quad ttt\\quad hhhhh\\quad t\\quad h\\quad tt\\quad hhhh.\n\\]\nIn a sequence of \\(n\\) flips of a fair coin, let \\(Y\\) be the number of streaks. What is the range of \\(Y\\), and what is its PMF?"
  },
  {
    "objectID": "problems/bank/counting/ross-balls.html",
    "href": "problems/bank/counting/ross-balls.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Seven balls are randomly withdrawn from an urn that contains 12 red, 16 blue, and 18 green balls. Find the probability that\n\n3 red, 2 blue, and 2 green balls are withdrawn;\nat least 2 red balls are withdrawn;\nall withdrawn balls are the same color;\neither exactly 3 red balls or exactly 3 blue balls are withdrawn."
  },
  {
    "objectID": "problems/bank/counting/ht-receipt.html",
    "href": "problems/bank/counting/ht-receipt.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "A few months ago I went to Harris Teeter to get some junk food, and when I looked at my receipt, I saw this:\n\n\n\n\n\nI thought, “Strange. What are the odds the total would be a whole number?” Well, let’s find out!\n\nImagine you randomly purchase two items that each cost less than $10, and assume all prices are equally likely. The prices of the two items are quoted to two decimal places. What is the probability that the two prices add up to a whole number total?\nNow let’s add a wrinkle. Imagine sales tax is 5%. This is calculated, rounded to the nearest cent, and then added to your bill. What is the probability that the cost of the two items plus the rounded sales tax adds up to a whole number?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nThe answer to part b is roughly 0.01, but showing this is tricky. Give it a shot!"
  },
  {
    "objectID": "problems/bank/counting/elevator.html",
    "href": "problems/bank/counting/elevator.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "An evil cult has its secret meetings in the basement of a building that has 15 above-ground floors. When the meeting is over, the nine cult members enjoy coffee and donuts and then depart to sow the seeds of villainy. They board the elevator, and each person randomly and independently selects one of the 15 floors to visit. Compute the probability that…\n\nNo two passengers share a floor;\nAt least two passengers share a floor;\nExactly two passengers share a floor;\nNo passengers get off at the top floor;\nAt least one passenger gets off at floor 1;\nAll passengers get off on consecutive floors;\nAll passengers get off below floor 10"
  },
  {
    "objectID": "problems/bank/counting/basic-counting.html",
    "href": "problems/bank/counting/basic-counting.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Suppose you are dealt six cards from a well-shuffled, standard deck of 52.\n\nWhat is the probability of getting 3 of one rank and 3 of another?\nWhat is the probability of getting 4 of one rank and 2 of another?\nWhat is the probability of getting 3 aces and 3 of another rank?\nWhat is the probability of getting 4 of one rank and 2 aces?\nWhat is the probability that all 6 cards are in the same suit?\nWhat is the probability that all 6 cards are consecutive (ie a hand with 3 hearts, 4 spades, 5 spades, 6 clubs, 7 hearts, and the 8 diamonds)? Assume that the ace can only be the high card;\nWhat is the answer to the previous question if we allow the ace to be either high or low?\nWhat is the probability of receiving the ace, king, queen, jack, 10 and 9 all in hearts?"
  },
  {
    "objectID": "problems/bank/joint-distributions/beta-gamma-hierarchy.html",
    "href": "problems/bank/joint-distributions/beta-gamma-hierarchy.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Consider a random pair \\((X,\\,Y)\\) with a joint distribution given by this hierarchy:\n\\[\n\\begin{aligned}\n    X\n    &\\sim\n    \\textrm{Beta}(a,\\, b)\n    \\\\\n    Y\\mid X = x\n    &\\sim\n    \\textrm{Gamma}\\left(a+b,\\, \\frac{c}{x}\\right).\n\\end{aligned}\n\\]\n\nWhat is the joint range?\nWhat is the marginal density of \\(Y\\)?\nWhat is the conditional density of \\(X\\)?\n\n\n\n\n\n\n\nNew distribution!\n\n\n\nIf \\(X\\sim \\textrm{Beta}(a,\\, b)\\), then \\(\\text{Range}(X)=(0,\\,1)\\) and the density is\n\\[\nf(x)=\\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)}x^{a-1}(1-x)^{b-1},\\quad0&lt;x&lt;1.\n\\]\nThat’s new to you, so see if you can work with it."
  },
  {
    "objectID": "problems/bank/joint-distributions/gamma-gamma-hierarchy.html",
    "href": "problems/bank/joint-distributions/gamma-gamma-hierarchy.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Consider the joint distribution of random variables \\(X\\) and \\(Y\\), written in hierarchical form:\n\\[\n\\begin{aligned}\nX&\\sim\\textrm{Gamma}\\left(\\frac{d_2}{2},\\, \\frac{d_2}{2}\\right)\\\\\nY\\,|\\, X = x&\\sim\\textrm{Gamma}\\left(\\frac{d_1}{2},\\, \\frac{d_1}{2}x\\right).\\\\\n\\end{aligned}\n\\]\nDo some serious “massage and squint” to show that the marginal pdf of \\(Y\\) is\n\\[\nf_Y(y)=\\frac{\\Gamma\\left(\\frac{d_1}{2}+\\frac{d_2}{2}\\right)}{\\Gamma\\left(\\frac{d_1}{2}\\right)\\Gamma\\left(\\frac{d_2}{2}\\right)}\\left(\\frac{d_1}{d_2}\\right)^{\\frac{d_1}{2}}y^{\\frac{d_1}{2}-1}\\left(1+\\frac{d_1}{d_2}y\\right)^{-\\frac{d_1+d_2}{2}},\\quad y&gt;0.\n\\]\nThis means that \\(Y\\) has the F-distribution."
  },
  {
    "objectID": "problems/bank/joint-distributions/weird-hard-thing.html",
    "href": "problems/bank/joint-distributions/weird-hard-thing.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "\\(X\\) and \\(Y\\) are jointly absolutely continuous with joint density\n\\[\nf_{XY}(x,\\, y)\n=\n\\frac{1}{8}\n(y^2-x^2)\ne^{-y}\n,\\quad\ny&gt;0\n;\\,\n-y&lt;x&lt;y.\n\\]\n\nSketch \\(\\textrm{Range}(X,\\, Y)\\).\nCompute the marginal density of \\(X\\).\nCompute the marginal density of \\(Y\\).\nCompute the conditional density of \\(X\\) given \\(Y = y\\).\nCompute the conditional density of \\(Y\\) given \\(X = x\\)."
  },
  {
    "objectID": "problems/bank/joint-distributions/bdj-joint.html",
    "href": "problems/bank/joint-distributions/bdj-joint.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Here is the joint density for a random pair:\n\\[\nf_{XY}(x,\\,y)\n=\n\\frac{24}{7}\nx(2-y),\\quad 0&lt;x&lt;1\\text{ and }0&lt;y&lt;1-x.\n\\]\n\nSketch the joint range of \\((X,\\,Y)\\).\nFind the marginal density of \\(Y\\).\nFind the marginal density of \\(X\\).\nFind the conditional density of \\(X\\) given \\(Y=y\\).\nUse this to compute \\(P(X&gt;1/2\\,|\\,Y=1/3)\\).\nWhat is \\(E(X\\,|\\,Y=y)\\)?"
  },
  {
    "objectID": "problems/bank/moments/discrete-uniform.html",
    "href": "problems/bank/moments/discrete-uniform.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Let \\(X\\) have the discrete uniform on \\(\\{1,\\,2,\\,...,\\,n\\}\\). This means \\(P(X=i)=1/n\\) for all \\(i = 1,\\,2,\\,...,\\,n\\).\n\nCompute the MGF of \\(X\\);\nUse it to compute the mean;\nUse it to compute the variance."
  },
  {
    "objectID": "problems/bank/moments/laplace.html",
    "href": "problems/bank/moments/laplace.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Let \\(X\\) have density\n\\[\nf_X(x)\n=\n\\frac{1}{2}\ne^{-|x|},\\quad x\\in\\mathbb{R}.\n\\]\n\nWhat is the mgf of \\(X\\)?\nWhat is the mean?\nWhat is the variance?"
  },
  {
    "objectID": "problems/bank/moments/binomial-mgf.html",
    "href": "problems/bank/moments/binomial-mgf.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Compute the moment-generating function of the binomial distribution;\nUse the mgf to compute the mean;\nUse the mgf to compute the variance.\n\nNote: we already know from lecture that the mean is \\(np\\) and the variance is \\(np(1-p)\\), so you know you did it right when you get the same answer."
  },
  {
    "objectID": "problems/bank/moments/simple-mgf.html",
    "href": "problems/bank/moments/simple-mgf.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Let \\(X\\) be a discrete random variable with\n\n\n\n\n\\(x\\)\n-1\n3\n7\n\n\n\n\n\\(P(X=x)\\)\n0.5\n0.2\n0.3\n\n\n\n\n\nCompute the mgf of \\(X\\).\nCompute the mean two ways: using the definition, and using the mgf. Confirm that you get the same answer.\nCompute the variance two ways: using the definition, and using the mgf. Confirm that you get the same answer."
  },
  {
    "objectID": "problems/bank/transformations/twerkout-3.html",
    "href": "problems/bank/transformations/twerkout-3.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "\\(X\\) possesses the goofy distribution if its PDF is:\n\\[\nf(x)=k\\frac{x^{k-1}}{\\theta}\\exp\\left(-\\frac{x^k}{\\theta}\\right),\\quad x&gt;0.\n\\]\nThe constants \\(k\\) and \\(\\theta\\) are both positive parameters. We denote this \\(X\\sim\\text{GF}(k,\\,\\theta)\\)\n\nIf we define a new random variable \\(Y=\\ln X\\), what is its distribution?\nIf we define a new random variable \\(Z = X^k\\), what is its distribution?"
  },
  {
    "objectID": "problems/bank/transformations/gamma-power.html",
    "href": "problems/bank/transformations/gamma-power.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Let \\(X\\sim\\text{Gamma}(\\alpha,\\,\\beta)\\), and define a new random variable \\(Y=X^r\\) for any positive constant \\(r&gt;0\\).\n\nWhat is the density of \\(Y\\)?\nDerive a formula for \\(E(X^n)\\) that works for any \\(n\\in\\mathbb{N}\\)."
  },
  {
    "objectID": "problems/bank/basic-rules/disjoint-complements.html",
    "href": "problems/bank/basic-rules/disjoint-complements.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Suppose the events \\(A\\) and \\(B\\) are disjoint. Under what conditions are \\(A^{c}\\) and \\(B^{c}\\) also disjoint events?"
  },
  {
    "objectID": "problems/bank/basic-rules/geniuses-and-chocolate-lovers.html",
    "href": "problems/bank/basic-rules/geniuses-and-chocolate-lovers.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Out of the students in a class, 60% are geniuses, 70% love chocolate, and 40% fall into both categories. Determine the probability that a randomly selected student is neither a genius nor a chocolate lover."
  },
  {
    "objectID": "problems/bank/basic-rules/xor.html",
    "href": "problems/bank/basic-rules/xor.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Let \\(A\\) and \\(B\\) be events in a sample space \\(S\\). Let \\(C\\) be the set of outcomes that are in either \\(A\\) or \\(B\\), but not both.\n\nDraw a well-labeled picture of \\(S\\), \\(A\\), \\(B\\), and \\(C\\).\nWrite down a formula for \\(C\\) in terms of \\(A\\) and \\(B\\) using any of the basic operations: union (\\(\\cup\\)), intersection (\\(\\cap\\)), complement (\\(^c\\)).\nUse set theory and the probability axioms to show that\n\n\\[\nP(C)=P(A)+P(B)-2P(A\\cap B).\n\\]\n\nExplain this result conceptually (with words and pictures)."
  },
  {
    "objectID": "problems/bank/basic-rules/poisson-measure.html",
    "href": "problems/bank/basic-rules/poisson-measure.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Suppose we have the sample space \\(S=\\mathbb{N}=\\{0,\\,1,\\,2,\\,...\\}\\) and a probability measure \\(P\\) that assigns the following individual probabilities to the singleton sets:\n\\[\nP(\\{i\\})=c\\frac{4^i}{i!},\\quad i\\in\\mathbb{N}.\n\\]\n\nIn order for \\(P\\) to satisfy the axiom of total measure one, what must be the value of the constant \\(c&gt;0\\).\nWhich outcome(s) in the sample space are most likely (ie have the largest individual probability of occurring)? Furthermore, how do you know for a fact that you’ve identified all of them? The sample space is infinite, so presumably you cannot literally check every outcome.\nWhat is the probability of the even numbers?\n\n\n\n\n\n\n\nBe careful!\n\n\n\nI could not care less about the answers to these questions. It’s all about the reasoning, and showing that you understand how the ideas fit together. To earn full credit, make sure you carefully justify everything by making appropriate reference to the rules and axioms."
  },
  {
    "objectID": "problems/bank/basic-rules/blood-type.html",
    "href": "problems/bank/basic-rules/blood-type.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "A simplified model of the human blood-type system has four types: A, B, AB, and O. Suppose that, for a randomly chosen person, the probability of type O blood is 0.5, of type A blood is 0.36, and of type B blood is 0.11. There are two antigens, anti-A and anti-B, that react with a person’s blood in different ways depending on the type. Anti-A reacts with blood types A and AB but not with B and O. Anti-B reacts with blood types B and AB, but not with A and O.\n\nLet \\(\\mathcal{A}\\) be the event that a person’s blood reacts with anti-A, and let \\(\\mathcal{B}\\) be the event that their blood reacts with anti-B. Classify the person’s blood type using the events \\(\\mathcal{A}\\) and \\(\\mathcal{B}\\) and their complements;\nWhat is the probability that both antigens will react with a random person’s blood?\nWhat is the probability that each antigen will react with a random person’s blood?"
  },
  {
    "objectID": "problems/bank/basic-rules/union-bound.html",
    "href": "problems/bank/basic-rules/union-bound.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Let \\(A_1,\\,A_2,\\,A_3,\\,...\\subseteq S\\) be an infinite sequence of possibly overlapping events in some probability space. Based on this arbitrary sequence, define a new sequence of events that starts with \\(B_1 = A_1\\) and then has \\(B_i=A_i\\cap \\left(\\bigcup_{j=1}^{i-1}A_j\\right)^c\\) for all \\(i&gt;1\\).\n\nShow that the \\(B_i\\) are pairwise disjoint.\nShow that\n\n\\[\n\\bigcup_{i=1}^\\infty A_i=\\bigcup_{i=1}^\\infty B_i.\n\\]\n\nUse the previous parts to show that\n\n\\[\nP\\left(\\bigcup_{i=1}^\\infty A_i\\right)\\leq\\sum\\limits_{i=1}^\\infty P(A_i).\n\\]"
  },
  {
    "objectID": "problems/bank/calculus-review/calc-gamma.html",
    "href": "problems/bank/calculus-review/calc-gamma.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Here is another inordinately silly function:\n\\[\n\\Gamma(x)=\\int_0^\\infty y^{x-1}e^{-y}\\,\\textrm{d} y,\\quad x&gt;0.\n\\]\nProve that \\(\\Gamma(x+1)=x\\Gamma(x)\\).\n\n\n\n\n\n\nHint\n\n\n\n\n\nStart on the left-hand side by writing out \\(\\Gamma(x+1)\\) and evaluating the integral by parts."
  },
  {
    "objectID": "problems/bank/calculus-review/calc-inflection.html",
    "href": "problems/bank/calculus-review/calc-inflection.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Here is a very silly function:\n\\[\nh(x)\n=\n\\exp\\left(-\\frac{1}{2}\\frac{(x-\\mu)^2}{\\sigma^2}\\right)\n,\\quad\n-\\infty&lt;x&lt;\\infty\n.\n\\]\nTreat \\(-\\infty&lt;\\mu&lt;\\infty\\) and \\(\\sigma&gt;0\\) as constants and compute the value(s) of \\(x\\) at which \\(h\\) has inflection points.\n\n\n\n\n\n\nHint\n\n\n\n\n\nHere is an example of what \\(h\\) might look like in the special case where \\(\\mu = 1\\) and \\(\\sigma=2\\):\n\nCodem = 1\ns = 2\npar(mar = c(4, 4, 0.1, 4))\ncurve(exp(-0.5 * ((x - m) / s)^2), \n      from = -6, to = 8, n = 500,\n      xlab = \"x\", ylab = \"h(x)\",\n      xaxt = \"n\")\naxis(1, at = -6:8)\n\n\n\n\n\n\n\nBefore you start doing any math, can you use the picture to guess what the answer will be?"
  },
  {
    "objectID": "problems/bank/calculus-review/calc-limit.html",
    "href": "problems/bank/calculus-review/calc-limit.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Let \\(f\\) be any function with the following properties:\nAssume \\(t\\) is a constant and compute\n\\[\n    \\lim_{x\\to\\infty} xf\\left(\\frac{t}{\\sqrt{x}}\\right)\n    .\n\\]"
  },
  {
    "objectID": "problems/bank/calculus-review/calc-limit.html#footnotes",
    "href": "problems/bank/calculus-review/calc-limit.html#footnotes",
    "title": "STA 240 Fall 2025",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis means that \\(f\\) and its first two derivatives are all continuous functions at and around zero.↩︎"
  },
  {
    "objectID": "problems/bank/calculus-review/calc-series.html",
    "href": "problems/bank/calculus-review/calc-series.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Assume \\(\\lambda&gt;0\\) is a constant and compute\n\\[\n    \\sum\\limits_{n=0}^\\infty n \\frac{\\lambda^n}{n!}e^{-\\lambda}\n    .\n\\]"
  },
  {
    "objectID": "problems/bank/bayes/gamma-exponential.html",
    "href": "problems/bank/bayes/gamma-exponential.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Imagine that I quit my job and open a factory that manufactures bow ties and light bulbs (Zito’s Bows and Bulbs). The ties are alright, but the bulbs suck. They burn out real quick. Each bulb is slightly different, and you can’t perfectly predict how long they will last, so the time (in hours) until the bulb dies is a random variable \\(X\\), and let’s assume \\(X\\sim\\text{Exponential}(\\lambda)\\), where \\(\\lambda&gt;0\\) is unknown. Recall that \\(E(X)=1/\\lambda\\), so the larger the rate, the sooner the burnout time.\nI want to estimate \\(\\lambda\\) to get a sense of how bad my light bulbs are, so I sample \\(n\\) bulbs and record their burnout times:\n\\[\nX_1,\\,X_2,\\,...,\\,X_n\\overset{\\text{iid}}{\\sim}\\text{Exponential}(\\lambda).\n\\]\nAt this point, I could just use the method of maximum likelihood to estimate \\(\\lambda\\) like we did in lecture, but before I do, I go and consult Great Grandma Zito. She’s been making bad light bulbs for decades and taught me everything that I know. She says that in her experience, \\(\\lambda\\) is in the ballpark of 1 (meaning our bulbs burn out in an hour, on average), but there’s uncertainty about that. In her opinion, the probability that \\(\\lambda&gt;3\\) is about 1.7%.\nI want to incorporate my grandmother’s prior knowledge into my analysis, so I decide to be Bayesian:\n\\[\n\\begin{aligned}\n\\lambda &\\sim\\text{Gamma}(\\alpha_0,\\,\\beta_0) && \\text{(prior)}\\\\\nX_1,\\,X_2,\\,...,\\,X_n\\,|\\,\\lambda&\\overset{\\text{iid}}{\\sim}\\text{Exponential}(\\lambda) && \\text{(data model)}.\n\\end{aligned}\n\\]\n\\(\\lambda \\sim\\text{Gamma}(\\alpha_0,\\,\\beta_0)\\) is my prior distribution for the unknown parameter, and \\(\\alpha_0,\\,\\beta_0&gt;0\\) are hyperparameters that I will tune in order to encode the prior knowledge about \\(\\lambda\\) that my grandmother described. I chose the gamma family simply because it is convenient and familiar to me, and I know that \\(\\lambda\\) is a continuous numerical quantity that must be positive.\n\nShow that the posterior distribution for \\(\\lambda\\) in this model is\n\\[\n\\lambda\\,|\\,X_{1:n}=x_{1:n} \\sim \\text{Gamma}(\\alpha_n,\\,\\beta_n).\n\\]\nAfter we see some data, what are the revised hyperparameters \\(\\alpha_n,\\,\\beta_n\\) equal to?\n\n\n\n\n\n\n\nPay attention to the notation here\n\n\n\nBefore I see any data, \\(\\text{Gamma}(\\alpha_0,\\,\\beta_0)\\) summarizes my beliefs about \\(\\lambda\\). After I see some data, \\(\\text{Gamma}(\\alpha_n,\\,\\beta_n)\\) summarizes my beliefs about \\(\\lambda\\). \\(\\alpha_0\\) and \\(\\beta_0\\) are the prior hyperparameters, and \\(\\alpha_n\\) and \\(\\beta_n\\) are the posterior hyperparameters. The subscript indicates how much data my beliefs are based on.\n\n\n\nShow that the posterior mean has the form\n\\[\nE(\\lambda\\,|\\,X_1,\\,X_2,\\,...,\\,X_n)=w_n\\hat{\\lambda}_n^{(MLE)} + (1-w_n)\\underbrace{E(\\lambda)}_{\\text{prior mean}},\n\\]\nwhere \\(w_n\\in(0,\\,1)\\) might depend on the data. This means that the posterior mean is a shrinkage estimator. We shrink the MLE toward our prior estimate of the parameter.\nHow should the prior hyperparameters \\(\\alpha_0\\) and \\(\\beta_0\\) be set so that the prior distribution captures my grandmother’s beliefs about \\(\\lambda\\)? There’s not a tremendous amount of math here. It’s just trial-and-error until you find numbers that work."
  },
  {
    "objectID": "problems/bank/bayes/inv-gamma-normal.html",
    "href": "problems/bank/bayes/inv-gamma-normal.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Consider this Bayesian model:\n\\[\n\\begin{aligned}\n\\theta&\\sim\\text{IG}(a_0,\\,b_0)\\\\\nX_{1:n}\\mid\\theta&\\overset{\\text{iid}}{\\sim}\\text{N}(0,\\,\\theta).\n\\end{aligned}\n\\]\n\nWhat is the posterior distribution for \\(\\theta\\) conditional on the data?\nDerive the posterior mean and show that it is a convex combination of the prior mean and the MLE;\nThe posterior mean can be thought of as an alternative estimator, in a classical sense. So, what are its mean, variance, bias, and MSE?\nIs it possible for this new estimator to be superior (lower MSE) than the pure MLE?"
  },
  {
    "objectID": "problems/bank/conceptual/exam-review.html",
    "href": "problems/bank/conceptual/exam-review.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Sadly, we have a midterm exam coming up on Thursday October 9. Everything you have seen on Problem Sets 1 through 4 is fair game for the exam: set theory, probability spaces and axioms, probability rules and proofs, counting, conditional probability and independence, and discrete random variables.\nLook back through the course materials thus far and select the topic that you think you understand the least. Study this topic carefully and then do two things:\n\nWrite a few paragraphs where you explain this topic to a five-year-old using only words, pictures, and toy examples;\nAfter you’re done reviewing, and after your response to part a, include one thing you’re still unsure about or a question you’d like to ask about your chosen topic.\n\nAfter you submit this problem set, I will answer all of the class’s questions and post an anonymized “Q&A” as part of the exam review materials."
  },
  {
    "objectID": "problems/bank/conceptual/sally-clark.html",
    "href": "problems/bank/conceptual/sally-clark.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Here are some short readings about the case of Sally Clark and Roy Meadow, as well as something called the prosecutor’s fallacy:\n\nGrimes (2023): “Bad Science and Bad Statistics in the Courtroom Convict Innocent People,” Scientific American;\nCarriquiry (2018): “Misuse of statistics in the courtroom: the Sally Clark case,” CSAFE;\nWestreich & Iliinsky (2014): “Epidemiology visualized: the prosecutor’s fallacy,” American Journal of Epidemiology;\nAP Dawid’s statement during the Clark appeal.\n\nPlease do the following:\n\nRead the articles;\nUsing the ideas of conditional probability and independence that we have studied, write a paragraph or two describing what the prosecutor’s fallacy is and how it applies to the Clark case;\nIf you were an attorney for the defense and had the chance to cross-examine Roy Meadow, what would you ask him about his statistical testimony? What are the most incisive questions you could ask? How would you ensure that the cross-examination would “land” with the members of the jury, who most likely know absolutely nothing about probability and statistics?\n\n\n\n\n\n\n\nPlease don’t break my heart\n\n\n\nI’m not asking you to read Moby-Dick here."
  },
  {
    "objectID": "problems/bank/random-variables-discrete/geometric-mgf.html",
    "href": "problems/bank/random-variables-discrete/geometric-mgf.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Compute the moment generating function of the geometric distribution. For what values of \\(t\\) is it defined?\nUse the mgf to compute the mean and variance of the geometric distribution."
  },
  {
    "objectID": "problems/bank/random-variables-discrete/ross-rv.html",
    "href": "problems/bank/random-variables-discrete/ross-rv.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Two balls are chosen randomly from an urn containing 8 green, 4 black, and 2 orange balls. Suppose that we win $2 for each black ball selected, we lose $1 for each green ball selected, and we earn nothing for each orange ball selected. Let \\(X\\) denote our winnings.\n\nMake a table with the possible values of \\(X\\) and the probabilities of each value.\nSketch the pmf of \\(X\\).\nSketch the cdf of \\(X\\).\nCompute the expected value of \\(X\\)."
  },
  {
    "objectID": "problems/bank/random-variables-discrete/waiting-for-five-or-seven.html",
    "href": "problems/bank/random-variables-discrete/waiting-for-five-or-seven.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "A pair of fair dice is rolled until a sum of either a 5 or a 7 appears. What is the probability that a 5 occurs first?"
  },
  {
    "objectID": "problems/bank/random-variables-discrete/bdj-rv.html",
    "href": "problems/bank/random-variables-discrete/bdj-rv.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "An urn contains three white balls and two red balls. The balls are drawn from the urn one at a time, at random and without replacement, until we have drawn all of the balls from a color group. As soon as every ball of either color is drawn, we stop. Let \\(X\\) be the number of balls drawn in this process.\n\nWhat is the range of the random variable \\(X\\)?\nWhat is the probability mass function of \\(X\\)?\nSketch the CDF of \\(X\\).\nCompute \\(E(X)\\).\nThree balls were drawn, and then we stopped because all of one of the colors was drawn. Given this, what is the probability that the last ball we drew was red?"
  },
  {
    "objectID": "problems/bank/random-variables-discrete/exponential-ceiling.html",
    "href": "problems/bank/random-variables-discrete/exponential-ceiling.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Let \\(X\\sim\\text{Exponential}(\\lambda)\\) for some arbitrary \\(\\lambda&gt;0\\). Define a new random variable \\(Y=\\lceil X\\rceil\\). Recall that the ceiling function \\(\\lceil\\cdot\\rceil\\) is the function that rounds a number up to the next integer. So \\(\\lceil0.5\\rceil=1\\), \\(\\lceil13.1\\rceil=14\\), and so on.\n\nWhat is the range of \\(Y\\)?\nWhat is the PMF of \\(Y\\)?\nDoes \\(Y\\) belong to a familiar distribution family?"
  },
  {
    "objectID": "problems/bank/random-variables-discrete/random-exam.html",
    "href": "problems/bank/random-variables-discrete/random-exam.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "I dreamed last night I got on the boat to heavenAnd by some chance I had brought my dice along\nGuys and Dolls\n\nAlas. You settle in for what you think will be a good night’s beauty sleep, and you end up having one of those awful dreams where you must take a test you haven’t studied for. It’s a multiple choice test, each question has five options with only one correct answer, there are fifty questions in total, and each is worth one point. The god Morpheus has kindly seen fit to leave a fair, five-sided die in your dream pocket, and so your dream self decides to do the only thing you can do: use the die to answer each of the exam questions completely at random.\n\nLet \\(X\\) be the number of questions you answer correctly. What is the distribution of \\(X\\)?\nPlot the probability mass function of \\(X\\);\nWhat is the expected number of questions you will answer correctly?\nWhat is the probability that you get at least a C- on the exam?"
  },
  {
    "objectID": "problems/bank/random-variables-discrete/indicator-party.html",
    "href": "problems/bank/random-variables-discrete/indicator-party.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Let \\(A_1\\), \\(A_2\\), \\(A_3\\), \\(A_4\\), \\(A_5\\), and \\(A_6\\) be events in some sample space, each with the same probability \\(0&lt;p&lt;1/6\\). Define the indicator random variables\n\\[\nI_k\n=\n\\begin{cases}\n0 & \\text{if }A_k^c\\text{ happens}\\\\\n1 & \\text{if }A_k\\text{ happens},\n\\end{cases}\n\\]\nand let \\(X\\) be their sum\n\\[\nX = I_1 + I_2 + I_3 + I_4+I_5+I_6.\n\\]\n\nAssume the events \\(A_i\\) are mutually disjoint and compute…\n\n\\(E(X)\\).\n\\(P(X=1)\\).\n\\(P(X=3)\\).\n\nAssume the \\(A_i\\) are independent and compute…\n\n\\(E(X)\\).\n\\(P(X=1)\\).\n\\(P(X=3)\\)."
  },
  {
    "objectID": "problems/bank/random-variables-discrete/binomial-birthdays.html",
    "href": "problems/bank/random-variables-discrete/binomial-birthdays.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Forty-nine members of the Cleveland Orchestra string section wish to compare their birthdays. Assume that no one has February 29th and that all birthdays are independent.\n\nWhat is the expected number of pairs of people with the same birthday?\nWhat is the expected number of days on which at least two players were born?"
  },
  {
    "objectID": "problems/bank/set-theory/basic-set-practice.html",
    "href": "problems/bank/set-theory/basic-set-practice.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Take \\(\\mathbb{R}\\) to be your reference set, and consider these subsets:\n\\[\n\\begin{align*}\nA&=[1,\\,5]\\\\\nB&=\\{x\\in\\mathbb{R}\\,:\\,|x|&gt;2\\}\\\\\nC&=(-\\infty,\\,0]\\\\\nI_n&=\\left[0,\\,\\frac{1}{n}\\right]\n\\end{align*}\n\\]\nExpress each of the following in as simplified and concise a form as possible:\n\n\\(A^c\\)\n\\(A \\cup B\\)\n\\(A\\cap C\\)\n\\(A\\cap C^c\\)\n\\(B \\cap C^c\\)\n\\(A^c \\cap B^c \\cap C^c\\)\n\\((A \\cup B) \\cap C\\)\n\\(\\bigcap_{n=1}^\\infty I_n\\)\n\\(\\bigcup_{n=1}^\\infty I_n\\)"
  },
  {
    "objectID": "problems/bank/conditional-probability/testing-symptoms.html",
    "href": "problems/bank/conditional-probability/testing-symptoms.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "We studied disease testing in class, and in our stylized example, a patient had only two attributes: their true disease status \\(D\\) and their test result \\(T\\). In reality of course, patients have many more relevant attributes: genetics, prior medical history, lifestyle, charm and good looks, etc. So in an attempt to be slightly more realistic, we shall extend our little model to include a third attribute: whether or not you have symptoms \\(S\\).\nTable 1 enumerates all of the possible states of the world together with their individual probabilities. These numbers come from a small COVID-19 study of undergraduates in Fall 2020. The test in question is an antigen test. Patients’ “true” disease status was determined by a PCR test that we assume is hella accurate.\n\n\n\nTable 1: Some COVID-19 numbers. Source: CDC\n\n\n\n\n\n\n\n\n\n\\(D\\)\n\\(S\\)\n\\(T\\)\n\\(P(D\\cap S\\cap T)\\)\n\n\n\n-\n-\n-\n0.7650\n\n\n-\n-\n+\n0.0128\n\n\n-\n+\n-\n0.1685\n\n\n-\n+\n+\n0.0018\n\n\n+\n-\n-\n0.0091\n\n\n+\n-\n+\n0.0064\n\n\n+\n+\n-\n0.0073\n\n\n+\n+\n+\n0.0291\n\n\n\n\n\n\n\n\nWhat is the overall disease prevalence?\nWhat is the sensitivity of the test?\nWhat is the specificity of the test?\nImagine you develop symptoms. So you visit Dr. Vinnie Boombatz to get tested, and the test comes back positive. Given everything we now know about you, what is the probability that you are truly infected?\nWhat is the overall probability that the test is wrong?"
  },
  {
    "objectID": "problems/bank/conditional-probability/dgs-exercise-2-3-1.html",
    "href": "problems/bank/conditional-probability/dgs-exercise-2-3-1.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Suppose that \\(k\\) events \\(B_1\\), \\(B_2\\), …, \\(B_k\\) form a partition of the sample space \\(S\\), and \\(A\\subseteq S\\) is some event with positive probability \\(P(A)&gt;0\\). Show that if \\(P(B_1\\mid A)&lt;P(B_1)\\), then that guarantees \\(P(B_i\\mid A)&gt;P(B_i)\\) for at least one \\(i\\in\\{1,\\,2,\\,...,\\,k\\}\\)."
  },
  {
    "objectID": "problems/bank/conditional-probability/bdj-white-balls.html",
    "href": "problems/bank/conditional-probability/bdj-white-balls.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "An urn contains 5 white and 10 black balls. Four tickets labeled 1, 2, 3, and 4 reside in a box. A ticket is drawn at random from the box, and that number of balls is randomly chosen from the urn, without replacement.\n\nWhat is the probability that all of the balls drawn are white?\nIf we learn that all balls drawn from the urn are white, what is the conditional probability that the #3 ticket was drawn?"
  },
  {
    "objectID": "problems/bank/conditional-probability/bdj-encryption.html",
    "href": "problems/bank/conditional-probability/bdj-encryption.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "An encryption algorithm generates a seed value \\(z\\) in the following way:\n\nA binary operator is chosen at random from \\(\\{+,\\,\\cdot\\}\\);\nTwo distinct numbers \\(x\\) and \\(y\\) are chosen at random from the set \\(\\{1,\\,2,\\,...,\\,9\\}\\) and the binary operation chosen above acts on \\(x\\) and \\(y\\) to produce \\(z\\).\n\n\nWhat is the probability that \\(z\\) is an even number?\nYou notice that the algorithm generates an even \\(z\\) value. Given this, what is the probability that the \\(+\\) operator was used to generate it?"
  },
  {
    "objectID": "problems/bank/conditional-probability/independent-complements.html",
    "href": "problems/bank/conditional-probability/independent-complements.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "If we know that \\(A\\) and \\(B\\) are independent events, show that their complements \\(A^c\\) and \\(B^c\\) are also independent events."
  },
  {
    "objectID": "problems/bank/conditional-probability/face-cards.html",
    "href": "problems/bank/conditional-probability/face-cards.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "In a standard deck of \\(52\\) cards, 3 (Jack, Queen, and King) of the 13 ranks are collectively known as the “face cards” of the deck. Recall that a standard deck of \\(52\\) cards also has \\(4\\) suits (Hearts, Diamonds, Clubs, and Spades).\nSuppose you deal out two cards from the deck. What is the probability that they are both face cards given that they are both hearts?"
  },
  {
    "objectID": "problems/bank/conditional-probability/recruiter.html",
    "href": "problems/bank/conditional-probability/recruiter.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "A JP Morgan recruiter is walking around campus with three bone-colored business cards. Two of the cards are normal; they have writing on one side and they are blank on the other. The third card was misprinted; the writing was duplicated on both sides. The recruiter drops one of the cards and orders his valet to pick it up. When the valet reaches for the card, he sees writing facing up. What is the probability that the recruiter dropped the misprinted card?"
  },
  {
    "objectID": "problems/bank/random-variables-continuous/gauntlet.html",
    "href": "problems/bank/random-variables-continuous/gauntlet.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Consider a nonnegative, absolutely continuous random variable with cdf\n\\[\nF_X(x)\n=\n\\begin{cases}\n1 - \\exp\\left(-\\frac{x^2}{2}\\right) & x\\geq 0\n\\\\\n0 & x&lt;0.\n\\end{cases}\n\\]\n\nWhat is \\(P(2\\leq X\\leq 3)\\);\nWhat is the density of \\(X\\)?\nWhat is \\(E(X^n)\\) for any \\(n\\in\\mathbb{N}\\)?\nWhat is \\(\\text{var}(X)\\)?\nWhat is the median of \\(X\\)?"
  },
  {
    "objectID": "problems/bank/random-variables-continuous/wald.html",
    "href": "problems/bank/random-variables-continuous/wald.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "inverse gaussian"
  },
  {
    "objectID": "problems/bank/random-variables-continuous/logistic.html",
    "href": "problems/bank/random-variables-continuous/logistic.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Imagine \\(X\\) has this cdf:\n\\[\nF(x)=\\frac{1}{1+\\exp\\left(-\\frac{x-\\mu}{s}\\right)},\\quad x\\in\\mathbb{R}.\n\\]\n\\(\\mu\\in\\mathbb{R}\\) and \\(s&gt;0\\) are just constants.\n\nWhat is the pdf of \\(X\\)?\nWhat is the mean of \\(X\\)?"
  },
  {
    "objectID": "problems/bank/random-variables-continuous/gamma-half.html",
    "href": "problems/bank/random-variables-continuous/gamma-half.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Recall the gamma function\n\\[\n\\Gamma(x)=\\int_0^\\infty y^{x-1}e^{-y}\\,\\text{d}y.\n\\]\nShow that \\(\\Gamma\\left(\\frac{1}{2}\\right)=\\sqrt{\\pi}\\)."
  },
  {
    "objectID": "problems/bank/random-variables-continuous/random-point-on-line.html",
    "href": "problems/bank/random-variables-continuous/random-point-on-line.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "A point is chosen at random on a line segment of length \\(L\\). Interpret this statement, and find the probability that the ratio of the shorter to the longer segment is less than \\(\\frac{1}{4}\\).\n\n\n\n\n\n\nHint\n\n\n\n\n\nIt might help to read about the continuous uniform distribution."
  },
  {
    "objectID": "problems/bank/random-variables-continuous/minimize-expected-error.html",
    "href": "problems/bank/random-variables-continuous/minimize-expected-error.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Let \\(X\\) be any absolutely continuous random variable with pdf \\(f\\) and cdf \\(F\\), and assume that \\(E[(X-a)^2]\\) and \\(E\\left[|X-a|\\right]\\) are finite for all \\(a\\in \\mathbb{R}\\).\n\nCompute and interpret\n\n\\[\na_0=\\underset{a\\in\\mathbb{R}}{\\arg\\min}\\,E[(X-a)^2].\n\\]\n\nCompute and interpret\n\n\\[\nb_0=\\underset{b\\in\\mathbb{R}}{\\arg\\min}\\,E\\left[|X-b|\\right].\n\\]"
  },
  {
    "objectID": "problems/bank/random-variables-continuous/basic-pdf-practice.html",
    "href": "problems/bank/random-variables-continuous/basic-pdf-practice.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "An absolutely continuous random variable \\(X\\) has pdf\n\\[\nf(x)=\\begin{cases}\n\\frac{3}{22}[5 - (x-1)^2] & 1\\leq x \\leq3\\\\\n0 & \\text{else}.\n\\end{cases}\n\\]\n\nWhat is the range of \\(X\\)?\nConfirm that \\(f\\) is a valid pdf.\nDerive the formula for the cdf of \\(X\\) and plot it.\nCompute \\(P(0.9 &lt; X &lt; 1.1)\\).\nCompute \\(E(X)\\).\nCompute \\(\\text{var}(X)\\)."
  },
  {
    "objectID": "problems/bank/random-variables-continuous/all-moments-finite-no-mgf.html",
    "href": "problems/bank/random-variables-continuous/all-moments-finite-no-mgf.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Here is the cdf of an absolutely continuous random variable \\(X\\):\n\\[\nF(x)\n=\n\\begin{cases}\n1-\\exp\\left(-\\sqrt{x}\\right)&x\\geq 0\\\\\n0&\\text{else}.\n\\end{cases}\n\\]\n\nWhat is \\(P(4\\leq X &lt; 9)\\)?\nWhat is the pdf of \\(X\\)?\nCompute \\(E(X^n)\\) for any \\(n\\in\\mathbb{N}\\).\nWhat is the variance of \\(X\\)?\nWhat is the median of \\(X\\)?\n\n\n\n\n\n\n\nHint\n\n\n\nAll of the moments of this distribution are finite, but nevertheless, it doesn’t have a moment-generating function (MGF). So you’ll hit a dead-end if you try using that to compute the moments. Instead, use LOTUS. It’s a subtle calculation, but it actually works out pretty nice."
  },
  {
    "objectID": "problems/bank/random-variables-continuous/quantum.html",
    "href": "problems/bank/random-variables-continuous/quantum.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Quantum mechanics has a reputation for being an intimidating branch of theoretical physics, and JZ certainly doesn’t understand a ding dong thing about it. Even so, because we have studied some basic probability theory, the mathematics of quantum mechanics is more accessible than you might guess. Behold:\nImagine a single particle at the (sub)atomic level1. A particle’s quantum state encodes information about all of its measurable properties, such as position, momentum, spin, and energy. In quantum mechanics, measuring these properties is fundamentally probabilistic. Before measurement, the particle doesn’t have a single, definite position or momentum; it has only probabilities for the possible outcomes. The quantum state of a particle is fully characterized by its wave function, and the squared magnitude of the wave function is a probability density that describes the distribution of the particle’s position, or momentum, or whatever else you wish to study. Let’s explore this in a very simple example.\nConsider a single, one-dimensional particle located somewhere in the interval \\([0,\\,L]\\). The wave function associated with the particle’s quantum state is\n\\[\n\\psi_n(x)\n=\n\\begin{cases}\nA\\cdot\\sin\\left(\\frac{n\\pi}{L}x\\right) & 0\\leq x\\leq L\\\\\n0 & \\text{else},\n\\end{cases}\n\\]\nwhere \\(A\\geq 0\\) is the amplitude, and \\(n=1,\\,2,\\,3,\\,...\\) indexes the particle’s energy level. The higher the energy level, the more “excited” the particle is. Behold:\nBecause the particle does not have a definite position before measurement, we treat the position \\(X_n\\in[0,\\,L]\\) of the particle as a continuous random variable whose probability density function is \\(f_n(x)=|\\psi_n(x)|^2\\)."
  },
  {
    "objectID": "problems/bank/random-variables-continuous/quantum.html#footnotes",
    "href": "problems/bank/random-variables-continuous/quantum.html#footnotes",
    "title": "STA 240 Fall 2025",
    "section": "Footnotes",
    "text": "Footnotes\n\nIf you’re like me, you cannot do this. Oh well.↩︎"
  },
  {
    "objectID": "problems/bank/mle/mle-weibull.html",
    "href": "problems/bank/mle/mle-weibull.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Consider these data:\n\\[\nX_1,\\,X_2,\\,...,\\,X_n\\overset{\\text{iid}}{\\sim}\\text{GF}(k,\\,\\theta).\n\\]\nSo again, we are recycling the distribution family from Problem 13 above. Throughout, just treat \\(k&gt;0\\) as fixed and known.\n\nWhat is the maximum likelihood estimator of \\(\\theta\\in\\mathbb{R}\\)?\nWhat is the sampling distribution of the estimator?\nWhat is the MSE of the estimator?\nBased on the MSE, what are the statistical properties of this estimator?"
  },
  {
    "objectID": "problems/bank/mle/mle-lomax.html",
    "href": "problems/bank/mle/mle-lomax.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Let \\(X_1\\), \\(X_2\\), …, \\(X_n\\) be iid from some member of the parametric family that you met on Midterm 2:\n\\[\nf(x\\,|\\,\\theta)=\\theta(x+1)^{-(\\theta+1)},\\quad x&gt;0.\n\\]\n\nWhat is the maximum likelihood estimator of \\(\\theta&gt;0\\)?\nWhat is the sampling distribution of the estimator? Problem 14 might be helpful here.\nWhat is the MSE of the estimator?"
  },
  {
    "objectID": "problems/bank/mle/mle-normal-variance.html",
    "href": "problems/bank/mle/mle-normal-variance.html",
    "title": "STA 240 Fall 2025",
    "section": "",
    "text": "Consider these data:\n\\[\nX_1,\\,X_2,\\,...,\\,X_n\\overset{\\text{iid}}{\\sim}\\text{N}(0,\\,\\theta).\n\\]\n\nWhat is the maximum likelihood estimator of \\(\\theta&gt;0\\)?\nWhat is the sampling distribution of the estimator?\nWhat is the MSE of the estimator?\nBased on the MSE, what are the statistical properties of this estimator?"
  },
  {
    "objectID": "problems/pset-4.html",
    "href": "problems/pset-4.html",
    "title": "Problem Set 4",
    "section": "",
    "text": "Recommend some music for us to listen to while we grade this.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 4"
    ]
  },
  {
    "objectID": "problems/pset-4.html#problem-0",
    "href": "problems/pset-4.html#problem-0",
    "title": "Problem Set 4",
    "section": "",
    "text": "Recommend some music for us to listen to while we grade this.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 4"
    ]
  },
  {
    "objectID": "problems/pset-4.html#problem-1",
    "href": "problems/pset-4.html#problem-1",
    "title": "Problem Set 4",
    "section": "Problem 1",
    "text": "Problem 1\nSadly, we have a midterm exam coming up on Thursday October 9. Everything you have seen on Problem Sets 1 through 4 is fair game for the exam: set theory, probability spaces and axioms, probability rules and proofs, counting, conditional probability and independence, and discrete random variables.\nLook back through the course materials thus far and select the topic that you think you understand the least. Study this topic carefully and then do two things:\n\nWrite a few paragraphs where you explain this topic to a five-year-old using only words, pictures, and toy examples;\nAfter you’re done reviewing, and after your response to part a, include one thing you’re still unsure about or a question you’d like to ask about your chosen topic.\n\nAfter you submit this problem set, I will answer all of the class’s questions and post an anonymized “Q&A” as part of the exam review materials.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 4"
    ]
  },
  {
    "objectID": "problems/pset-4.html#problem-2",
    "href": "problems/pset-4.html#problem-2",
    "title": "Problem Set 4",
    "section": "Problem 2",
    "text": "Problem 2\nAssume that \\(P(A)&gt;0\\) and prove that\n\\[\nP(A\\cap B\\,|\\, A)\\geq P(A\\cap B\\,|\\, A\\cup B).\n\\]",
    "crumbs": [
      "Problem Sets",
      "Problem Set 4"
    ]
  },
  {
    "objectID": "problems/pset-4.html#problem-3",
    "href": "problems/pset-4.html#problem-3",
    "title": "Problem Set 4",
    "section": "Problem 3",
    "text": "Problem 3\nLet \\(A\\), \\(B\\), and \\(C\\) be events with \\(P(C) &gt; 0\\). \\(A\\) and \\(B\\) are conditionally independent given \\(C\\) if and only if\n\\[\nP(A \\cap B \\mid C) = P(A \\mid C)P(B \\mid C).\n\\]\nShow that the above implies \\(P(A \\mid B\\cap C) = P(A \\mid C)\\).",
    "crumbs": [
      "Problem Sets",
      "Problem Set 4"
    ]
  },
  {
    "objectID": "problems/pset-4.html#problem-4",
    "href": "problems/pset-4.html#problem-4",
    "title": "Problem Set 4",
    "section": "Problem 4",
    "text": "Problem 4\nAn urn contains three white balls and two red balls. The balls are drawn from the urn one at a time, at random and without replacement, until we have drawn all of the balls from a color group. As soon as every ball of either color is drawn, we stop. Let \\(X\\) be the number of balls drawn in this process.\n\nWhat is the range of the random variable \\(X\\)?\nWhat is the probability mass function of \\(X\\)?\nSketch the CDF of \\(X\\).\nCompute \\(E(X)\\).\nThree balls were drawn, and then we stopped because all of one of the colors was drawn. Given this, what is the probability that the last ball we drew was red?",
    "crumbs": [
      "Problem Sets",
      "Problem Set 4"
    ]
  },
  {
    "objectID": "problems/pset-4.html#problem-5",
    "href": "problems/pset-4.html#problem-5",
    "title": "Problem Set 4",
    "section": "Problem 5",
    "text": "Problem 5\nKing Elessar summons the wizard Gandalf to court, seeking his counsel. A foul plague has been incapacitating the soldiers. King Elessar asks Gandalf if there exists a test for the disease that is 100% sensitive and 100% specific (ie perfect). Gandalf says that a perfect blood test does exist, but it is very expensive, requiring the venom of a special lizard found only in the Morgul Vale. Each blood test would cost one gold piece, but the Royal Purse only has 500 gold pieces to spare, and there are 1000 soldiers that need to be tested. Gandalf sleeps on it and returns the next day with a plan for testing all of them that won’t break the bank:\n\nTake a blood sample from each of the 1000 soldiers;\nSplit each of these samples into two parts;\nTake one part of each sample and mix batches of ten of the samples together to create 100 mixed samples.\nTest each of those mixed samples;\nA few things could happen:\n\nIf the test gives a negative result, we know all of the original ten samples are clear, and we can move on;\nIf the test gives a positive result, then we know at least one of the original ten samples is positive. In that case we test the second part of the sample for each of the ten soldiers. We can then identify which soldiers are positive.\n\n\n\nGandalf finishes by saying “I expect sixty of the mixed batches to be negative. For the other forty batches, I expect the results to be positive, and hence we will have to test all four hundred of the original samples. This gives a total expected cost of 500 gold pieces.”\n\nWhat disease prevalence is Gandalf implicitly assuming here?\nCan Gandalf revise his scheme to reduce the expected cost even more?",
    "crumbs": [
      "Problem Sets",
      "Problem Set 4"
    ]
  },
  {
    "objectID": "problems/pset-4.html#problem-6",
    "href": "problems/pset-4.html#problem-6",
    "title": "Problem Set 4",
    "section": "Problem 6",
    "text": "Problem 6\nNot all election challenges are frivolous and cynical. Sometimes there are illegal/ineligible ballots in a close election, and if a candidate raises a legal challenge, a court must make a decision: certify the original results? adjust the vote totals somehow, possibly reversing the election? void the results entirely and order a new election? The concern of course is that the presence of the bad ballots might have swung the outcome, but how likely is this? Let’s model it.\nImagine an election where Angela Lansbury receives \\(a\\) votes, Bernadette Peters receives \\(b\\) votes, and Angela provisionally wins with a margin of victory \\(a-b&gt;0\\). However, of the total \\(a+b\\) votes, we learn that \\(k&gt;a-b\\) were invalid or illegal. But we know nothing else. We do not know who these folks are, where they came from, or who they voted for. We just know that they exist. If the \\(k\\) invalid ballots were removed from the total, what is the probability that it would flip the result of the election? If this probability is “too big,” we might start to worry.\nTo model this, think of the original \\(a+b\\) ballots as balls in the proverbial urn. \\(a\\) of these balls are red, and \\(b\\) of these balls are blue, and assume that \\(a,\\, b&gt;k\\). Removing the \\(k\\) invalid ballots is like drawing \\(k\\) balls out of the urn without replacement. Since we know nothing about the illegal ballots apart from the fact that they exist, assume that each of the \\(a+b\\) original ballots is equally likely to have been invalid. Next, imagine we randomly remove \\(k\\) balls from the urn, and let \\(X\\) be a random variable that counts the number of the removed/invalid ballots that belong to Angela. In other words, the number of red balls removed. If \\(X\\) is large enough, it will swing the election in Bernadette’s favor.\n\nWhat are the range and the pmf of the random variable \\(X\\)?\nFor what values of \\(X\\) does removing \\(k\\) ballots reverse the outcome of the election?\nBased on your answers to the previous parts, what is the probability that the election result is reversed when the \\(k\\) bad ballots are removed? Give a general formula for this probability, and then compute the probability for each of the real elections listed in Table 1;\nThe random variable \\(X\\) that you derived in the first part can be approximated by \\(\\text{Binom}(k,\\,1/2)\\). Explain why this approximation might be “good enough,” and verify that it’s not too bad by computing the approximate probability of reversal for the elections in Table 1. Compare the approximate probabilities to the actual ones from the previous part;\nLet’s say that we consider the probability of reversal “too big” if it is greater than 5%. Create a plot in R like Figure 1 with the following features:\n\nthe margin of victory \\(a-b\\) on the horizontal axis;\nthe number of bad ballots \\(k\\) on the vertical axis;\nscatterplot where each point represents an election in Table 1, with the points colored according to the decision that was made in the case;\ndivide the plane into two regions, one where the approximate probability of reversal is less than 0.05 and one where it is greater.\n\n\nWrite a few pithy paragraphs commenting on the following:\n\nthe assumptions of our lil’ model. Do you think they are innocuous? What might you change, and how?\nthe match or lack thereof between our model and the decisions that were actually made in the real cases;\nHow do you think a court should handle a legitimate election challenge? What are their responsibilities to the electorate, and what should they prioritize? Should they err on the side of upholding or voiding results? Should they think probabilistically? Should they consult this model, or one like it?\n\n\n\n\n\nTable 1: Close elections\n\n\n\n\n\n\n\n\n\n\n\n\n\nelection\nyear\nwin\nloss\ntotal\nmargin\ninvalid\ndecision\n\n\n\nIppolito v. Power\n1968\n1,422\n1,405\n2,827\n17\n101\nvoid\n\n\nSantucci v. Power\n1969\n58,076\n57,981\n116,057\n95\n448\nvoid\n\n\nDeMartini v. Power\n1970\n2,656\n2,594\n5,250\n62\n136\nupheld\n\n\nMaine House\n1976\n1,193\n1,060\n2,253\n133\n208\nvoid\n\n\nAnn Arbor mayor\n1977\n10,660\n10,659\n21,319\n1\n20\nvoid\n\n\nBrunswick ME town council\n1980\n2,390\n2,383\n4,773\n7\n16\nupheld\n\n\nWashington governor\n2004\n1,373,357\n1,373,228\n2,746,585\n129\n1,439\nupheld\n\n\n\n\n\n\n\nCodeelections &lt;- data.frame(\n  election = c(\"Ippolito\", \"Santucci\", \"DeMartini\", \"Maine\", \"Ann Arbor\", \"Brunswick\", \"Washington\"),\n  a = c(1422, 58076, 2656, 1193, 10660, 2390, 1373357),\n  b = c(1405, 57981, 2594, 1060, 10659, 2383, 1373228),\n  k = c(101, 448, 136, 208, 20, 16, 1439),\n  decision = c(\"void\", \"void\", \"upheld\", \"void\", \"void\", \"upheld\", \"upheld\"),\n  col = c(\"red\", \"red\", \"blue\", \"red\", \"red\", \"blue\", \"blue\")\n)\n\nreversal_prob_binom &lt;- function(a, b, k){\n  cutoff = ceiling(0.5 * (k + a - b))\n  1 - pbinom(cutoff - 1, k, 1/2)\n}\n\nmargin_grid &lt;- 0:150\nk_grid &lt;- 0:450\nn_points &lt;- length(margin_grid) * length(k_grid)\n\ngrid &lt;- data.frame(\n  margin = numeric(n_points),\n  k = numeric(n_points),\n  color = rep(\"lightblue\", n_points)\n)\n\nrow &lt;- 0\n\nfor(x in margin_grid){\n  for(y in k_grid){\n    row &lt;- row + 1\n    grid[row, 1] &lt;- x\n    grid[row, 2] &lt;- y\n    if(reversal_prob_binom(1 + x, 1, y) &gt;= 0.05){\n      grid[row, 3] &lt;- \"pink\"\n    }\n  }\n}\n\nplot(grid$margin, grid$k, col = grid$color, \n     pch = 19, cex = 0.2,\n     xlab = \"margin of victory (a - b)\", \n     ylab = \"number of invalid ballots (k)\",\n     main = \"Approximate probability of election reversal\")\npoints(elections$a - elections$b, elections$k, \n       pch = 19, col = elections$col)\nlegend(-7, 450, \"&gt; 0.05\", text.col = \"salmon\", bty = \"n\", cex = 1.5)\nlegend(115, 450, \"&lt; 0.05\", text.col = \"blue\", bty = \"n\", cex = 1.5)\nlegend(125, 85, c(\"Voided\", \"Upheld\"), pch = 19, col = c(\"red\", \"blue\"), bty = \"n\")\n\n\n\n\n\n\nFigure 1",
    "crumbs": [
      "Problem Sets",
      "Problem Set 4"
    ]
  },
  {
    "objectID": "problems/pset-4.html#problem-7",
    "href": "problems/pset-4.html#problem-7",
    "title": "Problem Set 4",
    "section": "Problem 7",
    "text": "Problem 7\nForty-nine members of the Cleveland Orchestra string section wish to compare their birthdays. Assume that no one has February 29th and that all birthdays are independent.\n\nWhat is the expected number of pairs of people with the same birthday?\nWhat is the expected number of days on which at least two players were born?",
    "crumbs": [
      "Problem Sets",
      "Problem Set 4"
    ]
  },
  {
    "objectID": "problems/pset-4.html#problem-8",
    "href": "problems/pset-4.html#problem-8",
    "title": "Problem Set 4",
    "section": "Problem 8",
    "text": "Problem 8\nA pair of fair dice is rolled until a sum of either a 5 or a 7 appears. What is the probability that a 5 occurs first?",
    "crumbs": [
      "Problem Sets",
      "Problem Set 4"
    ]
  },
  {
    "objectID": "problems/pset-4.html#problem-9",
    "href": "problems/pset-4.html#problem-9",
    "title": "Problem Set 4",
    "section": "Problem 9",
    "text": "Problem 9\nLet \\(X\\) be a random variable with the following pmf:\n\n\n\n\\(x\\)\n\\(P(X=x)\\)\n\n\n\n-1\n0.500\n\n\n0\n0.250\n\n\n1\n0.125\n\n\n2\n0.125\n\n\n\n\n\nWhat is the mean of \\(X\\)?\nWhat is the pmf of \\(Y = 2X-3\\)?\nWhat is the mean of \\(Y\\)?\nWhat is the pmf of \\(W = X^2\\)?\nWhat is the mean of \\(W\\)?\nWhat is the conditional pmf of \\(X\\) given \\(X \\neq 0\\)?\nWhat is the conditional mean of \\(X\\) given \\(X \\neq 0\\)?",
    "crumbs": [
      "Problem Sets",
      "Problem Set 4"
    ]
  },
  {
    "objectID": "problems/pset-4.html#problem-10",
    "href": "problems/pset-4.html#problem-10",
    "title": "Problem Set 4",
    "section": "Problem 10",
    "text": "Problem 10\nIf \\(X\\sim\\text{Binomial}(n,\\,p)\\), show that\n\\[\nE\\left[\\frac{1}{X+1}\\right]=\\frac{1-(1-p)^{n+1}}{(n+1)p}.\n\\]",
    "crumbs": [
      "Problem Sets",
      "Problem Set 4"
    ]
  },
  {
    "objectID": "problems/pset-4.html#submission",
    "href": "problems/pset-4.html#submission",
    "title": "Problem Set 4",
    "section": "Submission",
    "text": "Submission\nYou are free to compose your solutions for this problem set however you wish (scan or photograph written work, handwriting capture on a tablet device, LaTeX, Quarto, whatever) as long as the final product is a single PDF file. You must upload this to Gradescope and mark the pages associated with each problem.\nDo not forget to include the following:\n\nFor each problem, please acknowledge your collaborators;\nIf a problem required you to code something, please include both the code and the output. “Including the code” can be as crude as a screenshot, but you might also use Quarto to get a nice lil’ pdf that you can merge with the rest of your submission.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 4"
    ]
  },
  {
    "objectID": "problems/pset-1.html",
    "href": "problems/pset-1.html",
    "title": "Problem Set 1",
    "section": "",
    "text": "Some of these problems ask you to prove new probability rules using a small set of raw materials: set theory, the axioms, and the basic rules we saw in lecture. The purpose of these problems is to demonstrate that you understand every step in the reasoning, even if it seems trivial and obvious. So, don’t leave anything out! Every step of the argument should be explicit, and you should clearly indicate which axioms/results you are invoking when you move from point A to point B. After you’ve paid your dues, we’ll give you some latitude to hand wave and yada-yada your way through certain basic arguments. But not yet.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 1"
    ]
  },
  {
    "objectID": "problems/pset-1.html#before-you-start",
    "href": "problems/pset-1.html#before-you-start",
    "title": "Problem Set 1",
    "section": "",
    "text": "Some of these problems ask you to prove new probability rules using a small set of raw materials: set theory, the axioms, and the basic rules we saw in lecture. The purpose of these problems is to demonstrate that you understand every step in the reasoning, even if it seems trivial and obvious. So, don’t leave anything out! Every step of the argument should be explicit, and you should clearly indicate which axioms/results you are invoking when you move from point A to point B. After you’ve paid your dues, we’ll give you some latitude to hand wave and yada-yada your way through certain basic arguments. But not yet.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 1"
    ]
  },
  {
    "objectID": "problems/pset-1.html#problem-0",
    "href": "problems/pset-1.html#problem-0",
    "title": "Problem Set 1",
    "section": "Problem 0",
    "text": "Problem 0\nRecommend some music for us to listen to while we grade this.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 1"
    ]
  },
  {
    "objectID": "problems/pset-1.html#problem-1",
    "href": "problems/pset-1.html#problem-1",
    "title": "Problem Set 1",
    "section": "Problem 1",
    "text": "Problem 1\n\n\nPersi Diaconis is a famous researcher in probability and statistics. He has a cute line of research where he probes the randomness that we take for granted in simple things like coin tossing, dice, and playing cards. In the interview above he discusses coin tossing, and says both “coin tossing is as close to a random phenomenon as I know” and “coin tossing is a deterministic process. What’s random about it?” Wut?\nWatch the interview, and report back with a brief summary of Diaconis’ explanation of what makes a coin toss random. Do you agree or disagree?",
    "crumbs": [
      "Problem Sets",
      "Problem Set 1"
    ]
  },
  {
    "objectID": "problems/pset-1.html#problem-2",
    "href": "problems/pset-1.html#problem-2",
    "title": "Problem Set 1",
    "section": "Problem 2",
    "text": "Problem 2\nTake \\(\\mathbb{R}\\) to be your reference set, and consider these subsets:\n\\[\n\\begin{align*}\nA&=[1,\\,5]\\\\\nB&=\\{x\\in\\mathbb{R}\\,:\\,|x|&gt;2\\}\\\\\nC&=(-\\infty,\\,0]\\\\\nI_n&=\\left[0,\\,\\frac{1}{n}\\right]\n\\end{align*}\n\\]\nExpress each of the following in as simplified and concise a form as possible:\n\n\\(A^c\\)\n\\(A \\cup B\\)\n\\(A\\cap C\\)\n\\(A\\cap C^c\\)\n\\(B \\cap C^c\\)\n\\(A^c \\cap B^c \\cap C^c\\)\n\\((A \\cup B) \\cap C\\)\n\\(\\bigcap_{n=1}^\\infty I_n\\)\n\\(\\bigcup_{n=1}^\\infty I_n\\)",
    "crumbs": [
      "Problem Sets",
      "Problem Set 1"
    ]
  },
  {
    "objectID": "problems/pset-1.html#problem-3",
    "href": "problems/pset-1.html#problem-3",
    "title": "Problem Set 1",
    "section": "Problem 3",
    "text": "Problem 3\n“Prove” each set identity with a picture:\n\nDistributive property: \\(A\\cup(B\\cap C)=(A\\cup B)\\cap(A\\cup C)\\);\nDe Morgan: \\((A\\cap B)^{c}=A^{c}\\cup B^{c}\\);",
    "crumbs": [
      "Problem Sets",
      "Problem Set 1"
    ]
  },
  {
    "objectID": "problems/pset-1.html#problem-4",
    "href": "problems/pset-1.html#problem-4",
    "title": "Problem Set 1",
    "section": "Problem 4",
    "text": "Problem 4\nA simplified model of the human blood-type system has four types: A, B, AB, and O. Suppose that, for a randomly chosen person, the probability of type O blood is 0.5, of type A blood is 0.36, and of type B blood is 0.11. There are two antigens, anti-A and anti-B, that react with a person’s blood in different ways depending on the type. Anti-A reacts with blood types A and AB but not with B and O. Anti-B reacts with blood types B and AB, but not with A and O.\n\nLet \\(\\mathcal{A}\\) be the event that a person’s blood reacts with anti-A, and let \\(\\mathcal{B}\\) be the event that their blood reacts with anti-B. Classify the person’s blood type using the events \\(\\mathcal{A}\\) and \\(\\mathcal{B}\\) and their complements;\nWhat is the probability that both antigens will react with a random person’s blood?\nWhat is the probability that each antigen will react with a random person’s blood?",
    "crumbs": [
      "Problem Sets",
      "Problem Set 1"
    ]
  },
  {
    "objectID": "problems/pset-1.html#problem-5",
    "href": "problems/pset-1.html#problem-5",
    "title": "Problem Set 1",
    "section": "Problem 5",
    "text": "Problem 5\nOne day last semester I looked down at my phone and saw this screen:\n\n\n\n\n\nInterpret the three probabilities and explain how they fit together.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 1"
    ]
  },
  {
    "objectID": "problems/pset-1.html#problem-6",
    "href": "problems/pset-1.html#problem-6",
    "title": "Problem Set 1",
    "section": "Problem 6",
    "text": "Problem 6\nLet \\(A\\) and \\(B\\) be events in a sample space \\(S\\). Let \\(C\\) be the set of outcomes that are in either \\(A\\) or \\(B\\), but not both.\n\nDraw a well-labeled picture of \\(S\\), \\(A\\), \\(B\\), and \\(C\\).\nWrite down a formula for \\(C\\) in terms of \\(A\\) and \\(B\\) using any of the basic operations: union (\\(\\cup\\)), intersection (\\(\\cap\\)), complement (\\(^c\\)).\nUse set theory and the probability axioms to show that\n\n\\[\nP(C)=P(A)+P(B)-2P(A\\cap B).\n\\]\n\nExplain this result conceptually (with words and pictures).",
    "crumbs": [
      "Problem Sets",
      "Problem Set 1"
    ]
  },
  {
    "objectID": "problems/pset-1.html#problem-7",
    "href": "problems/pset-1.html#problem-7",
    "title": "Problem Set 1",
    "section": "Problem 7",
    "text": "Problem 7\nLet \\(S\\) be a sample space, and consider events \\(A,\\, B,\\, C\\subseteq S\\). Recall that the law of inclusion/exclusion says that\n\\[\nP(A\\cup B)=P(A)+P(B)-P(A\\cap B).\n\\]\n\nHow should this be extended to unions of three events? \\[\nP(A\\cup B\\cup C)=P(A)+P(B)+P(C) +\\,...{???}\n\\] Explain your conjecture with words and pictures.\nUse set theory and the probabiity axioms to prove your conjecture in the previous part.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 1"
    ]
  },
  {
    "objectID": "problems/pset-1.html#problem-8",
    "href": "problems/pset-1.html#problem-8",
    "title": "Problem Set 1",
    "section": "Problem 8",
    "text": "Problem 8\nSuppose we have the sample space \\(S=\\mathbb{N}=\\{0,\\,1,\\,2,\\,...\\}\\) and a probability measure \\(P\\) that assigns the following individual probabilities to the singleton sets:\n\\[\nP(\\{i\\})=c\\frac{4^i}{i!},\\quad i\\in\\mathbb{N}.\n\\]\n\nIn order for \\(P\\) to satisfy the axiom of total measure one, what must be the value of the constant \\(c&gt;0\\).\nWhich outcome(s) in the sample space are most likely (ie have the largest individual probability of occurring)? Furthermore, how do you know for a fact that you’ve identified all of them? The sample space is infinite, so presumably you cannot literally check every outcome.\nWhat is the probability of the even numbers?\n\n\n\n\n\n\n\nBe careful!\n\n\n\nI could not care less about the answers to these questions. It’s all about the reasoning, and showing that you understand how the ideas fit together. To earn full credit, make sure you carefully justify everything by making appropriate reference to the rules and axioms.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 1"
    ]
  },
  {
    "objectID": "problems/pset-1.html#problem-9",
    "href": "problems/pset-1.html#problem-9",
    "title": "Problem Set 1",
    "section": "Problem 9",
    "text": "Problem 9\nA few months ago I went to Harris Teeter to get some junk food, and when I looked at my receipt, I saw this:\n\n\n\n\n\nI thought, “Strange. What are the odds the total would be a whole number?” Well, let’s find out!\n\nImagine you randomly purchase two items that each cost less than $10, and assume all prices are equally likely. The prices of the two items are quoted to two decimal places. What is the probability that the two prices add up to a whole number total?\nNow let’s add a wrinkle. Imagine sales tax is 5%. This is calculated, rounded to the nearest cent, and then added to your bill. What is the probability that the cost of the two items plus the rounded sales tax adds up to a whole number?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nThe answer to part b is roughly 0.01, but showing this is tricky. Give it a shot!",
    "crumbs": [
      "Problem Sets",
      "Problem Set 1"
    ]
  },
  {
    "objectID": "problems/pset-1.html#problem-10",
    "href": "problems/pset-1.html#problem-10",
    "title": "Problem Set 1",
    "section": "Problem 10",
    "text": "Problem 10\nSeven balls are randomly withdrawn from an urn that contains 12 red, 16 blue, and 18 green balls. Find the probability that\n\n3 red, 2 blue, and 2 green balls are withdrawn;\nat least 2 red balls are withdrawn;\nall withdrawn balls are the same color;\neither exactly 3 red balls or exactly 3 blue balls are withdrawn.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 1"
    ]
  },
  {
    "objectID": "problems/pset-1.html#submission",
    "href": "problems/pset-1.html#submission",
    "title": "Problem Set 1",
    "section": "Submission",
    "text": "Submission\nYou are free to compose your solutions for this problem set however you wish (scan or photograph written work, handwriting capture on a tablet device, LaTeX, Quarto, whatever) as long as the final product is a single PDF file. You must upload this to Gradescope and mark the pages associated with each problem.\nDo not forget to include the following:\n\nFor each problem, please acknowledge your collaborators;\nIf a problem required you to code something, please include both the code and the output. “Including the code” can be as crude as a screenshot, but you might also use Quarto to get a nice lil’ pdf that you can merge with the rest of your submission.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 1"
    ]
  },
  {
    "objectID": "problems/exam-final.html",
    "href": "problems/exam-final.html",
    "title": "Final Exam Study Guide",
    "section": "",
    "text": "Our final exam is Friday December 12 from 9:00 AM to 12:00 PM. There will be 11 problems, and they will break down as follows:\nProblems 0 - 8 will be similar to the midterms. Since you’ve had more time to live with this material, I reserve the right to include problems that might stretch you a bit. For Problems 9 and 10, I assure you there will be no surprises. You can expect problems that are exactly like the examples you’ve seen in lecture, lab, Problem Set 7, and this study guide. The distributions may be different, and the details might change, but the format and the steps are the same. If you work every practice problem from scratch on your own and then study the solutions carefully, you are ready.",
    "crumbs": [
      "Exam Study Guides",
      "Final"
    ]
  },
  {
    "objectID": "problems/exam-final.html#problem-1",
    "href": "problems/exam-final.html#problem-1",
    "title": "Final Exam Study Guide",
    "section": "Problem 1",
    "text": "Problem 1\nSuppose the events \\(A\\) and \\(B\\) are disjoint. Under what conditions are \\(A^{c}\\) and \\(B^{c}\\) also disjoint events?",
    "crumbs": [
      "Exam Study Guides",
      "Final"
    ]
  },
  {
    "objectID": "problems/exam-final.html#problem-2",
    "href": "problems/exam-final.html#problem-2",
    "title": "Final Exam Study Guide",
    "section": "Problem 2",
    "text": "Problem 2\nLet \\(A_1,\\,A_2,\\,...,\\,A_n\\subseteq S\\) be a finite collection of possibly overlapping events in some probability space. Show that\n\\[\nP\\left(\\bigcap_{i=1}^n A_i\\right)\\geq \\sum\\limits_{i=1}^nP(A_i)-n+1.\n\\]",
    "crumbs": [
      "Exam Study Guides",
      "Final"
    ]
  },
  {
    "objectID": "problems/exam-final.html#want-more-review",
    "href": "problems/exam-final.html#want-more-review",
    "title": "Final Exam Study Guide",
    "section": "Want more review?",
    "text": "Want more review?\n\nLecture notes on axioms and rules;\nProblem Set 1: problems 4 - 8;\nProblem Set 2: problems 2 - 3;\nMidterm 1 Study Guide: problems 1 - 4;\nMidterm 1: problem 1.",
    "crumbs": [
      "Exam Study Guides",
      "Final"
    ]
  },
  {
    "objectID": "problems/exam-final.html#problem-3",
    "href": "problems/exam-final.html#problem-3",
    "title": "Final Exam Study Guide",
    "section": "Problem 3",
    "text": "Problem 3\nFour people are going to play bridge, which begins with an entire 52-card deck being dealt at random, 13 cards per player. What is the probability that each player is dealt exactly one ace?\n\n\n\n\n\n\nStill confused about ordered versus unordered sampling?\n\n\n\nThere are several ways to do this calculation. You should try it on your own, but then, go read Example 1.8.9 and the subsequent discussion on pages 37 - 39 of DeGroot & Schervish.",
    "crumbs": [
      "Exam Study Guides",
      "Final"
    ]
  },
  {
    "objectID": "problems/exam-final.html#problem-4",
    "href": "problems/exam-final.html#problem-4",
    "title": "Final Exam Study Guide",
    "section": "Problem 4",
    "text": "Problem 4\nAn evil cult has its secret meetings in the basement of a building that has 15 above-ground floors. When the meeting is over, the nine cult members enjoy coffee and donuts and then depart to sow the seeds of villainy. They board the elevator, and each person randomly and independently selects one of the 15 floors to visit. Compute the probability that…\n\nNo two passengers share a floor;\nAt least two passengers share a floor;\nExactly two passengers share a floor;\nNo passengers get off at the top floor;\nAt least one passenger gets off at floor 1;\nAll passengers get off on consecutive floors;\nAll passengers get off below floor 10",
    "crumbs": [
      "Exam Study Guides",
      "Final"
    ]
  },
  {
    "objectID": "problems/exam-final.html#want-more-review-1",
    "href": "problems/exam-final.html#want-more-review-1",
    "title": "Final Exam Study Guide",
    "section": "Want more review?",
    "text": "Want more review?\n\nLecture notes on counting theory;\nLecture notes with birthday problem and other worked examples;\nLab 2 worked examples;\nProblem Set 1: problems 9 - 10;\nProblem Set 2: problems 4 - 8;\nProblem Set 3: problems 2 - 3;\nMidterm 1 Study Guide: problems 5 - 7;\nMidterm 1: problems 2 - 3;\nOdd-numbered exercises in DeGroot & Schervish Chapters 1.6 - 1.8, 1.12.",
    "crumbs": [
      "Exam Study Guides",
      "Final"
    ]
  },
  {
    "objectID": "problems/exam-final.html#problem-5",
    "href": "problems/exam-final.html#problem-5",
    "title": "Final Exam Study Guide",
    "section": "Problem 5",
    "text": "Problem 5\nA JP Morgan recruiter is walking around campus with three bone-colored business cards. Two of the cards are normal; they have writing on one side and they are blank on the other. The third card was misprinted; the writing was duplicated on both sides. The recruiter drops one of the cards and orders his valet to pick it up. When the valet reaches for the card, he sees writing facing up. What is the probability that the recruiter dropped the misprinted card?",
    "crumbs": [
      "Exam Study Guides",
      "Final"
    ]
  },
  {
    "objectID": "problems/exam-final.html#problem-6",
    "href": "problems/exam-final.html#problem-6",
    "title": "Final Exam Study Guide",
    "section": "Problem 6",
    "text": "Problem 6\nSuppose that \\(k\\) events \\(B_1\\), \\(B_2\\), …, \\(B_k\\) form a partition of the sample space \\(S\\), and \\(A\\subseteq S\\) is some event with positive probability \\(P(A)&gt;0\\). Show that if \\(P(B_1\\mid A)&lt;P(B_1)\\), then that guarantees \\(P(B_i\\mid A)&gt;P(B_i)\\) for at least one \\(i\\in\\{1,\\,2,\\,...,\\,k\\}\\).",
    "crumbs": [
      "Exam Study Guides",
      "Final"
    ]
  },
  {
    "objectID": "problems/exam-final.html#want-more-review-2",
    "href": "problems/exam-final.html#want-more-review-2",
    "title": "Final Exam Study Guide",
    "section": "Want more review?",
    "text": "Want more review?\n\nLecture notes with worked examples;\n\nThe lil’ app on disease testing;\n\nProblem Set 2: problems 9 - 10;\nProblem Set 3: problems 4 - 8;\nProblem Set 4: problems 2 - 3;\nMidterm 1 Study Guide: problems 8 - 10;\nMidterm 1: problems 3 - 5;\nOdd-numbered exercises in DeGroot & Schervish Chapter 2.",
    "crumbs": [
      "Exam Study Guides",
      "Final"
    ]
  },
  {
    "objectID": "problems/exam-final.html#problem-7",
    "href": "problems/exam-final.html#problem-7",
    "title": "Final Exam Study Guide",
    "section": "Problem 7",
    "text": "Problem 7\nConsider a sequence of \\(n\\) independent coin flips of a fair coin. Define a streak of heads as an uninterrupted run of heads, and efine a streak of tails analogously. We are interested in the number of streaks in a sequence of flips. For example, consider the sequence below:\n\\[\nhhhttthhttthhhhhthtthhhh.\n\\]\nIt contains 9 streaks. Here is the sequence again with the streaks separated:\n\\[\nhhh\\quad ttt\\quad hh\\quad ttt\\quad hhhhh\\quad t\\quad h\\quad tt\\quad hhhh.\n\\]\nIn a sequence of \\(n\\) flips of a fair coin, let \\(Y\\) be the number of streaks. What is the range of \\(Y\\), and what is its PMF?",
    "crumbs": [
      "Exam Study Guides",
      "Final"
    ]
  },
  {
    "objectID": "problems/exam-final.html#problem-8",
    "href": "problems/exam-final.html#problem-8",
    "title": "Final Exam Study Guide",
    "section": "Problem 8",
    "text": "Problem 8\nLet \\(X\\sim\\text{Exponential}(\\lambda)\\) for some arbitrary \\(\\lambda&gt;0\\). Define a new random variable \\(Y=\\lceil X\\rceil\\). Recall that the ceiling function \\(\\lceil\\cdot\\rceil\\) is the function that rounds a number up to the next integer. So \\(\\lceil0.5\\rceil=1\\), \\(\\lceil13.1\\rceil=14\\), and so on.\n\nWhat is the range of \\(Y\\)?\nWhat is the PMF of \\(Y\\)?\nDoes \\(Y\\) belong to a familiar distribution family?",
    "crumbs": [
      "Exam Study Guides",
      "Final"
    ]
  },
  {
    "objectID": "problems/exam-final.html#want-more-review-3",
    "href": "problems/exam-final.html#want-more-review-3",
    "title": "Final Exam Study Guide",
    "section": "Want more review?",
    "text": "Want more review?\n\nLecture notes introducing random variables;\nLecture notes on special families of discrete random variables;\nLecture notes on expected value;\nLab 5 exercises;\nProblem Set 2: problem 7;\nProblem Set 3: problems 9 - 10;\nProblem Set 4: problems 4 - 10;\nMidterm 1 Study Guide: problems 11 - 12;\nMidterm 2 Study Guide: problems 1 - 2;\nMidterm 1: problem 6;\nMidterm 2: problem 2.",
    "crumbs": [
      "Exam Study Guides",
      "Final"
    ]
  },
  {
    "objectID": "problems/exam-final.html#problem-9",
    "href": "problems/exam-final.html#problem-9",
    "title": "Final Exam Study Guide",
    "section": "Problem 9",
    "text": "Problem 9\nImagine \\(X\\) has this cdf:\n\\[\nF(x)=\\frac{1}{1+\\exp\\left(-\\frac{x-\\mu}{s}\\right)},\\quad x\\in\\mathbb{R}.\n\\]\n\\(\\mu\\in\\mathbb{R}\\) and \\(s&gt;0\\) are just constants.\n\nWhat is the pdf of \\(X\\)?\nWhat is the mean of \\(X\\)?",
    "crumbs": [
      "Exam Study Guides",
      "Final"
    ]
  },
  {
    "objectID": "problems/exam-final.html#problem-10",
    "href": "problems/exam-final.html#problem-10",
    "title": "Final Exam Study Guide",
    "section": "Problem 10",
    "text": "Problem 10\nHere is the goofy pdf of some nonnegative random variable \\(X\\):\n\\[\nf(x)\n=\ne^{1-e^x+x},\\quad x\\geq 0.\n\\]\n\nCompute the cdf of \\(X\\);\nCompute the median of \\(X\\).",
    "crumbs": [
      "Exam Study Guides",
      "Final"
    ]
  },
  {
    "objectID": "problems/exam-final.html#want-more-review-4",
    "href": "problems/exam-final.html#want-more-review-4",
    "title": "Final Exam Study Guide",
    "section": "Want more review?",
    "text": "Want more review?\n\nExamples from lecture;\nLab 8;\nProblem Set 5: problems 2 - 8;\nProblem Set 6: problems 1 - 2;\nMidterm 2 Study Guide: problems 3 - 4;\nMidterm 2: problem 1;\nOdd-numbered exercises in DeGroot & Schervish Chapters 3.2, 5.6, 5.7.",
    "crumbs": [
      "Exam Study Guides",
      "Final"
    ]
  },
  {
    "objectID": "problems/exam-final.html#problem-11",
    "href": "problems/exam-final.html#problem-11",
    "title": "Final Exam Study Guide",
    "section": "Problem 11",
    "text": "Problem 11\nA certain random variable \\(X\\) has moment-generating function\n\\[\nM(t)=\\frac{e^{2t}}{1-3t},\\quad t&lt;1/3.\n\\]\n\nCompute the mean and variance of \\(X\\).\nWhat is the moment generating function of the new random variable\n\n\\[\nY=\\frac{\\pi}{2}-\\frac{X}{3}.\n\\]",
    "crumbs": [
      "Exam Study Guides",
      "Final"
    ]
  },
  {
    "objectID": "problems/exam-final.html#problem-12",
    "href": "problems/exam-final.html#problem-12",
    "title": "Final Exam Study Guide",
    "section": "Problem 12",
    "text": "Problem 12\nConsider the following:\n\\[\n\\begin{aligned}\nN&\\sim\\text{Poisson}(\\lambda)\\\\\nX_1,\\,X_2,\\,X_3,\\,...&\\overset{\\text{iid}}{\\sim}M\\\\\nS&=\\sum\\limits_{i=1}^NX_i.\n\\end{aligned}\n\\]\n\\(N\\) is independent of all of the \\(X_i\\). The \\(X_i\\) are an infinite sequence of iid random variables each sharing a common moment-generating function \\(M(t)=E(e^{tX_1})\\). \\(S\\) is a random sum of random variables. The terms are random, and so is \\(N\\), the number of terms being summed.\n\nWhat is the MGF of \\(S\\)?\nIf I had an iid collection of \\(S_1\\), \\(S_2\\), …, \\(S_m\\) each possessing the same distribution that you just derived, what would be the distribution of \\(T=S_1+S_2+...+S_m\\)?",
    "crumbs": [
      "Exam Study Guides",
      "Final"
    ]
  },
  {
    "objectID": "problems/exam-final.html#want-more-review-5",
    "href": "problems/exam-final.html#want-more-review-5",
    "title": "Final Exam Study Guide",
    "section": "Want more review?",
    "text": "Want more review?\n\nworked examples from lecture;\nnew theorems and examples from lecture about updating MGFs under linear transformations;\nProblem Set 5: problems 9 - 10;\nProblem Set 6: problems 3 - 5;\nProblem Set 7: problems 1 - 2;\nMidterm 2 Study Guide: problems 11 - 12;\nMidterm 2: problem 2;\nOdd-numbered exercises in DeGroot & Schervish Chapter 4.4.",
    "crumbs": [
      "Exam Study Guides",
      "Final"
    ]
  },
  {
    "objectID": "problems/exam-final.html#sec-weibull",
    "href": "problems/exam-final.html#sec-weibull",
    "title": "Final Exam Study Guide",
    "section": "Problem 13",
    "text": "Problem 13\n\\(X\\) possesses the goofy distribution if its PDF is:\n\\[\nf(x)=k\\frac{x^{k-1}}{\\theta}\\exp\\left(-\\frac{x^k}{\\theta}\\right),\\quad x&gt;0.\n\\]\nThe constants \\(k\\) and \\(\\theta\\) are both positive parameters. We denote this \\(X\\sim\\text{GF}(k,\\,\\theta)\\)\n\nIf we define a new random variable \\(Y=\\ln X\\), what is its distribution?\nIf we define a new random variable \\(Z = X^k\\), what is its distribution?",
    "crumbs": [
      "Exam Study Guides",
      "Final"
    ]
  },
  {
    "objectID": "problems/exam-final.html#problem-14",
    "href": "problems/exam-final.html#problem-14",
    "title": "Final Exam Study Guide",
    "section": "Problem 14",
    "text": "Problem 14\nLet \\(X\\sim\\text{Gamma}(\\alpha,\\,\\beta)\\), and define a new random variable \\(Y=X^r\\) for any positive constant \\(r&gt;0\\).\n\nWhat is the density of \\(Y\\)?\nDerive a formula for \\(E(X^n)\\) that works for any \\(n\\in\\mathbb{N}\\).",
    "crumbs": [
      "Exam Study Guides",
      "Final"
    ]
  },
  {
    "objectID": "problems/exam-final.html#want-more-review-6",
    "href": "problems/exam-final.html#want-more-review-6",
    "title": "Final Exam Study Guide",
    "section": "Want more review?",
    "text": "Want more review?\n\nLab 7;\nProblem Set 6: problem 6;\nMidterm 2 Study Guide: problem 7;\nMidterm 2: problem 3;\nRead DeGroot & Schervish Chapter 3.8 and do the odd-numbered exercises.",
    "crumbs": [
      "Exam Study Guides",
      "Final"
    ]
  },
  {
    "objectID": "problems/exam-final.html#problem-15",
    "href": "problems/exam-final.html#problem-15",
    "title": "Final Exam Study Guide",
    "section": "Problem 15",
    "text": "Problem 15\n\\(X\\) and \\(Y\\) are jointly absolutely continuous with joint density\n\\[\nf_{XY}(x,\\, y)\n=\n\\frac{1}{8}\n(y^2-x^2)\ne^{-y}\n,\\quad\ny&gt;0\n;\\,\n-y&lt;x&lt;y.\n\\]\n\nSketch \\(\\textrm{Range}(X,\\, Y)\\).\nCompute the marginal density of \\(X\\).\nCompute the marginal density of \\(Y\\).\nCompute the conditional density of \\(X\\) given \\(Y = y\\).\nCompute the conditional density of \\(Y\\) given \\(X = x\\).",
    "crumbs": [
      "Exam Study Guides",
      "Final"
    ]
  },
  {
    "objectID": "problems/exam-final.html#problem-16",
    "href": "problems/exam-final.html#problem-16",
    "title": "Final Exam Study Guide",
    "section": "Problem 16",
    "text": "Problem 16\nConsider a random pair \\((Q,\\,Z)\\) of continuous random variables whose joint distribution is given by this hierarchy:\n\\[\n\\begin{aligned}\nQ&\\sim \\text{Gamma}(a,\\,b)\\\\\nZ\\mid Q=q&\\sim\\text{GF}(k, 1/q).\n\\end{aligned}\n\\]\nSo conditionally, \\(Z\\) has the distribution introduced in Problem 13 above, with \\(1/q\\) serving as the second parameter. \\(k&gt;0\\) is just a constant throughout.\n\nWhat is the joint density of \\((Q,\\,Z)\\)?\nWhat is the marginal distribution of \\(Z\\)?\nWhat is the conditional distribution of \\(Q\\) given \\(Z=z\\)?",
    "crumbs": [
      "Exam Study Guides",
      "Final"
    ]
  },
  {
    "objectID": "problems/exam-final.html#want-more-review-7",
    "href": "problems/exam-final.html#want-more-review-7",
    "title": "Final Exam Study Guide",
    "section": "Want more review?",
    "text": "Want more review?\n\njointly discrete examples from lecture;\njointly contnuous examples from lecture;\nmixed Poisson-gamma example from Lab 9;\nProblem Set 6: problems 7 - 10;\nMidterm 2 Study Guide: problems 8 - 9;\nMidterm 2: problem 4;\nOdd-numbered exercises in DeGroot & Schervish Chapters 3.4 - 3.6.",
    "crumbs": [
      "Exam Study Guides",
      "Final"
    ]
  },
  {
    "objectID": "problems/exam-final.html#problem-17",
    "href": "problems/exam-final.html#problem-17",
    "title": "Final Exam Study Guide",
    "section": "Problem 17",
    "text": "Problem 17\nConsider these data:\n\\[\nX_1,\\,X_2,\\,...,\\,X_n\\overset{\\text{iid}}{\\sim}\\text{N}(\\theta,\\,1).\n\\]\n\nWhat is the maximum likelihood estimator of \\(\\theta\\in\\mathbb{R}\\)?\nWhat is the sampling distribution of the estimator?\nWhat is the MSE of the estimator?\nBased on the MSE, what are the statistical properties of this estimator?",
    "crumbs": [
      "Exam Study Guides",
      "Final"
    ]
  },
  {
    "objectID": "problems/exam-final.html#problem-18",
    "href": "problems/exam-final.html#problem-18",
    "title": "Final Exam Study Guide",
    "section": "Problem 18",
    "text": "Problem 18\nConsider these data:\n\\[\nX_1,\\,X_2,\\,...,\\,X_n\\overset{\\text{iid}}{\\sim}\\text{GF}(k,\\,\\theta).\n\\]\nSo again, we are recycling the distribution family from Problem 13 above. Throughout, just treat \\(k&gt;0\\) as fixed and known.\n\nWhat is the maximum likelihood estimator of \\(\\theta\\in\\mathbb{R}\\)?\nWhat is the sampling distribution of the estimator?\nWhat is the MSE of the estimator?\nBased on the MSE, what are the statistical properties of this estimator?",
    "crumbs": [
      "Exam Study Guides",
      "Final"
    ]
  },
  {
    "objectID": "problems/exam-final.html#want-more-review-8",
    "href": "problems/exam-final.html#want-more-review-8",
    "title": "Final Exam Study Guide",
    "section": "Want more review?",
    "text": "Want more review?\n\nLecture examples;\nLab 10;\nProblem Set 7: problems 3 - 4.",
    "crumbs": [
      "Exam Study Guides",
      "Final"
    ]
  },
  {
    "objectID": "problems/exam-final.html#problem-19",
    "href": "problems/exam-final.html#problem-19",
    "title": "Final Exam Study Guide",
    "section": "Problem 19",
    "text": "Problem 19\nConsider this Bayesian model:\n\\[\n\\begin{aligned}\n\\theta&\\sim\\text{N}(m_0,\\,\\tau_0^2)\\\\\nX_{1:n}\\mid\\theta&\\overset{\\text{iid}}{\\sim}\\text{N}(\\theta,\\,1).\n\\end{aligned}\n\\]\n\nWhat is the posterior distribution for \\(\\theta\\) conditional on the data?\nDerive the posterior mean and show that it is a convex combination of the prior mean \\(m_0\\) and the MLE.\nThe posterior mean can be thought of as an alternative estimator, in a classical sense. So, what are its mean, variance, bias, and MSE?\nIs it possible for this new estimator to be superior (lower MSE) than the pure MLE?",
    "crumbs": [
      "Exam Study Guides",
      "Final"
    ]
  },
  {
    "objectID": "problems/exam-final.html#problem-20",
    "href": "problems/exam-final.html#problem-20",
    "title": "Final Exam Study Guide",
    "section": "Problem 20",
    "text": "Problem 20\nConsider this Bayesian model:\n\\[\n\\begin{aligned}\n\\theta&\\sim\\text{IG}(a_0,\\,b_0)\\\\\nX_{1:n}\\mid\\theta&\\overset{\\text{iid}}{\\sim}\\text{N}(0,\\,\\theta).\n\\end{aligned}\n\\]\n\nWhat is the posterior distribution for \\(\\theta\\) conditional on the data?\nDerive the posterior mean and show that it is a convex combination of the prior mean and the MLE;\nThe posterior mean can be thought of as an alternative estimator, in a classical sense. So, what are its mean, variance, bias, and MSE?\nIs it possible for this new estimator to be superior (lower MSE) than the pure MLE?",
    "crumbs": [
      "Exam Study Guides",
      "Final"
    ]
  },
  {
    "objectID": "problems/exam-final.html#problem-21",
    "href": "problems/exam-final.html#problem-21",
    "title": "Final Exam Study Guide",
    "section": "Problem 21",
    "text": "Problem 21\nRecall this infernal distribution family that you met on Problem Set 6, Midterm 2, and again in lecture:\n\\[\nf(x\\mid \\theta)=\\theta(x+1)^{-(\\theta+1)},\\quad x&gt;0.\n\\]\nWe’ve done MLE for this, now let’s go Bayes. Consider this model:\n\\[\n\\begin{aligned}\n\\theta&\\sim\\text{Gamma}(a_0,\\,b_0)\\\\\nX_{1:n}\\mid\\theta&\\overset{\\text{iid}}{\\sim}f(x\\mid \\theta).\n\\end{aligned}\n\\]\n\nWhat is the posterior distribution for \\(\\theta\\) conditional on the data?\nDerive the posterior mean and show that it is a convex combination of the prior mean and the MLE;\nThe posterior mean can be thought of as an alternative estimator, in a classical sense. So, what are its mean, variance, bias, and MSE?\nIs it possible for this new estimator to be superior (lower MSE) than the pure MLE?",
    "crumbs": [
      "Exam Study Guides",
      "Final"
    ]
  },
  {
    "objectID": "problems/exam-final.html#want-more-review-9",
    "href": "problems/exam-final.html#want-more-review-9",
    "title": "Final Exam Study Guide",
    "section": "Want more review?",
    "text": "Want more review?\n\nLecture examples;\nLab 11;\nProblem Set 7: problems 5 - 6.",
    "crumbs": [
      "Exam Study Guides",
      "Final"
    ]
  },
  {
    "objectID": "problems/pset-3.html",
    "href": "problems/pset-3.html",
    "title": "Problem Set 3",
    "section": "",
    "text": "Recommend some music for us to listen to while we grade this.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 3"
    ]
  },
  {
    "objectID": "problems/pset-3.html#problem-0",
    "href": "problems/pset-3.html#problem-0",
    "title": "Problem Set 3",
    "section": "",
    "text": "Recommend some music for us to listen to while we grade this.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 3"
    ]
  },
  {
    "objectID": "problems/pset-3.html#problem-1",
    "href": "problems/pset-3.html#problem-1",
    "title": "Problem Set 3",
    "section": "Problem 1",
    "text": "Problem 1\nHere are some short readings about the case of Sally Clark and Roy Meadow, as well as something called the prosecutor’s fallacy:\n\nGrimes (2023): “Bad Science and Bad Statistics in the Courtroom Convict Innocent People,” Scientific American;\nCarriquiry (2018): “Misuse of statistics in the courtroom: the Sally Clark case,” CSAFE;\nWestreich & Iliinsky (2014): “Epidemiology visualized: the prosecutor’s fallacy,” American Journal of Epidemiology;\n\nAP Dawid’s statement during the Clark appeal.\n\nPlease do the following:\n\nRead the articles;\nUsing the ideas of conditional probability and independence that we have studied, write a paragraph or two describing what the prosecutor’s fallacy is and how it applies to the Clark case;\nIf you were an attorney for the defense and had the chance to cross-examine Roy Meadow, what would you ask him about his statistical testimony? What are the most incisive questions you could ask? How would you ensure that the cross-examination would “land” with the members of the jury, who most likely know absolutely nothing about probability and statistics?\n\n\n\n\n\n\n\nPlease don’t break my heart\n\n\n\nI’m not asking you to read Moby-Dick here.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 3"
    ]
  },
  {
    "objectID": "problems/pset-3.html#problem-2",
    "href": "problems/pset-3.html#problem-2",
    "title": "Problem Set 3",
    "section": "Problem 2",
    "text": "Problem 2\nPoker dice is played by simultaneously rolling five dice. The dice are fair, six-sided, and distinct (imagine that each one is a different color). Find the probabilities of the following dice rolls:\n\nfive different numbers;\nthree of a kind (three dice show the same number, while the other two dice are singles);\nfull house (three dice show the same number, while the other two show a separate pair).",
    "crumbs": [
      "Problem Sets",
      "Problem Set 3"
    ]
  },
  {
    "objectID": "problems/pset-3.html#problem-3",
    "href": "problems/pset-3.html#problem-3",
    "title": "Problem Set 3",
    "section": "Problem 3",
    "text": "Problem 3\nConsider selecting \\(k\\) from \\(n\\) with replacement but ignoring order.\n\nLet’s take the specific case of \\(k=2\\) and \\(n=4\\). List out all possible ways you can select \\(k=2\\) items from \\(\\{1, 2, 3, 4\\}\\) in such a scenario.\nLet \\(x_i\\) be the number of times that element \\(\\{i\\}\\) is selected, for each \\(i = 1,\\,2,\\,3,\\, 4\\). For each combination identified in (a), show that it satisfies the following equation:\n\n\\[x_1 + x_2 + x_3 + x_4 = 2, \\quad x_1, x_2, x_3, x_4 \\in \\{0, 1, 2\\}.\\]\n\nState an analogous equation to part b for general \\(n\\) and \\(k\\). Use this to show that the number of ways you can select \\(k\\) from \\(n\\) with replacement and ignoring order is\n\n\\[\n\\binom{n+k-1}{k}=\\frac{(n+k-1)!}{k!(n-1)!}.\n\\]",
    "crumbs": [
      "Problem Sets",
      "Problem Set 3"
    ]
  },
  {
    "objectID": "problems/pset-3.html#problem-4",
    "href": "problems/pset-3.html#problem-4",
    "title": "Problem Set 3",
    "section": "Problem 4",
    "text": "Problem 4\nLet \\(S\\) be a sample space with probability measure \\(P\\), and let \\(B\\subseteq S\\) be some event with \\(P(B)&gt;0\\). Show that the function \\(G(A)=P(A\\,|\\, B)\\) is a new probability measure on \\(S\\). That is, show that \\(G\\) satisfies the axioms:\n\ntotal measure 1;\nnonnegativity;\ncountable additivity.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 3"
    ]
  },
  {
    "objectID": "problems/pset-3.html#problem-5",
    "href": "problems/pset-3.html#problem-5",
    "title": "Problem Set 3",
    "section": "Problem 5",
    "text": "Problem 5\nWe studied disease testing in class, and in our stylized example, a patient had only two attributes: their true disease status \\(D\\) and their test result \\(T\\). In reality of course, patients have many more relevant attributes: genetics, prior medical history, lifestyle, charm and good looks, etc. So in an attempt to be slightly more realistic, we shall extend our little model to include a third attribute: whether or not you have symptoms \\(S\\).\nTable 1 enumerates all of the possible states of the world together with their individual probabilities. These numbers come from a small COVID-19 study of undergraduates in Fall 2020. The test in question is an antigen test. Patients’ “true” disease status was determined by a PCR test that we assume is hella accurate.\n\n\n\nTable 1: Some COVID-19 numbers. Source: CDC\n\n\n\n\n\n\n\n\n\n\\(D\\)\n\\(S\\)\n\\(T\\)\n\\(P(D\\cap S\\cap T)\\)\n\n\n\n-\n-\n-\n0.7650\n\n\n-\n-\n+\n0.0128\n\n\n-\n+\n-\n0.1685\n\n\n-\n+\n+\n0.0018\n\n\n+\n-\n-\n0.0091\n\n\n+\n-\n+\n0.0064\n\n\n+\n+\n-\n0.0073\n\n\n+\n+\n+\n0.0291\n\n\n\n\n\n\n\n\nWhat is the overall disease prevalence?\nWhat is the sensitivity of the test?\nWhat is the specificity of the test?\nImagine you develop symptoms. So you visit Dr. Vinnie Boombatz to get tested, and the test comes back positive. Given everything we now know about you, what is the probability that you are truly infected?\nWhat is the overall probability that the test is wrong?",
    "crumbs": [
      "Problem Sets",
      "Problem Set 3"
    ]
  },
  {
    "objectID": "problems/pset-3.html#problem-6",
    "href": "problems/pset-3.html#problem-6",
    "title": "Problem Set 3",
    "section": "Problem 6",
    "text": "Problem 6\n\n\n\n\n\nFigure 1\n\n\nJerry Lewis is throwing darts at the board pictured in Figure 1. It has the following properties:\n\nThe board has radius \\(r\\);\nThe board is partitioned into five rings, each with the same “thickness” \\(r/5\\);\nThe board is mounted on a wall of area \\(A\\).\n\nJerry is not very skilled. His dart is guaranteed to reach the wall and stick someplace, but beyond that, the probability that it lands in any given region is just proportional to that region’s area.\nTask: Derive a general formula for the conditional probability that a dart lands in ring \\(i\\) given that it hits the board and not the wall. Your formula should be a function of \\(i=1,\\,2,\\,3,\\,4,\\,5\\). Check that these probabilities are nonnegative and sum to one.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 3"
    ]
  },
  {
    "objectID": "problems/pset-3.html#problem-7",
    "href": "problems/pset-3.html#problem-7",
    "title": "Problem Set 3",
    "section": "Problem 7",
    "text": "Problem 7\nAn encryption algorithm generates a seed value \\(z\\) in the following way:\n\nA binary operator is chosen at random from \\(\\{+,\\,\\cdot\\}\\);\nTwo distinct numbers \\(x\\) and \\(y\\) are chosen at random from the set \\(\\{1,\\,2,\\,...,\\,9\\}\\) and the binary operation chosen above acts on \\(x\\) and \\(y\\) to produce \\(z\\).\n\n\nWhat is the probability that \\(z\\) is an even number?\nYou notice that the algorithm generates an even \\(z\\) value. Given this, what is the probability that the \\(+\\) operator was used to generate it?",
    "crumbs": [
      "Problem Sets",
      "Problem Set 3"
    ]
  },
  {
    "objectID": "problems/pset-3.html#problem-8",
    "href": "problems/pset-3.html#problem-8",
    "title": "Problem Set 3",
    "section": "Problem 8",
    "text": "Problem 8\n\n\n\n\nThe figure above displays a circuit. Switch \\(S_i\\) closes with probability \\(p_i\\), and the switches close independently of one another. What is the probability that electricity can flow from \\(A\\) to \\(G\\)?",
    "crumbs": [
      "Problem Sets",
      "Problem Set 3"
    ]
  },
  {
    "objectID": "problems/pset-3.html#problem-9",
    "href": "problems/pset-3.html#problem-9",
    "title": "Problem Set 3",
    "section": "Problem 9",
    "text": "Problem 9\nTwo balls are chosen randomly from an urn containing 8 green, 4 black, and 2 orange balls. Suppose that we win $2 for each black ball selected, we lose $1 for each green ball selected, and we earn nothing for each orange ball selected. Let \\(X\\) denote our winnings.\n\nMake a table with the possible values of \\(X\\) and the probabilities of each value.\nSketch the pmf of \\(X\\).\nSketch the cdf of \\(X\\).\nCompute the expected value of \\(X\\).",
    "crumbs": [
      "Problem Sets",
      "Problem Set 3"
    ]
  },
  {
    "objectID": "problems/pset-3.html#problem-10",
    "href": "problems/pset-3.html#problem-10",
    "title": "Problem Set 3",
    "section": "Problem 10",
    "text": "Problem 10\n\n\n\n\n\nI dreamed last night I got on the boat to heavenAnd by some chance I had brought my dice along\nGuys and Dolls\n\nAlas. You settle in for what you think will be a good night’s beauty sleep, and you end up having one of those awful dreams where you must take a test you haven’t studied for. It’s a multiple choice test, each question has five options with only one correct answer, there are fifty questions in total, and each is worth one point. The god Morpheus has kindly seen fit to leave a fair, five-sided die in your dream pocket, and so your dream self decides to do the only thing you can do: use the die to answer each of the exam questions completely at random.\n\nLet \\(X\\) be the number of questions you answer correctly. What is the distribution of \\(X\\)?\nPlot the probability mass function of \\(X\\);\nWhat is the expected number of questions you will answer correctly?\nWhat is the probability that you get at least a C- on the exam?",
    "crumbs": [
      "Problem Sets",
      "Problem Set 3"
    ]
  },
  {
    "objectID": "problems/pset-3.html#submission",
    "href": "problems/pset-3.html#submission",
    "title": "Problem Set 3",
    "section": "Submission",
    "text": "Submission\nYou are free to compose your solutions for this problem set however you wish (scan or photograph written work, handwriting capture on a tablet device, LaTeX, Quarto, whatever) as long as the final product is a single PDF file. You must upload this to Gradescope and mark the pages associated with each problem.\nDo not forget to include the following:\n\nFor each problem, please acknowledge your collaborators;\nIf a problem required you to code something, please include both the code and the output. “Including the code” can be as crude as a screenshot, but you might also use Quarto to get a nice lil’ pdf that you can merge with the rest of your submission.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 3"
    ]
  },
  {
    "objectID": "problems/exam-midterm-1.html",
    "href": "problems/exam-midterm-1.html",
    "title": "Midterm 1 Study Guide",
    "section": "",
    "text": "Midterm Exam 1 will take place on Thursday October 9 during your lab section. There will be 7 problems, and they will break down as follows:\nBelow are new practice problems for each of these areas, as well as guidance about what course materials to refer to if you want more review on a particular topic.",
    "crumbs": [
      "Exam Study Guides",
      "Midterm 1"
    ]
  },
  {
    "objectID": "problems/exam-midterm-1.html#problem-1",
    "href": "problems/exam-midterm-1.html#problem-1",
    "title": "Midterm 1 Study Guide",
    "section": "Problem 1",
    "text": "Problem 1\nOut of the students in a class, 60% are geniuses, 70% love chocolate, and 40% fall into both categories. Determine the probability that a randomly selected student is neither a genius nor a chocolate lover.",
    "crumbs": [
      "Exam Study Guides",
      "Midterm 1"
    ]
  },
  {
    "objectID": "problems/exam-midterm-1.html#problem-2",
    "href": "problems/exam-midterm-1.html#problem-2",
    "title": "Midterm 1 Study Guide",
    "section": "Problem 2",
    "text": "Problem 2\nSuppose that \\(A\\) and \\(B\\) are mutually exclusive events for which \\(P(A) = 0.3\\) and \\(P(B) = 0.5\\). What is the probability that…\n\neither \\(A\\) or \\(B\\) occurs?\n\\(A\\) occurs but \\(B\\) does not?\nboth \\(A\\) and \\(B\\) occur?\nAre \\(A\\) and \\(B\\) independent? Why or why not?",
    "crumbs": [
      "Exam Study Guides",
      "Midterm 1"
    ]
  },
  {
    "objectID": "problems/exam-midterm-1.html#problem-3",
    "href": "problems/exam-midterm-1.html#problem-3",
    "title": "Midterm 1 Study Guide",
    "section": "Problem 3",
    "text": "Problem 3\nLet \\(S\\) be a sample space, let \\(A,\\, B\\subseteq S\\) be events, and define a new set\n\\[\nA-B = \\{x\\in S:x\\in A\\text{ and }x\\notin B\\}.\n\\]\n\nDraw a well-labeled picture of \\(A\\), \\(B\\), \\(S\\), and the set \\(A-B\\);\nWrite down an equivalent expression for \\(A-B\\) that only makes use of our three basic operations: union, intersection, and complement;\nProve that \\(P(A-B)=P(A)-P(A\\cap B)\\).",
    "crumbs": [
      "Exam Study Guides",
      "Midterm 1"
    ]
  },
  {
    "objectID": "problems/exam-midterm-1.html#problem-4",
    "href": "problems/exam-midterm-1.html#problem-4",
    "title": "Midterm 1 Study Guide",
    "section": "Problem 4",
    "text": "Problem 4\nLet \\(A\\), \\(B\\), and \\(C\\) be three arbitrary events, and let \\(D\\) denote the event that “exactly one of these three events occurs.”\n\nDraw a cartoon of \\(A\\), \\(B\\), \\(C\\), and \\(D\\);\nUse the three basic set operations (union, intersection, and complement) to express \\(D\\) in terms of \\(A\\), \\(B\\), and \\(C\\);\nShow that\n\n\\[\nP(D)=P(A) + P(B) + P(C) - 2P(A \\cap B) - 2P(A \\cap C) - 2P(B \\cap C) + 3P(A \\cap B \\cap C).\n\\]",
    "crumbs": [
      "Exam Study Guides",
      "Midterm 1"
    ]
  },
  {
    "objectID": "problems/exam-midterm-1.html#want-more-review",
    "href": "problems/exam-midterm-1.html#want-more-review",
    "title": "Midterm 1 Study Guide",
    "section": "Want more review?",
    "text": "Want more review?\n\nLecture notes on axioms and rules;\nProblem Set 1: problems 4 - 8;\nProblem Set 2: problems 2 - 3;",
    "crumbs": [
      "Exam Study Guides",
      "Midterm 1"
    ]
  },
  {
    "objectID": "problems/exam-midterm-1.html#problem-5",
    "href": "problems/exam-midterm-1.html#problem-5",
    "title": "Midterm 1 Study Guide",
    "section": "Problem 5",
    "text": "Problem 5\nSuppose you are dealt six cards from a well-shuffled, standard deck of 52.\n\nWhat is the probability of getting 3 of one rank and 3 of another?\nWhat is the probability of getting 4 of one rank and 2 of another?\nWhat is the probability of getting 3 aces and 3 of another rank?\nWhat is the probability of getting 4 of one rank and 2 aces?\nWhat is the probability that all 6 cards are in the same suit?\nWhat is the probability that all 6 cards are consecutive (ie a hand with 3 hearts, 4 spades, 5 spades, 6 clubs, 7 hearts, and the 8 diamonds)? Assume that the ace can only be the high card;\nWhat is the answer to the previous question if we allow the ace to be either high or low?\nWhat is the probability of receiving the ace, king, queen, jack, 10 and 9 all in hearts?",
    "crumbs": [
      "Exam Study Guides",
      "Midterm 1"
    ]
  },
  {
    "objectID": "problems/exam-midterm-1.html#problem-6",
    "href": "problems/exam-midterm-1.html#problem-6",
    "title": "Midterm 1 Study Guide",
    "section": "Problem 6",
    "text": "Problem 6\nCongratulations! You and two of your friends are now proud owners of brand-new library cards. In order to use your new cards, all you and your friends need to do is create four-digit PINs, with numbers \\(0\\) - \\(9\\) available (in theory) at each position. Predictably, all of you have different ideas of what an acceptable PIN is (odd or even, leading zeros or not, etc.).\n\nYou are fairly chill about what you want in a PIN – all you want is an even PIN, and you’re totally fine if it starts with a zero. How many possible PINs fit your preference?\nOne of your friends is a bit more persnickety – they want a PIN divisible by \\(5\\), and they cannot stand PINs starting with a zero. How many possible PINs fit this friend’s preferences?\nYour other friend has some more…particular preferences – they have an affinity for the number \\(16\\) and thus want the digits \\(1\\) and \\(6\\) to appear in that exact order at some point in the PIN. Further, they also despise PINs starting with a zero. How many possible PINs fit this friend’s preferences?",
    "crumbs": [
      "Exam Study Guides",
      "Midterm 1"
    ]
  },
  {
    "objectID": "problems/exam-midterm-1.html#problem-7",
    "href": "problems/exam-midterm-1.html#problem-7",
    "title": "Midterm 1 Study Guide",
    "section": "Problem 7",
    "text": "Problem 7\nThe United States Senate contains two senators – one senior and one junior – from each of the \\(50\\) states.\n\nIf a committee of eight senators is selected, what is the probability that it will contain at least one of the two senators from a certain specified state?\nIf a committee of twenty senators is selected, what is the probability it will contain \\(15\\) junior senators and \\(5\\) senior senators?\nWhat is the probability that a group of \\(50\\) senators selected at random will contain one senator from each state?",
    "crumbs": [
      "Exam Study Guides",
      "Midterm 1"
    ]
  },
  {
    "objectID": "problems/exam-midterm-1.html#want-more-review-1",
    "href": "problems/exam-midterm-1.html#want-more-review-1",
    "title": "Midterm 1 Study Guide",
    "section": "Want more review?",
    "text": "Want more review?\n\nLecture notes on counting theory;\nLecture notes with birthday problem and other worked examples;\nLab 2 worked examples;\nProblem Set 1: problems 9 - 10;\nProblem Set 2: problems 4 - 8;\nProblem Set 3: problems 2 - 3;\nOdd-numbered exercises in DeGroot & Schervish Chapters 1.6 - 1.8, 1.12.",
    "crumbs": [
      "Exam Study Guides",
      "Midterm 1"
    ]
  },
  {
    "objectID": "problems/exam-midterm-1.html#problem-8",
    "href": "problems/exam-midterm-1.html#problem-8",
    "title": "Midterm 1 Study Guide",
    "section": "Problem 8",
    "text": "Problem 8\nIn a standard deck of \\(52\\) cards, 3 (Jack, Queen, and King) of the 13 ranks are collectively known as the “face cards” of the deck. Recall that a standard deck of \\(52\\) cards also has \\(4\\) suits (Hearts, Diamonds, Clubs, and Spades).\nSuppose you deal out two cards from the deck. What is the probability that they are both face cards given that they are both hearts?",
    "crumbs": [
      "Exam Study Guides",
      "Midterm 1"
    ]
  },
  {
    "objectID": "problems/exam-midterm-1.html#problem-9",
    "href": "problems/exam-midterm-1.html#problem-9",
    "title": "Midterm 1 Study Guide",
    "section": "Problem 9",
    "text": "Problem 9\nAn urn contains 5 white and 10 black balls. Four tickets labeled 1, 2, 3, and 4 reside in a box. A ticket is drawn at random from the box, and that number of balls is randomly chosen from the urn, without replacement.\n\nWhat is the probability that all of the balls drawn are white?\nIf we learn that all balls drawn from the urn are white, what is the conditional probability that the #3 ticket was drawn?",
    "crumbs": [
      "Exam Study Guides",
      "Midterm 1"
    ]
  },
  {
    "objectID": "problems/exam-midterm-1.html#problem-10",
    "href": "problems/exam-midterm-1.html#problem-10",
    "title": "Midterm 1 Study Guide",
    "section": "Problem 10",
    "text": "Problem 10\n\nLet \\(S\\) be the set of all permutations of \\(a\\), \\(b\\), \\(c\\), together with the triples \\(aaa\\), \\(bbb\\), and \\(ccc\\). Imagine we draw an outcome randomly from this set. Define the event \\(A_k=\\{\\text{the kth spot is occupied by the letter a}\\}\\) for \\(k=1,\\,2,\\,3\\). Compute the probabilities \\(P(A_k)\\), \\(P(A_i\\cap A_j)\\), \\(P(A_1\\cap A_2\\cap A_3)\\), and comment on the independence of the three of events;\nImagine we roll two fair, six-sided die. Compute \\(P(A)\\), \\(P(B)\\), \\(P(C)\\), \\(P(A\\cap B)\\), \\(P(A\\cap C)\\), \\(P(B\\cap C)\\), and \\(P(A\\cap B\\cap C)\\) for the following three events and comment on their independence:\n\n\\[\n\\begin{aligned}\nA&=\\text{first die is 1, 2, 3}\\\\\nB&=\\text{first die is 3, 4, 5}\\\\\nC&=\\text{the sum of the two rolls is 9}.\n\\end{aligned}\n\\]",
    "crumbs": [
      "Exam Study Guides",
      "Midterm 1"
    ]
  },
  {
    "objectID": "problems/exam-midterm-1.html#want-more-review-2",
    "href": "problems/exam-midterm-1.html#want-more-review-2",
    "title": "Midterm 1 Study Guide",
    "section": "Want more review?",
    "text": "Want more review?\n\nLecture notes with worked examples;\n\nThe lil’ app on disease testing;\n\nProblem Set 2: problems 9 - 10;\nProblem Set 3: problems 4 - 8;\nProblem Set 4: problems 2 - 3;\nOdd-numbered exercises in DeGroot & Schervish Chapter 2.",
    "crumbs": [
      "Exam Study Guides",
      "Midterm 1"
    ]
  },
  {
    "objectID": "problems/exam-midterm-1.html#problem-11",
    "href": "problems/exam-midterm-1.html#problem-11",
    "title": "Midterm 1 Study Guide",
    "section": "Problem 11",
    "text": "Problem 11\nSignore Fibonacci has created a a balanced, six-sided die whose faces are labeled, naturally, 1, 2, 3, 5, 8, 13. He gives a pair to Enzo, who rolls them repeatedly, eager to get double 13s. Let \\(X\\) be the number of times he has to roll the pair of dice in order to see double 13s for the first time.\n\nWhat is the pmf of \\(X\\)?\nWhat is the expected value of \\(X\\)?\nFind a closed-form expression for \\(P(X\\geq m)\\).",
    "crumbs": [
      "Exam Study Guides",
      "Midterm 1"
    ]
  },
  {
    "objectID": "problems/exam-midterm-1.html#problem-12",
    "href": "problems/exam-midterm-1.html#problem-12",
    "title": "Midterm 1 Study Guide",
    "section": "Problem 12",
    "text": "Problem 12\nLet \\(D \\sim \\text{Geometric}(p)\\), and fix positive integers \\(n\\) and \\(k\\). What is \\(P(D = n + k \\,|\\, D &gt; k)\\)? Does this look familiar?",
    "crumbs": [
      "Exam Study Guides",
      "Midterm 1"
    ]
  },
  {
    "objectID": "problems/exam-midterm-1.html#want-more-review-3",
    "href": "problems/exam-midterm-1.html#want-more-review-3",
    "title": "Midterm 1 Study Guide",
    "section": "Want more review?",
    "text": "Want more review?\n\nLecture notes introducing random variables;\nLecture notes on special families of discrete random variables;\nLecture notes on expected value;\nLab 5 exercises;\nProblem Set 2: problem 7;\nProblem Set 3: problems 9 - 10;\nProblem Set 4: problems 4 - 10.",
    "crumbs": [
      "Exam Study Guides",
      "Midterm 1"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STA 240 Probability for Statistical Inference",
    "section": "",
    "text": "Below is a prospective outline for the course. Due dates are firm, but topics may change with advanced notice.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWEEK\nDATE\nPREPARE\nTOPIC\nMATERIALS\nDUE\n\n\n\n1\nMon, Aug 25\n\nDS: 1.1 - 1.2\n🧑‍🏫 Welcome!\nslides\n\n\n\n\nWed, Aug 27\n\nDS: 1.3 - 1.4\n🧑‍🏫 Set Theory\nnotes\n\n\n\n\nThu, Aug 28\n\n💻 Lab 0 (calculus review)\n\n\n\n\n2\nMon, Sep 1\n\n❌ Labor Day - No Lecture\n\n\n\n\n\n\nWed, Sep 3\n\nDS: 1.5\n🧑‍🏫 Probability Spaces\n\nslides notes\n\n\nIntro Survey @ 3 PM\n\n\n\nThu, Sep 4\n\n💻 Lab 1 (hello R)\n\n\nLab 1 @ 11:59 PM\n\n\n\nFri, Sep 5\n\n\n\n\nPSET 0 @ 5PM\n\n\n3\nMon, Sep 8\n\nDS: 1.6 - 1.8\n🧑‍🏫 Counting\nnotes\n\n\n\n\nWed, Sep 10\n\nDS: 1.6 - 1.8\n🧑‍🏫 Counting\nnotes\n\n\n\n\nThu, Sep 11\n\n💻 Lab 2 (counting)\n\n\nLab 2 @ 11:59 PM\n\n\n\nFri, Sep 12\n\n\n\n\nPSET 1 @ 5PM\n\n\n4\nMon, Sep 15\n\nDS: 2\n🧑‍🏫 Conditional Probability\n\nnotes Play around!\n\n\n\n\n\nWed, Sep 17\n\nDS: 2\n🧑‍🏫 Conditional Probability\n\nRead! Play around!\n\n\n\n\n\nThu, Sep 18\n\n💻 Lab 3 (disease testing)\nRead!\n\nLab 3 @ 11:59 PM\n\n\n\nFri, Sep 19\n\n\n\nPSET 0 corrections PSET 2 @ 5PM\n\n\n5\nMon, Sep 22\n\nDS: 3.1, 3.3, 4.1\n🧑‍🏫 Random variable intro\nnotes\n\n\n\n\nWed, Sep 24\n\nDS: 5.1 - 5.4\n🧑‍🏫 Discrete families\nnotes\n\n\n\n\nThu, Sep 25\n\n💻 Lab 4 (ChatGPT)\n\n\nLab 4 @ 11:59 PM\n\n\n\nFri, Sep 26\n\n\n\n\nPSET 3 @ 5PM\n\n\n6\nMon, Sep 29\n\nDS: 4.2\n🧑‍🏫 Expected value\nnotes\n\n\n\n\nWed, Oct 1\n\nDS: 4.3\n🧑‍🏫 Variance\nnotes\n\n\n\n\nThu, Oct 2\n\n💻 Lab 5 (random dating)\n\n\nLab 5 @ 11:59 PM\n\n\n\nSun, Oct 5\n\n\n\n\nPSET 4 @ 5PM\n\n\n7\nMon, Oct 6\n\nDS: 3.3\n🧑‍🏫 CDF\nnotes\n\n\n\n\nWed, Oct 8\n\nDS: 3.2\n🧑‍🏫 Absolutely continuous\nnotes\n\n\n\n\nThu, Oct 9\nstudy guide\n📝 Midterm 1\n\n\n\n\n\n8\nMon, Oct 13\n\n❌ Fall Break - No Lecture\n\n\n\n\n\n\nWed, Oct 15\n\nDS: 5.6 - 5.7\n🧑‍🏫 Normal, gamma\n\nnormal primer gamma primer\n\n\n\n\n\nThu, Oct 16\n\n💻 Lab 6 (normal and gamma)\n\n\nLab 6 @ 11:59 PM\n\n\n9\nMon, Oct 20\n\nDS: 4.4 - 4.5\n🧑‍🏫 Moments, quantiles\nnotes\n\n\n\n\nWed, Oct 22\n\nDS: 4.4 - 4.5\n🧑‍🏫 MGF\n\n\n\n\n\nThu, Oct 23\n\nDS: 3.8\n💻 Lab 7 (transformations)\n\n\nLab 7 @ 11:59 PM\n\n\n10\nMon, Oct 27\n\nDS: 3.4 - 3.7\n🧑‍🏫 Jointly discrete\nnotes\n\nPSET 5 @ 3PM\n\n\n\nWed, Oct 29\n\nDS: 3.4 - 3.7\n🧑‍🏫 Jointly continuous\nnotes\n\n\n\n\nThu, Oct 30\n\nDS: 12\n💻 Lab 8 (simulation)\n\n\nLab 8 @ 11:59 PM\n\n\n11\nMon, Nov 3\n\nDS: 6\n🧑‍🏫 Tower property, Student’s t\n\n\n\n\n\nWed, Nov 5\n\nDS: 6\n🧑‍🏫 Sums and averages\nnotes\n\n\n\n\nThu, Nov 6\n\n💻 Lab 9 (insurance)\n\n\nLab 9 @ 11:59 PM\n\n\n\nSun, Nov 9\n\n\n\n\nPSET 6 @ 5PM\n\n\n12\nMon, Nov 10\n\nDS: 6\n🧑‍🏫 Limit theorems\nnotes\n\n\n\n\nWed, Nov 12\n\nDS: 7.1\n🧑‍🏫 What is statistics?\n\n\n\n\n\nThu, Nov 13\nstudy guide\n📝 Midterm 2\n\n\n\n\n\n13\nMon, Nov 17\n\nDS: 7.1\n🧑‍🏫 What is statistics?\n\n\n\n\n\nWed, Nov 19\n\nDS: 7.5 - 7.6\n🧑‍🏫 Maximum likelihood\nnotes\n\n\n\n\nThu, Nov 20\n\n💻 Lab 10 (mle)\n\n\nLab 10 @ 11:59 PM\n\n\n14\nMon, Nov 24\n\nDS: 7.5 - 7.6\n🧑‍🏫 Maximum likelihood\nnotes\n\n\n\n\nWed, Nov 26\n\n❌ Thanksgiving - No Lecture\n\n\n\n\n\n\nThu, Nov 27\n\n❌ Thanksgiving - No Lab\n\n\n\n\n\n15\nMon, Dec 1\n\nDS: 7.2 - 7.4\n🧑‍🏫 Bayesian statistics\nnotes\n\n\n\n\nWed, Dec 3\n\nDS: 7.2 - 7.4\n🧑‍🏫 Bayesian statistics\n\n\n\n\n\nThu, Dec 4\n\n💻 Lab 11 (bayes)\n\n\nLab 11 @ 11:59 PM\n\n\n16\nMon, Dec 8\n\n\n\n\nPSET 7 @ 5PM\n\n\n\nFri, Dec 12\n\nstudy guide more stats help\n\n📝 Final (9AM - 12PM)\n\n\n\n\n\n\n\n\n\nExtra notes\nYou’re not responsible for any of this, but it’s extra stuff I had lying around that you might find interesting:\n\nAlternative notes on MLE, with more discussion of confidence intervals: set 1, set 2;\n“AP Statistics in 75 minutes.” After this class, we have all of the tools to derive the itchy formulas that we take for granted in AP Stats or STA 101.",
    "crumbs": [
      "Syllabus",
      "Schedule"
    ]
  },
  {
    "objectID": "math/series.html",
    "href": "math/series.html",
    "title": "Infinite series",
    "section": "",
    "text": "Roughly speaking, an infinite series is the sum of infinitely many numbers:\n\\[\n\\sum\\limits_{n=0}^\\infty a_n=a_0+a_1+a_2+a_3+\\cdots\n\\]\nStrictly speaking, it is the limit of the sequence of partial sums:\n\\[\n\\sum\\limits_{n=0}^\\infty a_n=\\lim_{k\\to\\infty}\\sum\\limits_{n=0}^ka_n.\n\\]\nThis limit may or may not be finite, and there are many famous examples of this.",
    "crumbs": [
      "Math Helpers",
      "Infinite series"
    ]
  },
  {
    "objectID": "math/series.html#geometric-series",
    "href": "math/series.html#geometric-series",
    "title": "Infinite series",
    "section": "Geometric series",
    "text": "Geometric series\nDon’t you love this:\n\\[\n\\frac{a}{1-r}=\\sum\\limits_{n=0}^\\infty ar^n,\\quad |r|&lt;1.\n\\]\nThe partial sums are also well-behaved:\n\\[\n\\sum\\limits_{n=0}^k ar^n\n=\n\\begin{cases}\na(k+1) & r=1\\\\\na\\left(\\frac{1-r^{k+1}}{1-r}\\right) & \\text{else}.\n\\end{cases}\n\\]",
    "crumbs": [
      "Math Helpers",
      "Infinite series"
    ]
  },
  {
    "objectID": "math/series.html#p-series",
    "href": "math/series.html#p-series",
    "title": "Infinite series",
    "section": "\\(p\\)-series",
    "text": "\\(p\\)-series\nIf \\(p&gt;1\\), this converges:\n\\[\n\\sum\\limits_{n=1}^\\infty\\frac{1}{n^p}.\n\\]\nHere are concrete examples:\n\\[\n\\begin{aligned}\n\\sum\\limits_{n=1}^\\infty\\frac{1}{n}&=\\infty &&\\text{(harmonic series)}\\\\\n\\sum\\limits_{n=1}^\\infty\\frac{1}{n^2}&=\\frac{\\pi^2}{6}\\\\\n\\sum\\limits_{n=1}^\\infty\\frac{1}{n^3}&\\approx 1.202057.\n\\end{aligned}\n\\]",
    "crumbs": [
      "Math Helpers",
      "Infinite series"
    ]
  },
  {
    "objectID": "math/series.html#taylor-series",
    "href": "math/series.html#taylor-series",
    "title": "Infinite series",
    "section": "Taylor series",
    "text": "Taylor series\nIf a function \\(f\\) is infinitely differentiable at 0, then we can rewrite it using the Taylor expansion:\n\\[\nf(x)\n=\n\\sum\\limits_{n=0}^\\infty \\frac{f^{(n)}(0)}{n!}x^n,\\quad \\text{for some  }x.\n\\]\nThe set of \\(x\\)-values for which the Taylor expansion is valid is called the radius of convergence. Here are examples for specific functions:\n\\[\n\\begin{aligned}\n\\frac{1}{1-x}&=\\sum\\limits_{n=0}^\\infty x^n,&&-1&lt;x&lt;1\\\\\ne^x&=\\sum\\limits_{n=0}^\\infty\\frac{x^n}{n!},&&-\\infty &lt; x &lt; \\infty\\\\\n\\ln(1-x)&=-\\sum\\limits_{n=1}^\\infty\\frac{x^n}{n},&&-1\\leq x &lt; 1.\n\\end{aligned}\n\\]\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 500\n\n# app.R\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  titlePanel(\"Taylor Polynomial Approximation\"),\n  \n  sidebarLayout(\n    sidebarPanel(\n      selectInput(\n        inputId = \"func\",\n        label = \"Choose a function:\",\n        choices = c(\"exp(x)\", \"sin(x)\")\n      ),\n      sliderInput(\n        inputId = \"degree\",\n        label = \"Degree of Taylor polynomial (n):\",\n        min = 0, max = 10, value = 1, step = 1\n      )\n    ),\n    \n    mainPanel(\n      plotOutput(\"taylorPlot\")\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  output$taylorPlot &lt;- renderPlot({\n    # Define function and y-limits\n    f &lt;- switch(input$func,\n                \"exp(x)\" = function(x) exp(x),\n                \"sin(x)\" = function(x) sin(x))\n    x &lt;- seq(-5, 5, length.out = 400)\n    y_true &lt;- f(x)\n    \n    # Fix y-limits by function\n    ylims &lt;- switch(input$func,\n                    \"exp(x)\" = c(-1, 150),\n                    \"sin(x)\" = c(-1.5, 1.5))\n    \n    # Define Taylor polynomial\n    taylor_poly &lt;- function(x, n) {\n      sapply(x, function(xi) {\n        sum(sapply(0:n, function(k) {\n          deriv_k &lt;- switch(input$func,\n                            \"exp(x)\" = 1,\n                            \"sin(x)\" = if (k %% 4 == 0) 0 else if (k %% 4 == 1) 1 else if (k %% 4 == 2) 0 else -1)\n          deriv_k * xi^k / factorial(k)\n        }))\n      })\n    }\n    \n    y_approx &lt;- taylor_poly(x, input$degree)\n    \n    # Draw the plot\n    plot(x, y_true, type = \"l\", col = \"black\", lwd = 2,\n         ylim = ylims, xlab = \"x\", ylab = \"y\",\n         main = paste(\"Taylor approximation of\", input$func, \"around x = 0\"),\n         bty = \"n\", axes = FALSE)\n    \n    # Add axes through origin\n    abline(h = 0, v = 0, col = \"gray70\")\n    axis(1, pos = 0)\n    axis(2, pos = 0, las = 1)\n    \n    # Add Taylor curve\n    lines(x, y_approx, col = \"red\", lwd = 2)\n    \n    # Add legend\n    legend(\"topleft\",\n           legend = c(\"f(x)\", paste(\"Taylor polynomial (n = \", input$degree, \")\", sep = \"\")),\n           col = c(\"black\", \"red\"), lwd = 2, bty = \"n\")\n  })\n}\n\nshinyApp(ui = ui, server = server)",
    "crumbs": [
      "Math Helpers",
      "Infinite series"
    ]
  },
  {
    "objectID": "math/gamma-function.html",
    "href": "math/gamma-function.html",
    "title": "The gamma function",
    "section": "",
    "text": "The gamma function is a continuous function that interpolates the factorials. It is an interesting mathematical object in its own right, but for students of probability, it appears in the normalizing constant of the gamma density, and recognizing the gamma function is a useful technique for simplifying certain integrals.\nDefinition\nHere it is:\n\\[\n\\Gamma(x)=\\int_0^\\infty z^{x-1}e^{-z}\\,\\text{d}z,\\quad x &gt;0.\n\\]\nPay attention. \\(x\\) is the argument to the function. It’s treated as a constant. \\(z\\) is the dummy variable of integration. It doesn’t affect the actual value of the expression, and you can change it to whatever you want: \\(y\\), \\(u\\), \\(t\\), whatever. At the end of the day, the value of the integral is just a number; plug in different \\(x\\) values, and you get different numbers. That’s the gamma function. Figure 1 displays the picture.\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\nR provides the gamma command for actually computing this:\n\ngamma(0.5) # √π\n\n[1] 1.772454\n\ngamma(1)   # 1\n\n[1] 1\n\ngamma(4)   # 3!\n\n[1] 6\n\ngamma(8)   # 7!\n\n[1] 5040\n\n\nProperties\nYou first met the gamma function on Problem Set 0, where you showed that it satisfies the following recursion:\n\\[\n\\Gamma(x+1) = x\\Gamma(x).\n\\]\nThat works for any real number \\(x&gt;0\\). But now let’s say you plug in an integer \\(n\\geq 2\\). As a result of the recursion, something cute happens:\n\\[\n\\begin{aligned}\n\\Gamma(n)&=\\Gamma(n-1+1)\\\\\n&=(n-1)\\Gamma(n-1)\\\\\n&=(n-1)(n-2)\\Gamma(n-2)\\\\\n&=(n-1)(n-2)(n-3)\\Gamma(n-3)\\\\\n&\\vdots\\\\\n&=(n-1)(n-2)(n-3)\\cdots 3\\cdot2\\cdot\\Gamma(2)\\\\\n&=(n-1)(n-2)(n-3)\\cdots 3\\cdot2\\cdot1\\cdot\\Gamma(1)\\\\\n&=(n-1)!\n\\end{aligned}\n\\]\nThe last line follows because \\(\\Gamma(1)=\\int_0^\\infty \\exp(-x)\\,\\textrm{d}x=1\\) (shown here, but make sure you can do it yourself). The punchline is that the gamma function is a continuous function that interpolates the factorials. That is, it’s a smooth curve that “connects the dots,” like you see in Figure 1.\nThe gamma function has many funky properties that have kept mathematicians busy for a long time. For instance, \\(\\Gamma(1/2)=\\sqrt{\\pi}\\), which you showed on Problem Set 5. If you’re interested, there’s more where that came from.",
    "crumbs": [
      "Math Helpers",
      "Gamma function"
    ]
  },
  {
    "objectID": "math/limits.html",
    "href": "math/limits.html",
    "title": "Limits and continuity",
    "section": "",
    "text": "The limit of a function \\(f(x)\\) at a point \\(c\\) is the value that the function approaches as \\(x\\) approaches \\(c\\). We denote this:\n\\[\n\\lim_{x\\to c}f(x).\n\\]\nIf the limit of the function and the actual value of the function agree, then the function is continuous:\n\\[\n\\lim_{x\\to c}f(x)=f(c)\n\\] If a function is continuous, that makes it especially convenient to evaluate limits because it means you can just plug-in and evaluate the darn function. But as Figure 1 illustrates, not all functions are continuous. Some functions have breaks and jumps and holes that make them discontinuous.\nBut not all discontinuities are created equal:",
    "crumbs": [
      "Math Helpers",
      "Limits and continuity"
    ]
  },
  {
    "objectID": "math/limits.html#limit-rules",
    "href": "math/limits.html#limit-rules",
    "title": "Limits and continuity",
    "section": "Limit rules",
    "text": "Limit rules\nAssume \\(\\lim_{x\\to c}f(x)\\) and \\(\\lim_{x\\to c}g(x)\\) exist, let \\(a,\\,b\\in\\mathbb{R}\\) be arbitrary constants, and let \\(r&gt;0\\). Then:\n\n\n\n\n\n\n\nrule\nformula\n\n\n\n\nscaling\n\\[\\lim_{x\\to c}[a\\cdot f(x)]=a\\lim_{x\\to c}f(x)\\]\n\n\nsum/difference\n\\[\\lim_{x\\to c}[f(x)\\pm g(x)]=\\lim_{x\\to c}f(x)\\pm\\lim_{x\\to c}g(x)\\]\n\n\nlinearity\n\\[\\lim_{x\\to c}[a\\cdot f(x)\\pm b\\cdot g(x)]=a\\lim_{x\\to c}f(x)\\pm b\\lim_{x\\to c}g(x)\\]\n\n\nproduct\n\\[\\lim_{x\\to c}f(x)g(x)=\\left[\\lim_{x\\to c}f(x)\\right]\\left[\\lim_{x\\to c}g(x)\\right]\\]\n\n\nquotient\n\\[\\lim_{x\\to c}\\frac{f(x)}{g(x)}=\\frac{\\lim_{x\\to c} f(x)}{\\lim_{x\\to c} g(x)},\\quad \\text{if }\\lim_{x\\to c} g(x)\\neq0\\]\n\n\npower\n\\[\\lim_{x\\to c}f(x)^r=\\left[\\lim_{x\\to c}f(x)\\right]^r,\\quad \\text{if }c&gt;0,\\, \\lim_{x\\to c} f(x)\\geq0\\]",
    "crumbs": [
      "Math Helpers",
      "Limits and continuity"
    ]
  },
  {
    "objectID": "math/limits.html#indeterminate-forms-and-lhôpitals-rule",
    "href": "math/limits.html#indeterminate-forms-and-lhôpitals-rule",
    "title": "Limits and continuity",
    "section": "Indeterminate forms and L’Hôpital’s rule",
    "text": "Indeterminate forms and L’Hôpital’s rule\nMost of the time, we hope that we can appeal to continuity and evaluate limits by simply “plugging-in.” But sometimes plugging in gives loony stuff:\n\\[\n\\begin{aligned}\n\\lim_{x\\to 0}\\frac{\\sin x}{x}=1, &\\text{ but }\\frac{\\sin(0)}{0}=\\frac{0}{0};\\\\\n\\lim_{x\\to \\infty}\\frac{\\ln x}{x}=0, &\\text{ but }\\frac{\\ln\\infty}{\\infty}=\\frac{\\infty}{\\infty}.\\\\\n\\end{aligned}\n\\]\nThings like \\(0/0\\), \\(\\infty/\\infty\\), and \\(0\\times\\infty\\) are examples of indeterminate forms. If you get one, it doesn’t necessarily mean that the limit does not exist. It just means that the jury is still out; it’s indeterminate. To continue investigating, you can use a tool called L’Hôpital’s rule. Whenever \\(\\lim_{x\\to c}f(x)=\\lim_{x\\to c}g(x)=0\\) or \\(\\pm\\infty\\), the rule says that\n\\[\n\\lim_{x\\to c}\\frac{f(x)}{g(x)}=\\lim_{x\\to c}\\frac{f'(x)}{g'(x)}.\n\\] The hope is that the new function \\(f'(x)/g'(x)\\) will be well-behaved at \\(c\\), meaning you can simplify by just plugging in. However, after applying L’Hôpital’s rule once and trying to plug-in, you may still get an indeterminate form. In that case, you’ll have to take another round of derivatives, and maybe another after that, until the clouds finally part and the sun shines upon you.\n\n\n\n\n\n\nExample: multiple rounds of L’Hôpital’s\n\n\n\nConsider:\n\\[\n\\lim_{x\\to\\infty}\\frac{(\\ln x)^2}{x}.\n\\]\n\\(\\lim_{x\\to\\infty}(\\ln x)^2=\\infty\\) and \\(\\lim_{x\\to\\infty}x=\\infty\\), so let’s take some derivatives:\n\\[\n\\begin{aligned}\n\\frac{\\text{d}}{\\text{d}x}(\\ln x)^2&=\\frac{2\\ln x}{x}\\\\\n\\frac{\\text{d}}{\\text{d}x}x&=1.\n\\end{aligned}\n\\] The ratio of derivatives is thus \\(2(\\ln x)/x\\), but this still gives an indeterminate form because \\(\\lim_{x\\to\\infty}2\\ln x=\\infty\\) and \\(\\lim_{x\\to\\infty}x=\\infty\\), so we have to go again:\n\\[\n\\begin{aligned}\n\\frac{\\text{d}}{\\text{d}x}2\\ln x&=\\frac{2}{x}\\\\\n\\frac{\\text{d}}{\\text{d}x}x&=1.\n\\end{aligned}\n\\] The ratio of these derivatives is just \\(2/x\\), and that goes to 0 as \\(x\\to \\infty\\). In sum,\n\\[\n\\lim_{x\\to\\infty}\\frac{(\\ln x)^2}{x}\n\\overset{\\text{L'H}}{=}\n\\lim_{x\\to\\infty}\\frac{2\\ln x}{x}\n\\overset{\\text{L'H}}{=}\n\\lim_{x\\to\\infty}\\frac{2}{x}\n=\n0.\n\\]",
    "crumbs": [
      "Math Helpers",
      "Limits and continuity"
    ]
  },
  {
    "objectID": "math/integration.html",
    "href": "math/integration.html",
    "title": "Integration",
    "section": "",
    "text": "Crudely, the definite integral of a function gives you the (signed) area between the graph of the function and the \\(x\\)-axis.\nWhen it exists, which it doesn’t always, the definite integral can be defined as the limit of Riemann sums:\n\\[\n\\int_a^b f(x)\\,\\text{d}x=\\lim_{\\Delta x\\to 0}\\sum\\limits_{i=1}^n f(x_i^*)\\Delta x.\n\\]\nJust as the derivative is the continuous analog to a discrete difference, the integral is the continuous analog to a discrete sum.",
    "crumbs": [
      "Math Helpers",
      "Integration"
    ]
  },
  {
    "objectID": "math/integration.html#integral-rules",
    "href": "math/integration.html#integral-rules",
    "title": "Integration",
    "section": "Integral rules",
    "text": "Integral rules\n\n\n\n\n\n\nrule\nformula\n\n\n\ndummy variable\n\\(\\int_a^bf(x)\\,\\text{d}x=\\int_a^bf(t)\\,\\text{d}t=\\int_a^bf(u)\\,\\text{d}u=\\cdots\\)\n\n\nopposite\n\\(\\int_a^bf(x)\\,\\text{d}x=-\\int_b^af(x)\\,\\text{d}x\\)\n\n\nscaling\n\\(\\int_a^bcf(x)\\,\\text{d}x=c\\int_a^bf(x)\\,\\text{d}x\\)\n\n\nsum/difference\n\\(\\int_a^b[f(x)\\pm g(x)]\\text{d}x=\\int_a^bf(x)\\,\\text{d}x \\pm\\int_a^bg(x)\\text{d}x\\)\n\n\nlinearity\n\\(\\int_a^b[p\\cdot f(x)\\pm q\\cdot g(x)]\\text{d}x=p\\int_a^bf(x)\\,\\text{d}x \\pm q\\int_a^bg(x)\\text{d}x\\)\n\n\nsplit\n\\(\\int_a^bf(x)\\,\\text{d}x=\\int_a^cf(x)\\,\\text{d}x+\\int_c^bf(x)\\,\\text{d}x\\)",
    "crumbs": [
      "Math Helpers",
      "Integration"
    ]
  },
  {
    "objectID": "math/integration.html#the-fundamental-theorem-of-calculus",
    "href": "math/integration.html#the-fundamental-theorem-of-calculus",
    "title": "Integration",
    "section": "The fundamental theorem of calculus",
    "text": "The fundamental theorem of calculus\nThe fundamental theorem of calculus is a bridge between the seemingly unrelated worlds of derivatives and integrals:\n\\[\n\\int_a^bf(x)\\,\\text{d}x=F(b)-F(a),\\quad f(x)=F'(x).\n\\]\nOn a practical level, it provides a route for evaluating definite integrals by antidifferentiation. The function you are trying to integrate (called the integrand) is \\(f\\). You imagine \\(f\\) is the derivative of some function \\(F\\) which you don’t yet know. If you can “undo” the differentiation and figure out what \\(F\\) is, then you can evaluate the integral.\n\n\n\n\n\n\nExample: antidifferentiation\n\n\n\nConsider \\[\n\\int_1^2\\frac{1}{x^2}\\,\\text{d}x.\n\\] What function, when you take its derivative, gives you \\(1/x^2=x^{-2}\\)? To answer that, you’re essentially undoing the power rule. After a while, you realize \\[\n\\frac{\\text{d}}{\\text{d}x}\\left(-\\frac{1}{x}\\right)=\\frac{1}{x^2},\n\\] and so\n\\[\n\\int_1^2\\frac{1}{x^2}\\,\\text{d}x=\\left[-\\frac{1}{x}\\right]_1^2=-\\frac{1}{2}-\\left(-\\frac{1}{1}\\right)=\\frac{1}{2}.\n\\]",
    "crumbs": [
      "Math Helpers",
      "Integration"
    ]
  },
  {
    "objectID": "math/integration.html#integration-by-substitution",
    "href": "math/integration.html#integration-by-substitution",
    "title": "Integration",
    "section": "Integration by substitution",
    "text": "Integration by substitution\nWhen you perform a \\(u\\)-substitution, you’re basically trying to undo the chain rule:\n\\[\n\\int_a^bf(g(x))g'(x)\\,\\text{d}x=\\int_{g(a)}^{g(b)}f(u)\\,\\text{d}u.\n\\]\nThe goal is ultimately to rewrite the integral in a nicer form so that you can recognize what the appropriate antiderivative is and then apply the FTOC.\n\n\n\n\n\n\nExample: \\(u\\)-sub\n\n\n\nCheck out this ugly thing:\n\\[\n\\int_0^2\n\\frac{x}{\\sqrt{x^2+1}}\\,\\text{d}x.\n\\]\nIt’s actually not too bad if you make the right substitution: \\(u=x^2+1\\) and \\(\\text{d}u=2x\\,\\text{d}x\\). Then you can polish it off by undoing the power rule:\n\\[\n\\begin{aligned}\n\\int_0^2\n\\frac{x}{\\sqrt{x^2+1}}\\,\\text{d}x\n&=\n\\frac{1}{2}\\int_1^5\\frac{\\text{d}u}{\\sqrt{u}}\n\\\\\n&=\n\\frac{1}{2}\\left[2u^{1/2}\\right]_1^5\n\\\\\n&=\n\\frac{1}{2}\n\\left(2\\sqrt{5}-2\\sqrt{1}\\right)\n\\\\\n&=\n\\sqrt{5}-1.\n\\end{aligned}\n\\]\nPlease please please do not forget to change the bounds of the integral when you do a substitution. We had \\(u=g(x)=x^2+1\\), so after the change of variables, the lower bound becomes \\(g(0)=1\\) and the upper bound becomes \\(g(2)=5\\).",
    "crumbs": [
      "Math Helpers",
      "Integration"
    ]
  },
  {
    "objectID": "math/integration.html#integration-by-parts",
    "href": "math/integration.html#integration-by-parts",
    "title": "Integration",
    "section": "Integration by parts",
    "text": "Integration by parts\nLet’s face it; this sucks. But sometimes you have to integrate a funky product, and there’s no way around it, so you do this:\n\\[\n\\int_a^b u(x) v'(x) \\, \\text{d}x\n    = \\left[u(x) v(x)\\right]_a^b - \\int_a^b v(x)u'(x)  \\, \\text{d}x\n\\]\nThrough the subtlest of black magic, you have to decide what parts of the integrand to treat as \\(u\\) and \\(\\text{d}v=v'\\,\\text{d}x\\). Typically you make \\(u\\) something that is easy to differentiate, and you make \\(\\text{d}v\\) something that is easy to antidifferentiate. Good luck!\n\n\n\n\n\n\nExample: IBP\n\n\n\nConsider this integral:\n\\[\n\\int_1^2\\frac{\\ln(x)}{x^2}\\text{d}x\n\\] \\(\\ln(x)\\) is easier to differentiate, and \\(1/x^2\\) is easier to antidifferentiate, which suggests the following choice:\n\\[\n\\begin{aligned}\nu&=\\ln(x)\\\\\n\\text{d}u&=\\frac{\\text{d}x}{x}\\\\\nv&=-x^{-1}\\\\\n\\text{d}v&=x^{-2}\\,\\text{d}x.\n\\end{aligned}\n\\]\nLet the fun begin:\n\\[\n\\begin{aligned}\n\\int_1^2\\frac{\\ln(x)}{x^2}\\text{d}x\n&=\n\\left[-\\frac{\\ln(x)}{x}\\right]_1^2\n+\n\\int_1^2\\frac{\\text{d}x}{x^2}\n\\\\\n&=\n\\left[-\\frac{\\ln(x)}{x}\\right]_1^2\n+\n\\left[-x^{-1}\\right]_1^2\n\\\\\n&=\n\\left[-\\frac{\\ln(2)}{2}-\\left(-\\frac{\\ln(1)}{1}\\right)\\right]\n+\n\\left[-\\frac{1}{2}-\\left(-\\frac{1}{1}\\right)\\right]\n\\\\\n&=\n-\\frac{\\ln(2)}{2} + \\frac{1}{2}.\n\\end{aligned}\n\\]",
    "crumbs": [
      "Math Helpers",
      "Integration"
    ]
  },
  {
    "objectID": "math/integration.html#improper-integration-we-will-do-this-a-lot",
    "href": "math/integration.html#improper-integration-we-will-do-this-a-lot",
    "title": "Integration",
    "section": "Improper integration (we will do this a lot!)",
    "text": "Improper integration (we will do this a lot!)\nAn improper integral is the limit of a definite integral as one or more of the bounds tend toward some extreme value, typically \\(\\pm\\infty\\). So a one-sided improper integral is defined like this:\n\\[\n\\int_a^\\infty f(x)\\,\\text{d}x=\\lim_{b\\to\\infty}\\int_a^bf(x)\\,\\text{d}x.\n\\]\nA two-sided improper integral is defined as the sum of two one-sided improper integrals, where we make use of the “splitting” property:\n\\[\n\\begin{aligned}\n\\int_{-\\infty}^\\infty f(x)\\,\\text{d}x\n&=\n\\int_{-\\infty}^c f(x)\\,\\text{d}x + \\int_c^\\infty f(x)\\,\\text{d}x\n\\\\\n&=\n\\lim_{a\\to-\\infty}\\int_a^cf(x)\\,\\text{d}x+\\lim_{b\\to\\infty}\\int_c^bf(x)\\,\\text{d}x.\n\\end{aligned}\n\\]\nYou can pick whatever value of \\(c\\) is most convenient. Zero will be a common choice.\n\n\n\n\n\n\nExample: improper integral with infinite bound\n\n\n\n\\[\n\\begin{aligned}\n\\int_0^\\infty e^{-x}\\,\\text{d}x\n&=\n\\lim_{b\\to\\infty}\\int_0^be^{-x}\\,\\text{d}x\n\\\\\n&=\n\\lim_{b\\to\\infty}\n[-e^{-x}]_0^b\n\\\\\n&=\n\\lim_{b\\to\\infty}\n\\left(-e^{-b}-(-e^{-0})\\right)\n\\\\\n&=\n-\\lim_{b\\to\\infty}\\frac{1}{e^b}+1\n\\\\\n&=\n-0+1\n\\\\\n&=1.\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\nExample: improper integral with finite bound\n\n\n\nYou still might need improper integration, even for an ordinary-looking thing like this:\n\\[\n\\int_0^1\\frac{1}{\\sqrt{x}}\\text{d}x.\n\\]\nYou can see why if you plot the integrand:\n\n\n\n\n\n\n\n\nThere’s a vertical asymptote at \\(x=0\\), so this is also an improper integral:\n\\[\n\\begin{aligned}\n\\int_0^1\\frac{1}{\\sqrt{x}}\\text{d}x\n&=\n\\lim_{a\\to 0}\\int_a^1\\frac{1}{\\sqrt{x}}\\text{d}x\n\\\\\n&=\n\\lim_{a\\to 0}\\left[2x^{1/2}\\right]_a^1\n\\\\\n&=\n\\lim_{a\\to 0}\\left[2\\cdot 1^{1/2}-2a^{1/2}\\right]\n\\\\\n&=2-2\\cdot0^{1/2}\n\\\\\n&=\n2.\n\\end{aligned}\n\\]\n\\(2x^{1/2}\\) is continuous at 0, so to evaluate the limit you can just plug-in.",
    "crumbs": [
      "Math Helpers",
      "Integration"
    ]
  },
  {
    "objectID": "slides/2025-08-25-welcome.html#welcome-to-sta-240",
    "href": "slides/2025-08-25-welcome.html#welcome-to-sta-240",
    "title": "Welcome to STA 240!",
    "section": "Welcome to STA 240!",
    "text": "Welcome to STA 240!\n\n\nWhile you wait, please complete this brief questionnaire:"
  },
  {
    "objectID": "slides/2025-08-25-welcome.html#teaching-team",
    "href": "slides/2025-08-25-welcome.html#teaching-team",
    "title": "Welcome to STA 240!",
    "section": "Teaching Team",
    "text": "Teaching Team\n\n\n\n\n\n\n\n\nMug\nName\nRole\nOffice Hours\n\n\n\n\nHu, Yuang\nTA\nMon 7:30 PM - 9:30 PM\n\n\n\nLiu, Aurora\nHead TA\nWeTh 4:30 pm - 5:30 pm\n\n\n\nMa, Liane\nTA\nSun 10:00 am - 12:00 pm\n\n\n\nZito, John\nInstructor\nTue 3:00 pm - 6:00 pm"
  },
  {
    "objectID": "slides/2025-08-25-welcome.html#statistical-science-majors-union",
    "href": "slides/2025-08-25-welcome.html#statistical-science-majors-union",
    "title": "Welcome to STA 240!",
    "section": "Statistical Science Majors Union",
    "text": "Statistical Science Majors Union\n\n\n\n\nWelcome First-Years Event!\n\n9/4 at 7 PM\nEast Duke 108\n\nGBM #1 & Research Panel\n\n9/18 7 PM\nOld Chem 116\n\n\n\n\n\n\n\n    listserv\n\n\n\n\n\n    DukeGroups"
  },
  {
    "objectID": "slides/2025-08-25-welcome.html#example-the-birthday-problem",
    "href": "slides/2025-08-25-welcome.html#example-the-birthday-problem",
    "title": "Welcome to STA 240!",
    "section": "Example: the birthday problem",
    "text": "Example: the birthday problem\n\\(k\\) people convene for a birthday party:\n\nWhat is the probability that at least two of the attendees share a birthday?\nHow many people need to show up to the party for there to be a 50% chance of at least one match?\n\n\nMost people guess that you need, like, a lot."
  },
  {
    "objectID": "slides/2025-08-25-welcome.html#your-guesses",
    "href": "slides/2025-08-25-welcome.html#your-guesses",
    "title": "Welcome to STA 240!",
    "section": "Your guesses",
    "text": "Your guesses"
  },
  {
    "objectID": "slides/2025-08-25-welcome.html#example-the-birthday-problem-1",
    "href": "slides/2025-08-25-welcome.html#example-the-birthday-problem-1",
    "title": "Welcome to STA 240!",
    "section": "Example: the birthday problem",
    "text": "Example: the birthday problem\n\n\nno. of attendees (k)\nProb(at least one bday match)\n\n\n\n1\n0%\n\n\n4\n1.6%\n\n\n16\n28%\n\n\n23\n50.7%\n\n\n40\n89%\n\n\n56\n98%\n\n\n60\n99.4\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n366\n100.0%\n\n\n\nKey words: binomial coefficient, pigeonhole principle"
  },
  {
    "objectID": "slides/2025-08-25-welcome.html#example-the-birthday-problem-2",
    "href": "slides/2025-08-25-welcome.html#example-the-birthday-problem-2",
    "title": "Welcome to STA 240!",
    "section": "Example: the birthday problem",
    "text": "Example: the birthday problem"
  },
  {
    "objectID": "slides/2025-08-25-welcome.html#are-all-birthdays-in-fact-equally-likely",
    "href": "slides/2025-08-25-welcome.html#are-all-birthdays-in-fact-equally-likely",
    "title": "Welcome to STA 240!",
    "section": "Are all birthday’s in fact equally likely?",
    "text": "Are all birthday’s in fact equally likely?\nWe will assume this when we do the calculation. But actually…"
  },
  {
    "objectID": "slides/2025-08-25-welcome.html#any-matches-today",
    "href": "slides/2025-08-25-welcome.html#any-matches-today",
    "title": "Welcome to STA 240!",
    "section": "Any matches today?",
    "text": "Any matches today?\n\n\n\n\n\nbirthday\ncount\n\n\n\n05-25\n2\n\n\n01-02\n1\n\n\n01-03\n1\n\n\n01-05\n1\n\n\n01-06\n1\n\n\n01-11\n1\n\n\n01-25\n1\n\n\n02-02\n1\n\n\n02-11\n1\n\n\n03-01\n1\n\n\n03-02\n1\n\n\n03-31\n1\n\n\n04-03\n1\n\n\n04-11\n1\n\n\n04-21\n1\n\n\n05-09\n1\n\n\n05-15\n1\n\n\n05-30\n1\n\n\n06-08\n1\n\n\n06-20\n1\n\n\n06-25\n1\n\n\n07-06\n1\n\n\n07-08\n1\n\n\n08-30\n1\n\n\n09-06\n1\n\n\n09-22\n1\n\n\n09-30\n1\n\n\n11-01\n1\n\n\n11-07\n1\n\n\n11-10\n1\n\n\n11-21\n1\n\n\n11-23\n1\n\n\n12-01\n1\n\n\n12-11\n1"
  },
  {
    "objectID": "slides/2025-08-25-welcome.html#example-the-monty-hall-problem",
    "href": "slides/2025-08-25-welcome.html#example-the-monty-hall-problem",
    "title": "Welcome to STA 240!",
    "section": "Example: the Monty Hall problem",
    "text": "Example: the Monty Hall problem\nLet’s play: https://montyhall.io/\n\n\n\n\n\n\n\n\n\n\n\nVery counterintuitive\n\n\nMost people start out thinking that the two doors are equally likely to contain the prize, so switching doesn’t matter. In fact, you have a 2/3 chance of winning if you switch."
  },
  {
    "objectID": "slides/2025-08-25-welcome.html#you-people",
    "href": "slides/2025-08-25-welcome.html#you-people",
    "title": "Welcome to STA 240!",
    "section": "You people",
    "text": "You people"
  },
  {
    "objectID": "slides/2025-08-25-welcome.html#example-which-is-truly-random",
    "href": "slides/2025-08-25-welcome.html#example-which-is-truly-random",
    "title": "Welcome to STA 240!",
    "section": "Example: which is “truly” random?",
    "text": "Example: which is “truly” random?\n\n\n\n\n\n\n\n\n\n27 of you said Left is pure random;\n8 of you said Right is pure random."
  },
  {
    "objectID": "slides/2025-08-25-welcome.html#example-which-is-truly-random-1",
    "href": "slides/2025-08-25-welcome.html#example-which-is-truly-random-1",
    "title": "Welcome to STA 240!",
    "section": "Example: which is “truly” random?",
    "text": "Example: which is “truly” random?\n\n\n\n\n\n\n\n\n\nLeft: uniform random scatter;\nRight: points manipulated to “repel” each other (no clumps)."
  },
  {
    "objectID": "slides/2025-08-25-welcome.html#this-is-why-spotify-shuffle-stinks",
    "href": "slides/2025-08-25-welcome.html#this-is-why-spotify-shuffle-stinks",
    "title": "Welcome to STA 240!",
    "section": "This is why Spotify shuffle stinks",
    "text": "This is why Spotify shuffle stinks\n\n\n\nIt used to be a pure random shuffle;\nThen people complained that it didn’t feel random to them:\n\nstreaks of the same artist in a row;\nclumps of the same genre, etc;\n\n\nNow they do lord-only-knows-what."
  },
  {
    "objectID": "slides/2025-08-25-welcome.html#example-the-wisdom-of-crowds",
    "href": "slides/2025-08-25-welcome.html#example-the-wisdom-of-crowds",
    "title": "Welcome to STA 240!",
    "section": "Example: the wisdom of crowds",
    "text": "Example: the wisdom of crowds\n\n\n\n\nAt a 1906 country fair in Plymouth, 800 people participated in a contest to estimate the weight of an ox. Francis Galton observed that the median guess, 1207 lbs, was accurate within 1% of the true weight of 1198 lbs.\n\n\n\n\n\n\n\n\nLesson\n\n\nThe aggregation of many imperfect estimates/guesses is often better than a needle-in-haystack search for the “best” individual guess.\n\n\n\n\n\n\n\n\n\n\n\nIsn’t that obvious?\n\n\nIt took humans a long time to realize this. The first recorded uses of an “average” were during Isaac Newton’s lifetime (see Stigler’s Seven Pillars of Statistical Wisdom)."
  },
  {
    "objectID": "slides/2025-08-25-welcome.html#guess-the-weight",
    "href": "slides/2025-08-25-welcome.html#guess-the-weight",
    "title": "Welcome to STA 240!",
    "section": "Guess the weight",
    "text": "Guess the weight"
  },
  {
    "objectID": "slides/2025-08-25-welcome.html#guess-the-price",
    "href": "slides/2025-08-25-welcome.html#guess-the-price",
    "title": "Welcome to STA 240!",
    "section": "Guess the price",
    "text": "Guess the price"
  },
  {
    "objectID": "slides/2025-08-25-welcome.html#guess-the-age",
    "href": "slides/2025-08-25-welcome.html#guess-the-age",
    "title": "Welcome to STA 240!",
    "section": "Guess the age",
    "text": "Guess the age"
  },
  {
    "objectID": "slides/2025-08-25-welcome.html#the-modern-day-wisdom-of-crowds",
    "href": "slides/2025-08-25-welcome.html#the-modern-day-wisdom-of-crowds",
    "title": "Welcome to STA 240!",
    "section": "The modern-day wisdom of crowds?",
    "text": "The modern-day wisdom of crowds?\n\n\n\n\n\n\nPolymarket is having a moment.\n\n\n\n(source: CNN)"
  },
  {
    "objectID": "slides/2025-08-25-welcome.html#example-the-folly-of-doctors",
    "href": "slides/2025-08-25-welcome.html#example-the-folly-of-doctors",
    "title": "Welcome to STA 240!",
    "section": "Example: the folly of doctors",
    "text": "Example: the folly of doctors\n\nA 50-year-old, asymptomatic woman tests positive for breast cancer. Alarming, but no diagnostic test is perfect. If the prevalence of breast cancer in the population is 1%, if the true positive rate of the test is 90%, and if the false positive rate is 9%, what is the chance that the woman actually has cancer, given that she tested positive?\n\n\nYou will know how to answer this in four weeks. Doctors though…\n\n\n\n\n\n\n\n\nBBC News 2014\n\n\nOnly 34 out of 160 surveyed gynecologist got it right (9%). Almost half of them said 90%.\n“We can only imagine how much anxiety those innumerate doctors instil in women.”"
  },
  {
    "objectID": "slides/2025-08-25-welcome.html#sally-clark-1964---2007",
    "href": "slides/2025-08-25-welcome.html#sally-clark-1964---2007",
    "title": "Welcome to STA 240!",
    "section": "Sally Clark (1964 - 2007)",
    "text": "Sally Clark (1964 - 2007)\n\n\n\n\n\nEnglish solicitor;\n1996: first son died in bed at 11 weeks;\n1998: second son died in bed at 8 weeks;\nClark arrested on suspicion of murder;\nDefense claimed it was SIDS (sudden infant death syndrome)."
  },
  {
    "objectID": "slides/2025-08-25-welcome.html#sir-roy-meadow-1933--",
    "href": "slides/2025-08-25-welcome.html#sir-roy-meadow-1933--",
    "title": "Welcome to STA 240!",
    "section": "Sir Roy Meadow (1933 - )",
    "text": "Sir Roy Meadow (1933 - )\n\n\n\nPediatrician specializing in “Munchausen syndrome by proxy;”\nGave erroneous “expert” testimony that there is a “1 in 73,000,000” chance of two SIDS deaths in the same family;\n\nfirst, that’s not true;\nsecond, so what?\n\n\n1999: Clark convicted."
  },
  {
    "objectID": "slides/2025-08-25-welcome.html#a-sad-story",
    "href": "slides/2025-08-25-welcome.html#a-sad-story",
    "title": "Welcome to STA 240!",
    "section": "A sad story",
    "text": "A sad story\n\n\n\n\n\nStats community loudly panned Meadow’s testimony;\n2003: conviction overturned on appeal after revelation that exculpatory evidence was suppressed;\n2007: Clark drinks herself to death."
  },
  {
    "objectID": "slides/2025-08-25-welcome.html#reason-1-its-necessary.-1",
    "href": "slides/2025-08-25-welcome.html#reason-1-its-necessary.-1",
    "title": "Welcome to STA 240!",
    "section": "Reason 1: It’s necessary.",
    "text": "Reason 1: It’s necessary.\n\n\n\n\nHuman intuition and “common sense” about probability and statistics are often just flat out wrong, in silly and dangerous ways. Mere mortals require the scaffolding of mathematics to discipline our thinking."
  },
  {
    "objectID": "slides/2025-08-25-welcome.html#new-title-the-unreasonable-effectiveness-of-probability-ineverything",
    "href": "slides/2025-08-25-welcome.html#new-title-the-unreasonable-effectiveness-of-probability-ineverything",
    "title": "Welcome to STA 240!",
    "section": "New title: The Unreasonable Effectiveness of Probability in…Everything?",
    "text": "New title: The Unreasonable Effectiveness of Probability in…Everything?\n\n\n\n\n\n\nRead it!"
  },
  {
    "objectID": "slides/2025-08-25-welcome.html#example-llms",
    "href": "slides/2025-08-25-welcome.html#example-llms",
    "title": "Welcome to STA 240!",
    "section": "Example: LLMs",
    "text": "Example: LLMs\n\n\n\n\n\n\n\nQuestion\n\n\nWhat method does ChatGPT use to generate the next word in one of its responses?\n\n\n\n\n\nLet’s ask!\nhttps://chatgpt.com/share/677ef289-9564-8008-a662-9f8c2cfb04fc"
  },
  {
    "objectID": "slides/2025-08-25-welcome.html#example-the-mathematics-of-insurance",
    "href": "slides/2025-08-25-welcome.html#example-the-mathematics-of-insurance",
    "title": "Welcome to STA 240!",
    "section": "Example: the mathematics of insurance",
    "text": "Example: the mathematics of insurance\n\n\n\n\n\nWhen an insurance company sells you a policy, they are making a bet that you won’t need it;\nIf all policyholders make a claim, they’re ruined;\nGiven so much uncertainty, who should they sell to, and at what price, to ensure they don’t go broke?\nAsk an actuary! Licensed professionals with expertise in probability, statistics, and risk management."
  },
  {
    "objectID": "slides/2025-08-25-welcome.html#example-the-mathematics-of-insurance-1",
    "href": "slides/2025-08-25-welcome.html#example-the-mathematics-of-insurance-1",
    "title": "Welcome to STA 240!",
    "section": "Example: the mathematics of insurance",
    "text": "Example: the mathematics of insurance\n\n\n\nNurse practitioner\nIT Manager\nPhysician Assistant\nFinancial Manager\nSoftware Developer\nInformation Security Analyst\nHealth Services Manager\nData Scientist\nSpeech-Language Pathologist\nActuary\n\n\n\n\n\n\n\nFuture prospects (30%)\nWage potential (25%)\nEmployment (20%)\nJob safety/stability (15%)\nWork-life balance (10%)"
  },
  {
    "objectID": "slides/2025-08-25-welcome.html#probably-isnt-boring-these-days",
    "href": "slides/2025-08-25-welcome.html#probably-isnt-boring-these-days",
    "title": "Welcome to STA 240!",
    "section": "Probably isn’t boring these days",
    "text": "Probably isn’t boring these days\n\n\n\n\n\n\nPlenty of complex, challenging, high-impact issues:"
  },
  {
    "objectID": "slides/2025-08-25-welcome.html#example-whence-evolution",
    "href": "slides/2025-08-25-welcome.html#example-whence-evolution",
    "title": "Welcome to STA 240!",
    "section": "Example: whence evolution?",
    "text": "Example: whence evolution?\n\n\nHella simplified:\n\n\nMutation: random errors during DNA replication and repair;\n\nGenetic drift: chance events shift allele frequencies in the population;\n\nNatural selection: randomness generates options; selection chooses among them, producing evolution."
  },
  {
    "objectID": "slides/2025-08-25-welcome.html#example-quantum-mechanics",
    "href": "slides/2025-08-25-welcome.html#example-quantum-mechanics",
    "title": "Welcome to STA 240!",
    "section": "Example: quantum mechanics",
    "text": "Example: quantum mechanics\nkey words: Schrödinger’s cat, uncertainty principle, etc\n\n\n\n\n\nProbability is fundamentally braided into the fabric of reality;\nParticles at the (sub)atomic level do not have a definite position and momentum until measured;\nThere are only probabilities associated with the possible outcomes;\nThese ideas are hard, but some of the math will be accessible to us."
  },
  {
    "objectID": "slides/2025-08-25-welcome.html#reason-2-its-useful.-1",
    "href": "slides/2025-08-25-welcome.html#reason-2-its-useful.-1",
    "title": "Welcome to STA 240!",
    "section": "Reason 2: It’s useful.",
    "text": "Reason 2: It’s useful.\n\n\n\n\nNo area of science or technology can be properly understood without knowing something about probability. Period."
  },
  {
    "objectID": "slides/2025-08-25-welcome.html#two-old-hags",
    "href": "slides/2025-08-25-welcome.html#two-old-hags",
    "title": "Welcome to STA 240!",
    "section": "Two old hags",
    "text": "Two old hags\n\n\n\n\n\n\n\nBertrand Russell, Mysticism and Logic and Other Essays (1917)\n\n\n\n\n\n\n“Mathematics, rightly viewed, possesses not only truth, but supreme beauty cold and austere, like that of sculpture, without appeal to any part of our weaker nature, without the gorgeous trappings of painting or music, yet sublimely pure, and capable of a stern perfection such as only the greatest art can show.”\n\n\n\n\n\n\n\n\n\n\n\n\nAlbert Einstein, “The Late Emmy Noether” (1935)\n\n\n\n\n\n\n“Pure mathematics is, in its way, the poetry of logical ideas.”"
  },
  {
    "objectID": "slides/2025-08-25-welcome.html#this-is-a-theorem-we-will-prove",
    "href": "slides/2025-08-25-welcome.html#this-is-a-theorem-we-will-prove",
    "title": "Welcome to STA 240!",
    "section": "This is a theorem we will prove:",
    "text": "This is a theorem we will prove:"
  },
  {
    "objectID": "slides/2025-08-25-welcome.html#reason-3-its-beautiful.-1",
    "href": "slides/2025-08-25-welcome.html#reason-3-its-beautiful.-1",
    "title": "Welcome to STA 240!",
    "section": "Reason 3: It’s beautiful.",
    "text": "Reason 3: It’s beautiful.\n\n\n\n\nOstensibly random behavior can nevertheless be quite patternful, in ways that we can actually penetrate with elegant mathematics."
  },
  {
    "objectID": "slides/2025-08-25-welcome.html#why-study-mathematical-probstat",
    "href": "slides/2025-08-25-welcome.html#why-study-mathematical-probstat",
    "title": "Welcome to STA 240!",
    "section": "Why study mathematical probstat?",
    "text": "Why study mathematical probstat?\n\n\nIt’s necessary.\n\nHumans suck at thinking about this stuff.\n\n\n\nIt’s useful.\n\nIt makes studying other things so much easier.\n\n\n\nIt’s beautiful.\n\nThis is the main reason, honestly.\n\n\n\n\nSound good?"
  },
  {
    "objectID": "slides/2025-08-25-welcome.html#bookmark-the-course-page",
    "href": "slides/2025-08-25-welcome.html#bookmark-the-course-page",
    "title": "Welcome to STA 240!",
    "section": "Bookmark the course page!",
    "text": "Bookmark the course page!\n\n\n\n\n\nhttps://sta240-f25.github.io/"
  },
  {
    "objectID": "slides/2025-08-25-welcome.html#final-grade-breakdown",
    "href": "slides/2025-08-25-welcome.html#final-grade-breakdown",
    "title": "Welcome to STA 240!",
    "section": "Final grade breakdown",
    "text": "Final grade breakdown\nYour final course grade will be calculated as follows:\n\n\nCategory\nPercentage\n\n\n\nLabs\n10%\n\n\nProblem Sets\n30%\n\n\nMidterm Exam 1\n20%\n\n\nMidterm Exam 2\n20%\n\n\nFinal exam\n20%\n\n\n\n\n\n\n\n\n\nWarning\n\n\nThe final letter grade will be based on the usual thresholds, which will not change and will be applied exactly. So no curve and no rounding."
  },
  {
    "objectID": "slides/2025-08-25-welcome.html#sowheres-the-wiggle-room",
    "href": "slides/2025-08-25-welcome.html#sowheres-the-wiggle-room",
    "title": "Welcome to STA 240!",
    "section": "So…where’s the wiggle room?",
    "text": "So…where’s the wiggle room?\n\nWe drop the two lowest labs;\nWe drop the lowest problem set;\nWe will replace your lowest midterm score with your final exam score (if it’s better)."
  },
  {
    "objectID": "slides/2025-08-25-welcome.html#labs-10",
    "href": "slides/2025-08-25-welcome.html#labs-10",
    "title": "Welcome to STA 240!",
    "section": "Labs (10%)",
    "text": "Labs (10%)\nLead by Aurora in Perkins LINK 087 (Classroom 3):\n\nThursday 1:25 PM - 2:40 PM;\nThursday 3:05 PM - 4:20 PM.\n\nGuided activities introducing you to special topics, extensions, applications, and case studies. We will also introduce some basic R stuff, and we will use Quarto for the lab write-ups.\n\n\n\n\n\n\nPlan to attend regularly\n\n\nDesigned to be complete-able during the lab period, but due by 11:59 PM that same day."
  },
  {
    "objectID": "slides/2025-08-25-welcome.html#problem-sets-30",
    "href": "slides/2025-08-25-welcome.html#problem-sets-30",
    "title": "Welcome to STA 240!",
    "section": "Problem Sets (30%)",
    "text": "Problem Sets (30%)\n\nMostly pencil-and-paper math problems, with some coding thrown in occasionally;\nCompose solutions however you want: scan or photograph written work, handwriting capture, LaTeX, Quarto, whatever;\nSubmit a single PDF in Gradescope (and mark your pages!)\n\n\n\n\n\n\n\nLate policy\n\n\nNo late work will be accepted unless you request an extension in advance by e-mailing JZ. All reasonable requests will be entertained, but extensions will not be long."
  },
  {
    "objectID": "slides/2025-08-25-welcome.html#exams-20-each",
    "href": "slides/2025-08-25-welcome.html#exams-20-each",
    "title": "Welcome to STA 240!",
    "section": "Exams (20% each)",
    "text": "Exams (20% each)\nTraditional, in-class, written exams:\n\n\nMidterm 1: Thursday October 9 in your lab;\n\nMidterm 2: Thursday November 13 in your lab;\n\nFinal Exam: Friday December 12.\n\n\nYou are allowed only two resources:\n\nA “dumb” calculator (no wi-fi), which you won’t need;\nBoth sides of one 8.5” x 11” note sheet created by you.\n\n\n\n\n\n\n\n\n\nIf you need testing accommodations…\n\n\nMake sure I get an SDAO letter, and make your appointments in the Testing Center now."
  },
  {
    "objectID": "slides/2025-08-25-welcome.html#attendance",
    "href": "slides/2025-08-25-welcome.html#attendance",
    "title": "Welcome to STA 240!",
    "section": "Attendance",
    "text": "Attendance\nNot required. Live your life."
  },
  {
    "objectID": "slides/2025-08-25-welcome.html#communication",
    "href": "slides/2025-08-25-welcome.html#communication",
    "title": "Welcome to STA 240!",
    "section": "Communication",
    "text": "Communication\nIf you wish to ask questions in writing…\n\nPost on Ed: about general course policies and content;\nEmail JZ directly: personal matters.\n\nYou should not really be emailing the TAs directly for any reason."
  },
  {
    "objectID": "slides/2025-08-25-welcome.html#collaboration",
    "href": "slides/2025-08-25-welcome.html#collaboration",
    "title": "Welcome to STA 240!",
    "section": "Collaboration",
    "text": "Collaboration\nYou are enthusiastically encouraged to work together on labs and problem sets. You will learn a lot from each other! Two policies:\n\n✅ Acknowledge your collaborators: “Aloysius, Cybill, and I worked together on this problem;”\n❌ Do not outright share or copy solutions. All submitted work must be your own.\n\nViolation of the second policy is plagiarism. Sharers and recipients alike are referred to the conduct office and receive zeros."
  },
  {
    "objectID": "slides/2025-08-25-welcome.html#use-of-outside-resources-including-ai",
    "href": "slides/2025-08-25-welcome.html#use-of-outside-resources-including-ai",
    "title": "Welcome to STA 240!",
    "section": "Use of outside resources, including AI",
    "text": "Use of outside resources, including AI\n\nUsing ChatGPT to complete assignments is like bringing a forklift into the weight room; you will never improve your cognitive fitness that way.\n\n\nIf you find a problem solution online (or prompt an LLM to generate one) and submit it as your own work, that will obviously be considered plagiarism;\nOtherwise, all outside resources are fair game for you to study and get extra practice;\nIf you outsource all of your thinking to a language model, you will probably tank every exam. Good luck!"
  },
  {
    "objectID": "slides/2025-08-25-welcome.html#whats-the-level-of-math-here",
    "href": "slides/2025-08-25-welcome.html#whats-the-level-of-math-here",
    "title": "Welcome to STA 240!",
    "section": "What’s the level of math here?",
    "text": "What’s the level of math here?\nThis is essentially a pencil-and-paper math class. We will use all of the basic skills taught in Calc I and II:\n\ndifferentiation (power rule, chain rule, all that)\nintegration (FTOC, improper integrals, substitution, by parts…)\nlimits and continuity (L’Hôpital’s rule, etc)\ninfinite series (Taylor series for \\(e^x\\), anyone?)\n\n\nREVIEW: Problem Set 0 is due 5PM Friday September 5.\n\n\n\n\n\n\n\n\nThis is not sink-or-swim.\n\n\n\nLab on Thursday August 28 is a review session and work period for this assignment;\nThe course page has a math review section you can refer to."
  },
  {
    "objectID": "slides/2025-08-25-welcome.html#probability",
    "href": "slides/2025-08-25-welcome.html#probability",
    "title": "Welcome to STA 240!",
    "section": "Probability",
    "text": "Probability\n\n\n\n\n\n\n\nWhat do we do?\n\n\nState the distribution (rules) of some random phenomenon, and then study how the realizations of that phenomenon “typically” behave."
  },
  {
    "objectID": "slides/2025-08-25-welcome.html#an-archetypal-probability-problem",
    "href": "slides/2025-08-25-welcome.html#an-archetypal-probability-problem",
    "title": "Welcome to STA 240!",
    "section": "An archetypal probability problem",
    "text": "An archetypal probability problem\n\nGiven a fair coin, how many flips will it take on average until you observe the first head?"
  },
  {
    "objectID": "slides/2025-08-25-welcome.html#an-archetypal-probability-problem-1",
    "href": "slides/2025-08-25-welcome.html#an-archetypal-probability-problem-1",
    "title": "Welcome to STA 240!",
    "section": "An archetypal probability problem",
    "text": "An archetypal probability problem\n\nGiven a fair coin, how many flips will it take on average until you observe the first head?\n\nWhy is this a probability problem?\n\nselect a phenomenon to study (the outcome of a coin flip);\nfully specify its distribution (it’s a fair, 50-50 coin);\nstudy typical behavior of realizations (how many flips on average until the first head?)."
  },
  {
    "objectID": "slides/2025-08-25-welcome.html#statistics-is-probability-in-reverse",
    "href": "slides/2025-08-25-welcome.html#statistics-is-probability-in-reverse",
    "title": "Welcome to STA 240!",
    "section": "Statistics is “probability in reverse”",
    "text": "Statistics is “probability in reverse”\n\n\n\n\n\n\n\nWhat do we do?\n\n\nStart with the realizations of a random phenomenon with unknown distribution, and try to use those realizations to figure out what the distribution is."
  },
  {
    "objectID": "slides/2025-08-25-welcome.html#an-archetypal-statistics-problem",
    "href": "slides/2025-08-25-welcome.html#an-archetypal-statistics-problem",
    "title": "Welcome to STA 240!",
    "section": "An archetypal statistics problem",
    "text": "An archetypal statistics problem\n\nGiven 28 flips from a mysterious coin, can you tell if it is fair?"
  },
  {
    "objectID": "slides/2025-08-25-welcome.html#an-archetypal-statistics-problem-1",
    "href": "slides/2025-08-25-welcome.html#an-archetypal-statistics-problem-1",
    "title": "Welcome to STA 240!",
    "section": "An archetypal statistics problem",
    "text": "An archetypal statistics problem\n\nGiven 28 flips from a mysterious coin, can you tell if it is fair?\n\n\nIn probability, we assumed that the coin was fair, and then reasoned deductively from that;\nIn statistics, we do not do this;\nWe start with some coin, which may or may not be fair, and then we try to “infer” its properties based on the data."
  },
  {
    "objectID": "slides/2025-08-25-welcome.html#probability-and-statistics",
    "href": "slides/2025-08-25-welcome.html#probability-and-statistics",
    "title": "Welcome to STA 240!",
    "section": "Probability and statistics",
    "text": "Probability and statistics\n\n\n\n\n\n\n\n\n\n\n\n\nProbability\nForward problem\nDeductive\nReasons from rules to consequences\n\n\nStatistics\nInverse problem\nInductive\nObserve consequences, and infer rules"
  },
  {
    "objectID": "slides/2025-08-25-welcome.html#forward-problem-versus-inverse-problem",
    "href": "slides/2025-08-25-welcome.html#forward-problem-versus-inverse-problem",
    "title": "Welcome to STA 240!",
    "section": "Forward problem versus inverse problem",
    "text": "Forward problem versus inverse problem\n\n\n\n\n\nForward: read the rulebook, and then play a game of chess;\nInverse: watch a chess match, and based on the players’ behavior, try to guess the rules."
  },
  {
    "objectID": "slides/2025-08-25-welcome.html#forward-problem-versus-inverse-problem-1",
    "href": "slides/2025-08-25-welcome.html#forward-problem-versus-inverse-problem-1",
    "title": "Welcome to STA 240!",
    "section": "Forward problem versus inverse problem",
    "text": "Forward problem versus inverse problem\n\nDifferentiation is a forward problem. You know the function \\(F\\), and you take its darn derivative;\nIntegration is an inverse problem. Given the derivative, you have to work backward to figure out what the original function was:\n\n\n\\[\n\\text{FTOC:}\\quad\\int_a^bF'(x)\\,\\text{d} x=F(b)-F(a).\n\\]"
  },
  {
    "objectID": "slides/2025-08-25-welcome.html#inverse-problems-are-tricky",
    "href": "slides/2025-08-25-welcome.html#inverse-problems-are-tricky",
    "title": "Welcome to STA 240!",
    "section": "Inverse problems are tricky!",
    "text": "Inverse problems are tricky!\n\n\nForward\n\n“Here’s the question. What’s the answer?”\n\n\nInverse\n\n“If this is the answer, then what was the question?”\n\n\n\n\n\n\n\n\n\nGird your loins\n\n\nLike all inverse problems, you will find that statistics is subtler, less well-defined, less straightforward, and more open-ended than probability."
  },
  {
    "objectID": "slides/2025-08-25-welcome.html#the-philosophy-of-probability",
    "href": "slides/2025-08-25-welcome.html#the-philosophy-of-probability",
    "title": "Welcome to STA 240!",
    "section": "The philosophy of probability",
    "text": "The philosophy of probability\n\nTwo common interpretive perspectives:\n\n\nFrequentist: probability describes the long run frequency of repeatable events;\n\nSubjective: probability describes an observer’s subjective experience of uncertainty. AKA: their “degrees of belief.”\n\n\n\n\n\n\n\n\n\nYou need both perspectives.\n\n\nLike the wave-particle duality of light, both are true and useful, but their coexistence can be tense and uneasy. We just have to learn to live with that.\n\n\n\n\n\n\n\n\n\n\n\nThe math doesn’t care which you prefer.\n\n\nRegardless your interpretation, the mathematics of probability is the same."
  },
  {
    "objectID": "syllabus/syllabus_overview.html",
    "href": "syllabus/syllabus_overview.html",
    "title": "Course overview",
    "section": "",
    "text": "This is a course on the mathematics of probability and statistics. In probability, we describe the distribution of a random phenomenon and then study how the realizations of that phenomenon typically behave. In statistics, we do the reverse; we observe realizations of a random phenomenon with unknown distribution, and then use the data to figure out what the distribution is. We will spend twelve weeks on the first, and then three weeks on the second. Topics include set theory, probability spaces, counting methods, conditional probability, discrete and (absolutely) continuous random variables, transformations of random variables, (pseudo)random number generation, bivariate distributions, concentration inequalities, limit theorems, maximum likelihood estimation, and Bayesian inference with conjugate priors.\nAside from the concrete topics, the course emphasizes four generic intellectual themes:\n\nStudents in this class will improve their mathematical maturity. They will deepen their experience with all of the main ideas and techniques of univariate calculus, they will encounter topics like set theory and combinatorics, which may be new to them, and they will get a taste of rigorous mathematical proof;\nProbability and statistics weasel their way into pretty much everything these days, and students will study a wide variety of famous and obscure applications coming from the natural and social sciences, technology and industry, and even arts and culture;\nProbability and statistics are often counterintuitive, and humans frequently make silly and harmful mistakes in reasoning. Students will use their mathematical knowledge to identify and critique these errors…and hopefully avoid them themselves;\nThis is a course about doing the math, but in the modern era, this has a symbiotic relationship with computer simulation. Simulation is used both to verify and probe mathematical results, as well as to substitute for them when a proof remains beyond the frontier. Students will learn the basics of the R programming language and use it to simulate probabilistic environments and reinforce their mathematical reasoning.\n\nPrerequisites: single-variable calculus.",
    "crumbs": [
      "Syllabus",
      "Overview"
    ]
  },
  {
    "objectID": "syllabus/syllabus_overview.html#description",
    "href": "syllabus/syllabus_overview.html#description",
    "title": "Course overview",
    "section": "",
    "text": "This is a course on the mathematics of probability and statistics. In probability, we describe the distribution of a random phenomenon and then study how the realizations of that phenomenon typically behave. In statistics, we do the reverse; we observe realizations of a random phenomenon with unknown distribution, and then use the data to figure out what the distribution is. We will spend twelve weeks on the first, and then three weeks on the second. Topics include set theory, probability spaces, counting methods, conditional probability, discrete and (absolutely) continuous random variables, transformations of random variables, (pseudo)random number generation, bivariate distributions, concentration inequalities, limit theorems, maximum likelihood estimation, and Bayesian inference with conjugate priors.\nAside from the concrete topics, the course emphasizes four generic intellectual themes:\n\nStudents in this class will improve their mathematical maturity. They will deepen their experience with all of the main ideas and techniques of univariate calculus, they will encounter topics like set theory and combinatorics, which may be new to them, and they will get a taste of rigorous mathematical proof;\nProbability and statistics weasel their way into pretty much everything these days, and students will study a wide variety of famous and obscure applications coming from the natural and social sciences, technology and industry, and even arts and culture;\nProbability and statistics are often counterintuitive, and humans frequently make silly and harmful mistakes in reasoning. Students will use their mathematical knowledge to identify and critique these errors…and hopefully avoid them themselves;\nThis is a course about doing the math, but in the modern era, this has a symbiotic relationship with computer simulation. Simulation is used both to verify and probe mathematical results, as well as to substitute for them when a proof remains beyond the frontier. Students will learn the basics of the R programming language and use it to simulate probabilistic environments and reinforce their mathematical reasoning.\n\nPrerequisites: single-variable calculus.",
    "crumbs": [
      "Syllabus",
      "Overview"
    ]
  },
  {
    "objectID": "syllabus/syllabus_overview.html#meetings",
    "href": "syllabus/syllabus_overview.html#meetings",
    "title": "Course overview",
    "section": "Meetings",
    "text": "Meetings\n\n\n\n\n\n\n\n\n\nMeeting\nLocation\nTime\nStaff\n\n\n\n\nLectures\nOld Chem 116\nMoWe 3:05 PM - 4:20 PM\nJohn\n\n\nLab 01\nPerkins LINK 087 (Classroom 3)\nTh 1:25 PM - 2:40 PM\nAurora\n\n\nLab 02\nPerkins LINK 087 (Classroom 3)\nTh 3:05 PM - 4:20 PM\nAurora",
    "crumbs": [
      "Syllabus",
      "Overview"
    ]
  },
  {
    "objectID": "syllabus/syllabus_resources.html",
    "href": "syllabus/syllabus_resources.html",
    "title": "University resources",
    "section": "",
    "text": "If you are having difficulty with the costs associated with this course (obtaining a laptop, mostly), here are some resources:\n\nKarsh Office of Undergraduate Support: Regardless of your aid package, Karsh offers loans and resources for connecting students with campus programs that might help alleviate course costs.\nDukeLIFE: The Course Material Assistance program offers assistance for eligible students, including through the LIFE Loaner Laptop Program. Students who are eligible for DukeLIFE benefits are notified before the start of the semester; program resources are limited.\nDuke Link: They have a small supply of laptops that can be rented out for five days at a time.",
    "crumbs": [
      "Syllabus",
      "University resources"
    ]
  },
  {
    "objectID": "syllabus/syllabus_resources.html#course-costs",
    "href": "syllabus/syllabus_resources.html#course-costs",
    "title": "University resources",
    "section": "",
    "text": "If you are having difficulty with the costs associated with this course (obtaining a laptop, mostly), here are some resources:\n\nKarsh Office of Undergraduate Support: Regardless of your aid package, Karsh offers loans and resources for connecting students with campus programs that might help alleviate course costs.\nDukeLIFE: The Course Material Assistance program offers assistance for eligible students, including through the LIFE Loaner Laptop Program. Students who are eligible for DukeLIFE benefits are notified before the start of the semester; program resources are limited.\nDuke Link: They have a small supply of laptops that can be rented out for five days at a time.",
    "crumbs": [
      "Syllabus",
      "University resources"
    ]
  },
  {
    "objectID": "syllabus/syllabus_resources.html#tech-support",
    "href": "syllabus/syllabus_resources.html#tech-support",
    "title": "University resources",
    "section": "Tech support",
    "text": "Tech support\nContact the Duke OIT Service Desk at oit.duke.edu/help.",
    "crumbs": [
      "Syllabus",
      "University resources"
    ]
  },
  {
    "objectID": "syllabus/syllabus_resources.html#academic-support",
    "href": "syllabus/syllabus_resources.html#academic-support",
    "title": "University resources",
    "section": "Academic support",
    "text": "Academic support\nThere are times you may need help with the class that is beyond what can be provided by the teaching team. In those instances, I encourage you to visit the Academic Resource Center. The Academic Resource Center (the ARC) offers services to support students academically during their undergraduate careers at Duke. The ARC can provide support with time management, academic skills and strategies, course-specific tutoring, and more. ARC services are available free to all Duke undergraduate student studying any discipline.\nYou can contact the Academic Resource Center by phone at (919) 684-5917, by email at theARC@duke.edu, or by visiting http://arc.duke.edu/.",
    "crumbs": [
      "Syllabus",
      "University resources"
    ]
  },
  {
    "objectID": "syllabus/syllabus_resources.html#accessibility",
    "href": "syllabus/syllabus_resources.html#accessibility",
    "title": "University resources",
    "section": "Accessibility",
    "text": "Accessibility\nIf any portion of the course is not accessible to you due to challenges with technology or the course format, please let me know so we can make appropriate accommodations.\nThe Student Disability Access Office (SDAO) is available to ensure that students can engage with their courses and related assignments. Students should contact the SDAO to request or update accommodations under these circumstances.",
    "crumbs": [
      "Syllabus",
      "University resources"
    ]
  },
  {
    "objectID": "syllabus/syllabus_resources.html#mental-health-and-well-being",
    "href": "syllabus/syllabus_resources.html#mental-health-and-well-being",
    "title": "University resources",
    "section": "Mental health and well-being",
    "text": "Mental health and well-being\nDuke is committed to holistic student wellbeing, which includes one’s mental, emotional, and physical health. The university offers resources to help students manage daily stress, to encourage intentional self-care, and to access just-in-time support. If you find you need support, your mental and/or emotional health concerns are impacting your day-to-day activities, your academic performance, or you need someone to talk to, the resources below are available to you:\n\nDukeReach: DukeReach provides comprehensive outreach services to support students in managing all aspects of wellbeing, including referrals and follow-up services for students who are experiencing significant challenges related to mental health, physical health, social adjustment, and/or a variety of other stressors. You can contact the DukeReach team at dukereach@duke.edu;\nCounseling and Psychological Services (CAPS): CAPS offers counseling services to Duke students including virtual appointments, and referrals in the community. You do not need an appointment for an initial assessment. You may walk in or call 919-660-1000 to get started. Hours: Monday-Friday 9:00am - 4:00pm. After hours counseling services are available at no additional cost to students, you can call: 919-660-1000 Option 2;\nTimelyCare: TimelyCare is an online platform that is a convenient, confidential, and free way for Duke students to receive 24/7 mental health support through TalkNow and scheduled counseling;\nDuke Student Health: Student Health offers a wide range of healthcare services for all Duke students, many of which are covered by the student health fee. To make an appointment call (919) 681-9355. Hours: Monday - Friday, 8am - 4:30pm, Thursday 9am - 4:30pm. Closed from 12-12:30 each day.",
    "crumbs": [
      "Syllabus",
      "University resources"
    ]
  },
  {
    "objectID": "syllabus/syllabus_assignments.html",
    "href": "syllabus/syllabus_assignments.html",
    "title": "Assignments and grading",
    "section": "",
    "text": "Your final course grade will be calculated as follows:\nYour final letter grade will be determined based on the usual thresholds:",
    "crumbs": [
      "Syllabus",
      "Assignments and grading"
    ]
  },
  {
    "objectID": "syllabus/syllabus_assignments.html#labs-10",
    "href": "syllabus/syllabus_assignments.html#labs-10",
    "title": "Assignments and grading",
    "section": "Labs (10%)",
    "text": "Labs (10%)\nIn lab every Thursday, you will complete a guided activity that introduces you to special topics, extensions, or applications of the latest course material. Along the way, we will teach you how to implement some of the ideas from the course in the R programming language. Ideally you should be able to finish the activity by the end of your lab period, but in any case, you must submit your lab work by 11:59 ET PM that same day.\nEach lab assignment will be graded on the following scale:\n\n1.0: a complete, good faith attempt at every part of every problem;\n0.5: partially complete;\n0.0: no submission, or mostly incomplete.\n\n\n\n\n\n\n\nGrace\n\n\n\nYour two lowest lab scores will be dropped at the end of the semester.",
    "crumbs": [
      "Syllabus",
      "Assignments and grading"
    ]
  },
  {
    "objectID": "syllabus/syllabus_assignments.html#problem-sets-30",
    "href": "syllabus/syllabus_assignments.html#problem-sets-30",
    "title": "Assignments and grading",
    "section": "Problem Sets (30%)",
    "text": "Problem Sets (30%)\nMathematics is like everything else in life; if you practice, you improve. As such, the problem sets are really the heart of the course. They will mostly consist of pencil-and-paper math problems, but sometimes you will be asked to do some coding. You may compose you solutions however you like (scan or photograph written work, LaTeX, Quarto, handwriting capture on a tablet, whatever) so long as you ultimately upload a single .pdf file to Gradescope.\n\n\n\n\n\n\nGrace\n\n\n\nYour lowest problem set score will be dropped at the end of the semester.",
    "crumbs": [
      "Syllabus",
      "Assignments and grading"
    ]
  },
  {
    "objectID": "syllabus/syllabus_assignments.html#exams-20-each",
    "href": "syllabus/syllabus_assignments.html#exams-20-each",
    "title": "Assignments and grading",
    "section": "Exams (20% each)",
    "text": "Exams (20% each)\nThere will be three exams. The dates, times, and locations are firm, so mark your calendar now:\n\nMidterm 1: Thursday October 9 during your lab;\nMidterm 2: Thursday November 13 during your lab;\nFinal: Friday December 12 from 9 AM - 12 PM in Old Chem 116.\n\nThese will be old school, pencil-and-paper, in-class exams. You are allowed only two resources:\n\na “dumb” calculator (no wi-fi). You will not actually need it, but if you want it as a security blanket, be my guest;\nboth sides of one 8.5” x 11” sheet of notes, prepared by you and you alone. You can create it however you wish: handwritten, on a computer, etc.\n\nIf you seek testing accommodations, make sure the Student Disability Access Office sends me a letter, and then please make your appointments in the Testing Center as soon as possible.\n\n\n\n\n\n\nGrace\n\n\n\nIf you do better on the final exam than you did on one of the midterms, we will replace your lowest midterm exam score with your final exam score.\n\n\n\n\n\n\n\n\nNo make-up exams\n\n\n\nIf you are absent from one of the midterms for whatever reason, there will not be a make-up. Pursuant to the above policy, we will simply replace the missed midterm score with the final exam score. If you miss the final exam, you get a zero unless you have a Dean’s Excuse.",
    "crumbs": [
      "Syllabus",
      "Assignments and grading"
    ]
  },
  {
    "objectID": "explainers/histogram.html",
    "href": "explainers/histogram.html",
    "title": "Histogram",
    "section": "",
    "text": "To visualize the distribution of a set of numbers, you can plot a histogram. If you keep studying statistics, you will learn that a histogram is a statistical estimator of a probability density function. Neat!\nAnyway, this is super simple in R. Here is a vector of random numbers:\n\nset.seed(1)\nx &lt;- rgamma(5000, shape = 2, rate = 1)\n\nTo plot a histogram, use the hist command:\n\nhist(x, breaks = \"Scott\", freq = FALSE,\n     main = \"My histogram\",\n     xlab = \"x values\",\n     ylab = \"density\", \n     col = \"lightblue\", \n     border = \"white\")\n\n\n\n\n\n\n\nThe hist command has many arguments, and you can read the documentation for more detail. Apart from the cosmetic stuff controlling the labeling and the color, there are three main arguments to think about:\n\nthe first argument is obviously the vector of numbers whose distribution you want to visualize;\n\nbreaks determines the size of the bins (ie how many bars the histogram has). You can play around with different options, but I always set breaks = \"Scott\" because David Scott was one of my professors in graduate school and he knows his shit. So we pour one out for David and set breaks = \"Scott\";\nif you set freq = TRUE, the height of the bars will correspond to the count of how many values fall into the bin. freq = FALSE renormalizes the bar heights so that the area under the histogram is literally one, and the histogram can be directly compared with a density, which we will often do.\n\n\nhist(x, breaks = \"Scott\", freq = FALSE)\ncurve(dgamma(x, 2, 1), from = min(x), to = max(x), n = 1000, \n      col = \"red\", lwd = 3, add = TRUE)",
    "crumbs": [
      "`R` Helpers",
      "Histograms"
    ]
  },
  {
    "objectID": "explainers/loops.html",
    "href": "explainers/loops.html",
    "title": "for loops",
    "section": "",
    "text": "A for loop is a way to repeat the same steps several times. It’s useful when we want to do something over and over again, like simulate a random experiment.\nThe pattern looks like this:\nfor (i in 1:N) {\n  do_something\n}",
    "crumbs": [
      "`R` Helpers",
      "for loops"
    ]
  },
  {
    "objectID": "explainers/loops.html#example-counting-sixes-in-100-dice-rolls",
    "href": "explainers/loops.html#example-counting-sixes-in-100-dice-rolls",
    "title": "for loops",
    "section": "Example: Counting sixes in 100 dice rolls",
    "text": "Example: Counting sixes in 100 dice rolls\n\nsix_count &lt;- 0  # start a counter\n\nfor (i in 1:100) {\n  roll &lt;- sample(1:6, size = 1)\n  if (roll == 6) {\n    six_count &lt;- six_count + 1\n  }\n}\n\nsix_count\n\n[1] 16\n\n\n\nThe loop goes through i = 1, 2, ..., 100.\n\nEach time, it rolls a die and checks if it’s a six.\n\nIf it is, it increments six_count by 1.\n\nAt the end, six_count is the total number of sixes rolled in 100 trials.",
    "crumbs": [
      "`R` Helpers",
      "for loops"
    ]
  },
  {
    "objectID": "explainers/loops.html#example-plotting-many-lines",
    "href": "explainers/loops.html#example-plotting-many-lines",
    "title": "for loops",
    "section": "Example: plotting many lines",
    "text": "Example: plotting many lines\nIn this explainer, we saw how to use the curve function to plot lines. If you want a plot with many lines, you can just call the curve function many times with the add = TRUE argument to layer things. In that explainer, we just hard-coded the multiple calls to curve. A for loop can clean that up:\n\ncurve(1*x^2, from = -5, to = 5, n = 500, col = \"red\", ylab = \"\")\n\nfor(a in 2:25){\n  curve(a*x^2, from = -5, to = 5, n = 500, ylab = \"\", add = TRUE)\n}\n\n\n\n\n\n\n\nThat code plots the function \\(ax^2\\) every value of \\(a=1,\\,2,\\,3,\\,...,\\,25\\). Instead of writing 25 calls to the curve function, I consolidate with a loop.",
    "crumbs": [
      "`R` Helpers",
      "for loops"
    ]
  },
  {
    "objectID": "explainers/quarto.html",
    "href": "explainers/quarto.html",
    "title": "Quarto",
    "section": "",
    "text": "Quarto is a technical publishing system that allows you to write documents where text, math, code, and the output of that code are seamlessly integrated in a reproducible way. Quarto is baller. I had never used it before coming to Duke, and now I am low-key obsessed. You will mainly use Quarto for the lab write-ups in this class, but if you wish to use it for your problem set solutions as well, be my guest!\nBelow is a very brief primer to get you started. If you took STA 101 or STA 199, you have already used Quarto extensively and can probably skip this. If you wish to go deeper and become a Quarto power user, what are you waiting for?",
    "crumbs": [
      "`R` Helpers",
      "Quarto"
    ]
  },
  {
    "objectID": "explainers/quarto.html#quick-start",
    "href": "explainers/quarto.html#quick-start",
    "title": "Quarto",
    "section": "Quick start",
    "text": "Quick start\nHere is a screenshot of my RStudio container:\n\n\n\n\n\nIn the upper left, I have the editor open, and I’m working on a .qmd file. This is just a text file with a special format. When I’m done editing, I hit the Render button (circled in red), and if there are no errors, a nice .pdf file is produced in the Viewer tab in the bottom right.\nThat’s pretty much the whole ballgame:\n\nEdit the .qmd file → hit Render → get a .pdf\n\nAlright, so what goes in the .qmd file?\n\nAt the top, in between ---, you have the document’s metadata: title, author, date, and so forth. We are not going to get fancy with it, but there are loads of settings you can play with;\nYou can write text the same as you would in Google Docs or Microsoft Word. Quarto syntax is based on markdown, and you can learn about the formatting options here;\nThe grey block beginning with ```{r} and ending with ``` is called a code chunk. Chunks of code go in there. In the rendered document, the code will be nicely formatted and stylized amidst the text;\nWhen you render, the code in your code chunks is actually executed, and the output is included in the final document. This is super convenient for producing tables and figures;\nYou can also include nicely formatted equations and math notation in your Quarto documents. See here for details.",
    "crumbs": [
      "`R` Helpers",
      "Quarto"
    ]
  },
  {
    "objectID": "explainers/if-else.html",
    "href": "explainers/if-else.html",
    "title": "if-else statements and logical conditions",
    "section": "",
    "text": "Sometimes we want our code to make a decision: do one thing if a condition is true, and something else if it’s false. In R, the tool for this is if ... else. The pattern looks like this:\n\nif (condition) {\n  do_something\n} else {\n  do_something_else\n}\n\nSuppose we roll a six-sided die:\n\nroll &lt;- sample(1:6, size = 1)\nroll\n\n[1] 2\n\n\nWe might want to check whether the outcome is “big” (4, 5, or 6) or “small” (1, 2, or 3). Here’s how we can use if else to do that:\n\nif (roll &gt;= 4) {\n  result &lt;- \"Big\"\n} else {\n  result &lt;- \"Small\"\n}\n\nresult\n\n[1] \"Small\"\n\n\nLet’s unpack:\n\nFirst, sample(1:6, size = 1) chooses a random number from 1 to 6;\nThen the if else checks: if the roll is 4 or higher, we call it “Big.” Otherwise, we call it “Small.”\n\nThis is how you can make simple decisions in R code.\nLogical conditions\nIn the condition of if, you can use logical comparisons. Here are the most common ones:\n\n\n\n\n\n\n\nOperator\nMeaning\nExample\n\n\n\n==\nequal to\n\nx == 3 is TRUE if x is 3\n\n\n!=\nnot equal to\n\nx != 3 is TRUE if x is not 3\n\n\n&lt;\nless than\n\nx &lt; 5 is TRUE if x is less than 5\n\n\n&lt;=\nless than or equal\nx &lt;= 5\n\n\n&gt;\ngreater than\nx &gt; 10\n\n\n&gt;=\ngreater than or equal\nx &gt;= 10\n\n\n&\nAND (both conditions true)\nx &gt; 0 & x &lt; 5\n\n\n|\nOR (at least one true)\nx &lt; 0 | x &gt; 10\n\n\n!\nNOT (negates a condition)\n!(x == 3)\n\n\n%in%\nelement of a vector\n\nx %in% c(1,2,3) is TRUE if x is 1, 2, or 3\n\n\n\nHere they are in action:\n\nx &lt;- 4\nx &gt; 3       \n\n[1] TRUE\n\nx &lt; 3       \n\n[1] FALSE\n\nx &gt;= 3 & x &lt;= 5   \n\n[1] TRUE\n\nx &lt; 3 | x &gt; 5     \n\n[1] FALSE\n\n!(x == 4)   \n\n[1] FALSE\n\n\nHere is how these might be deployed in the context of if-else:\n\nif (x &gt; 0 & x %% 2 == 0) {\n  result &lt;- \"Positive even\"\n} else {\n  result &lt;- \"Other\"\n}\nresult\n\n[1] \"Positive even\"",
    "crumbs": [
      "`R` Helpers",
      "if-else and logic"
    ]
  },
  {
    "objectID": "explainers/latex.html",
    "href": "explainers/latex.html",
    "title": "Typesetting math with LaTeX and Quarto",
    "section": "",
    "text": "LaTeX is a software system for typesetting documents with a bunch of math in them1. It is fully integrated into Quarto, so you can do all of your writing and coding and plotting and math-ing in one place. Pretty neat. We’ll just show you a few things to get you going, but if you want to use the entire system for realz, you should make an Overleaf account and start playing.",
    "crumbs": [
      "`R` Helpers",
      "Typing math"
    ]
  },
  {
    "objectID": "explainers/latex.html#inline-math-goes-between-single-dollar-signs",
    "href": "explainers/latex.html#inline-math-goes-between-single-dollar-signs",
    "title": "Typesetting math with LaTeX and Quarto",
    "section": "Inline math goes between single dollar signs",
    "text": "Inline math goes between single dollar signs\n\n\n\n\n\n\n\nGoal\ninclude an equation in the middle of a sentence\n\n\nWhat you type in the .qmd file\nThe function $h(x)=\\ln x^2$ is my favorite\n\n\nHow it will look when you render\nThe function \\(h(x)=\\ln x^2\\) is my favorite.",
    "crumbs": [
      "`R` Helpers",
      "Typing math"
    ]
  },
  {
    "objectID": "explainers/latex.html#a-math-block-goes-between-double-dollar-signs",
    "href": "explainers/latex.html#a-math-block-goes-between-double-dollar-signs",
    "title": "Typesetting math with LaTeX and Quarto",
    "section": "A math block goes between double dollar signs",
    "text": "A math block goes between double dollar signs\nSay you want a whole bunch of math set off on a new line, in between your paragraphs, like this:\n\\[\nf(x)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{1}{2}\\frac{(x-\\mu)^2}{\\sigma^2}\\right).\n\\]\nIn that case, you put this into your source .qmd file:\n\n$$\nf(x)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{1}{2}\\frac{(x-\\mu)^2}{\\sigma^2}\\right).\n$$\n\nWhen you render, you’ll get the pretty math. Notice some of the commands we used:\n\n\n\\exp for \\(\\exp\\);\n\n\\sqrt{2} for the square root \\(\\sqrt{2}\\);\n\n\\frac{1}{2} to get a fraction \\(\\frac{1}{2}\\);\n\n(x-\\mu)^2 to get an exponent \\((x-\\mu)^2\\);\n\n\\mu, \\sigma, \\pi for the Greek letters \\(\\mu\\), \\(\\sigma\\), and \\(\\pi\\);\nand so on.\n\n\n\n\n\n\n\nMath commands in LaTeX\n\n\n\nHere is a bank of common LaTeX commands. You won’t use most of them. Alternatively, Detexify is a very useful site where you can draw a symbol and it will tell you what the LaTeX command is for it.",
    "crumbs": [
      "`R` Helpers",
      "Typing math"
    ]
  },
  {
    "objectID": "explainers/latex.html#lining-equations-up",
    "href": "explainers/latex.html#lining-equations-up",
    "title": "Typesetting math with LaTeX and Quarto",
    "section": "Lining equations up",
    "text": "Lining equations up\nIf you are typesetting a sequence of equalities, you should do it inside an aligned environment so that the equal signs are all lined up nice.\nIf you type this:\n\n$$\n\\begin{aligned}\nx(x+1)(x-1) &= (x^2+x)(x-1)\\\\\n&=x^3-x^2+x^2-x\\\\\n&=x^3-x.\n\\end{aligned}\n$$\n\nYou will get this:\n\\[\n\\begin{aligned}\nx(x+1)(x-1) &= (x^2+x)(x-1)\\\\\n&=x^3-x^2+x^2-x\\\\\n&=x^3-x.\n\\end{aligned}\n\\]\nThe \\\\ introduces a line break, and the & in each equation tells it where to line things up.",
    "crumbs": [
      "`R` Helpers",
      "Typing math"
    ]
  },
  {
    "objectID": "explainers/latex.html#footnotes",
    "href": "explainers/latex.html#footnotes",
    "title": "Typesetting math with LaTeX and Quarto",
    "section": "Footnotes",
    "text": "Footnotes\n\nThere is no consensus on how to pronounce “LaTeX.” JZ says “lah-tech.” Some folks say “lay-tech.” Some even say “lay-techs,” just like a latex glove. But we all agree that the emphasis is on the first syllable. Perhaps I’ll award 2.56 bonus points if you invent a new way to pronounce it. But probably not.↩︎",
    "crumbs": [
      "`R` Helpers",
      "Typing math"
    ]
  },
  {
    "objectID": "explainers/rng-seed.html",
    "href": "explainers/rng-seed.html",
    "title": "Setting a random number seed",
    "section": "",
    "text": "Statistical computing software like R can generate (pseudo)random numbers, like this:\n\nsample(c(1, 2, 3, 4, 5, 6), 5, replace = TRUE)\n\n[1] 1 4 1 2 5\n\n\nOr this:\n\nrbinom(5, 50, 0.3)\n\n[1] 20 16 16 10 12\n\n\nOr this:\n\nrnorm(5)\n\n[1] -0.928567035 -0.294720447 -0.005767173  2.404653389  0.763593461\n\n\nLook pretty random to me. This allows us to perform simulations, which is an important part of the modern statistician’s toolkit. Having said that, when you’re working with computer-generated random numbers, you want your work to be reproducible so that other people can check it. This means that you want to set a random number seed before you do a simulation. This ensures that the stream of random numbers in your simulation is the same every time, and someone else could run your code and get the exact same results that you did.\nSetting a seed looks like this:\n\nset.seed(8675309)\n\nrnorm(5)\n\n[1] -0.9965824  0.7218241 -0.6172088  2.0293916  1.0654161\n\n\nEvery time you run that code, you will get the same numbers:\n\nset.seed(8675309)\n\nrnorm(5)\n\n[1] -0.9965824  0.7218241 -0.6172088  2.0293916  1.0654161\n\n\nSo, if you ever write a code chunk that generates random numbers (eg. using the sample function or one of the r- functions), you should begin the code chunk by setting a random number seed so that you get the same results every time you run your stuff. The syntax as you saw above is set.seed(INTEGER). Sometimes we will tell you what number to use. Other times (and once you exit the course), you can put whatever you want. It doesn’t really matter. If you require inspiration, try these:\n\n1\n20\n988\n24601\n362436\n525600\n8675309\n8005882300",
    "crumbs": [
      "`R` Helpers",
      "Setting a seed"
    ]
  },
  {
    "objectID": "explainers/summary-statistics.html",
    "href": "explainers/summary-statistics.html",
    "title": "Summary statistics",
    "section": "",
    "text": "If you have a vector of numbers in R, there are many basic commands you can use to compute summary statistics. To illustrate, here is a vector of random numbers:\n\nset.seed(20)\nx &lt;- rnorm(100)\n\nOf course, these commands are useful no matter where the numbers came from:\n\nsum(x) # the total of the numbers\n\n[1] 0.4908104\n\nmean(x) # the average of the numbers\n\n[1] 0.004908104\n\nvar(x) # the variance\n\n[1] 0.9799034\n\nsd(x) # the standard deviation\n\n[1] 0.9899007\n\nmedian(x) # the median\n\n[1] -0.02486056\n\nmax(x) # the biggest number\n\n[1] 2.208443\n\nmin(x) # the smallest number\n\n[1] -2.889718",
    "crumbs": [
      "`R` Helpers",
      "Summary statistics"
    ]
  },
  {
    "objectID": "explainers/vectorization.html",
    "href": "explainers/vectorization.html",
    "title": "Vectorization",
    "section": "",
    "text": "Functions in R are often vectorized. For example:\n\nexp(2)\n\n[1] 7.389056\n\n\nI give it one number, and it returns one number. If instead, I supply a vector, I get this:\n\nexp(c(-3, 0.001, 2, 5, 600, 1, 0))\n\n[1]  4.978707e-02  1.001001e+00  7.389056e+00  1.484132e+02 3.773020e+260\n[6]  2.718282e+00  1.000000e+00\n\n\nSo when I supply a regular ol’ number as an argument, the exp function returns a number. When I supply a vector, it returns a vector; the exp function knows to act entry-wise and give me back a vector with the output of each entry-wise computation. In other words, I didn’t have to write a loop to manually do this. The function is vectorized, so it effectively runs the loop for me.\nSo consider for instance the set of numbers \\(A = \\{-100, -6, 0.001, \\pi, 30\\}\\), and this stupid function:\n\\[\nf(x) = \\ln^2\\left|\\frac{2x}{1-\\sin(x)}\\right|.\n\\]\nIf I wanted to compute the value of \\(f\\) for each of the arguments in \\(A\\), I could write a loop that goes entry-by-entry through \\(A\\) and applies the function \\(f\\) to it. But in R, because arithmetic operations are vectorized, I can do this:\n\nA = c(-100, -6, 0.001, pi, 30)\n\nlog( abs( (2 * A) / ( 1 - sin(A) ) ) ) ^ 2 \n\n[1] 36.051349  7.910714 38.608919  3.377792 11.609009\n\n\nDumb!\nAll of the basic arithmetic operations are vectorized like this:\n\nc(1, 2, 3) + 2\n\n[1] 3 4 5\n\n\n\nc(1, 2, 3) + c(1, 2, 3)\n\n[1] 2 4 6\n\n\n\nfloor(c(4, 5.1, 6.9))\n\n[1] 4 5 6",
    "crumbs": [
      "`R` Helpers",
      "Vectorization"
    ]
  },
  {
    "objectID": "distributions/continuous-normal.html",
    "href": "distributions/continuous-normal.html",
    "title": "Normal distribution",
    "section": "",
    "text": "I usually have a little blurb up here describing how the distribution arises or when you might use it, but the normal distribution is so central to probability and statistics that words fail. It shows up everywhere.",
    "crumbs": [
      "Distribution Families",
      "Normal"
    ]
  },
  {
    "objectID": "distributions/continuous-normal.html#basic-properties",
    "href": "distributions/continuous-normal.html#basic-properties",
    "title": "Normal distribution",
    "section": "Basic properties",
    "text": "Basic properties\n\n\n\n\n\n\n\nNotation\n\\(X\\sim\\text{N}(\\mu,\\,\\sigma^2)\\)\n\n\nRange\n\\(\\mathbb{R}=(-\\infty,\\,\\infty)\\)\n\n\nParameter space\n\\(\\begin{matrix}-\\infty&lt;\\mu&lt;\\infty\\\\\\sigma^2&gt;0\\end{matrix}\\)\n\n\nPDF\n\\(f(x)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{1}{2}\\frac{(x-\\mu)^2}{\\sigma^2}\\right)\\)\n\n\nCDF\n\\(F(x)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\int_{-\\infty}^x\\exp\\left(-\\frac{1}{2}\\frac{(t-\\mu)^2}{\\sigma^2}\\right)\\,\\text{d}t\\)\n\n\nMGF\n\n\\(M(t) = \\exp\\left(\\mu t+\\frac{\\sigma^2}{2}t^2\\right)\\), \\(t\\in\\mathbb{R}\\)\n\n\n\nExpectation\n\\(\\mu\\)\n\n\nVariance\n\\(\\sigma^2\\)\n\n\n\nFun facts:\n\nThis is also known as the Gaussian distribution;\nN(0, 1) is called the standard normal;\nThe density of the normal is the familiar bell curve;\nWe do not have a closed-form for the CDF. We can approximate it arbitrarily well in a computer, but we can’t actually simplify the integral and get a neat formula. Bummer!",
    "crumbs": [
      "Distribution Families",
      "Normal"
    ]
  },
  {
    "objectID": "distributions/continuous-normal.html#r-commands",
    "href": "distributions/continuous-normal.html#r-commands",
    "title": "Normal distribution",
    "section": "\nR commands",
    "text": "R commands\nHere is the documentation for the suite of commands that let you work with the normal distribution in R:\n\ndnorm(x, mean = 0, sd = 1) # PDF\npnorm(q, mean = 0, sd = 1) # CDF: F(q) = P(X &lt;= q)\nqnorm(p, mean = 0, sd = 1) # quantile function (inverse CDF)\nrnorm(n, mean = 0, sd = 1) # random numbers\n\n\n\n\n\n\n\nThe commands take the standard deviation, not the variance!\n\n\n\nYou will get burned by this at least once. I guarantee it. But please take note. If you want to compute \\(P(X\\leq -2.4)\\) for \\(X\\sim\\text{N}(4, 9)\\), you call pnorm(-2.4, 4, 3), because \\(\\text{sd}(X)=\\sqrt{\\text{var}(X)}=3\\).",
    "crumbs": [
      "Distribution Families",
      "Normal"
    ]
  },
  {
    "objectID": "distributions/continuous-normal.html#play-around",
    "href": "distributions/continuous-normal.html#play-around",
    "title": "Normal distribution",
    "section": "Play around!",
    "text": "Play around!\nAs we have seen, the parameters \\(\\mu\\) and \\(\\sigma\\) have multiple interpretations. From a probabilistic point-of-view, they are the mean \\(\\mu=E(X)\\) and the standard deviation \\(\\sigma=\\sqrt{\\text{var}(X)}\\). From a purely geometric point-of-view, they control the shape of the density curve. \\(\\mu\\) is the location of the peak, and \\(\\sigma\\) controls how far the inflection points are from \\(\\mu\\). You worked that out on Problem Set 0.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  titlePanel(\"Normal distribution CDF and PDF\"),\n  \n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"mu\", \"Mean (μ):\", \n                  min = -5, max = 5, value = 0, step = 0.1),\n      sliderInput(\"sigma\", \"Standard Deviation (σ):\", \n                  min = 0.5, max = 3, value = 1, step = 0.1)\n    ),\n    \n    mainPanel(\n      plotOutput(\"distPlot\", height = \"600px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  output$distPlot &lt;- renderPlot({\n    mu &lt;- input$mu\n    sigma &lt;- input$sigma\n    \n    # Fixed x range\n    x &lt;- seq(-10, 10, length.out = 1000)\n    \n    # Compute values\n    pdf_vals &lt;- dnorm(x, mean = mu, sd = sigma)\n    cdf_vals &lt;- pnorm(x, mean = mu, sd = sigma)\n    \n    # Inflection points at mu ± sigma\n    inflect_left &lt;- mu - sigma\n    inflect_right &lt;- mu + sigma\n    \n    # Fixed y limits\n    pdf_ylim &lt;- c(0, 0.8)\n    \n    par(mfrow = c(2, 1), mar = c(4, 4, 2, 1))\n    \n    # --- CDF Plot ---\n    plot(x, cdf_vals, type = \"l\", lwd = 2, col = \"blue\",\n         xlim = c(-10, 10), ylim = c(0, 1),\n         main = \"Cumulative Distribution Function (CDF)\",\n         xlab = \"\", ylab = \"F(x)\")\n    abline(h = c(0, 1), col = \"gray80\", lty = 2)\n    abline(v = mu, col = \"gray60\", lty = 3)\n    \n    # --- PDF Plot ---\n    plot(x, pdf_vals, type = \"l\", lwd = 2, col = \"darkred\",\n         xlim = c(-10, 10), ylim = pdf_ylim,\n         main = \"Probability Density Function (PDF)\",\n         xlab = \"x\", ylab = \"f(x)\")\n    \n    # Vertical lines at mean and inflection points\n    abline(v = mu, col = \"gray60\", lty = 3)\n    abline(v = c(inflect_left, inflect_right), col = \"gray70\", lty = 2)\n    \n    # Arrows showing sigma distance\n    y_arrow &lt;- 0.05\n    arrows(mu, y_arrow, inflect_right, y_arrow, code = 3, angle = 10, length = 0.1)\n    arrows(mu, y_arrow, inflect_left, y_arrow, code = 3, angle = 10, length = 0.1)\n    \n    # Label σ between mean and inflection points\n    text(mu + sigma / 2, y_arrow + 0.03, expression(sigma), cex = 1.1)\n    text(mu - sigma / 2, y_arrow + 0.03, expression(sigma), cex = 1.1)\n    \n    # Label μ in the margin below the x-axis\n    mtext(expression(mu), side = 1, line = 2.2, at = mu, cex = 1.2)\n  })\n}\n\nshinyApp(ui = ui, server = server)",
    "crumbs": [
      "Distribution Families",
      "Normal"
    ]
  },
  {
    "objectID": "distributions/continuous-normal.html#derivations",
    "href": "distributions/continuous-normal.html#derivations",
    "title": "Normal distribution",
    "section": "Derivations",
    "text": "Derivations\n\n\n\n\n\n\nMoment-generating function of N(0, 1)\n\n\n\n\n\n\\[\n\\begin{aligned}\nM(t)&=E[e^{tX}] && \\text{definition}\\\\\n&=\\int_{-\\infty}^\\infty e^{tx}\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}x^2}\\,\\textrm{d}x && \\text{LOTUS}\\\\\n&=\\int_{-\\infty}^\\infty\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}x^2+tx}\\,\\textrm{d}x && \\text{combine base-$e$ terms}\\\\\n&=\\int_{-\\infty}^\\infty\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}(x^2-2tx)}\\,\\textrm{d}x && \\text{factor out -1/2}\\\\\n&=\\int_{-\\infty}^\\infty\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}(x^2-2tx+t^2-t^2)}\\,\\textrm{d}x && \\text{add/subtract $t^2$}\\\\\n&=\\int_{-\\infty}^\\infty\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}[(x-t)^2-t^2]}\\,\\textrm{d}x && \\text{factor first three terms}\\\\\n&=\\int_{-\\infty}^\\infty\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}(x-t)^2+\\frac{1}{2}t^2}\\,\\textrm{d}x && \\text{distribute -1/2}\\\\\n&=\\int_{-\\infty}^\\infty\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}(x-t)^2}e^{\\frac{1}{2}t^2}\\,\\textrm{d}x\\\\\n&=e^{\\frac{1}{2}t^2}\\int_{-\\infty}^\\infty\\underbrace{\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}(x-t)^2}}_{\\textrm{N}(t,\\,1)\\text{ PDF}}\\,\\textrm{d}x&&\\text{pull constant out of integral}\\\\\n&=e^{\\frac{1}{2}t^2}\\cdot 1&&\\text{PDFs integrate to 1}\\\\\n&=e^{\\frac{1}{2}t^2}.\n\\end{aligned}\n\\]\nThis works for any \\(t\\in\\mathbb{R}\\).",
    "crumbs": [
      "Distribution Families",
      "Normal"
    ]
  },
  {
    "objectID": "distributions/continuous-uniform.html",
    "href": "distributions/continuous-uniform.html",
    "title": "Continuous uniform distribution",
    "section": "",
    "text": "If \\(X\\) is a continuous random quantity that is “equally likely” to take on any value in a bounded interval \\([a,\\, b]\\), then we can model it with the continuous uniform distribution.",
    "crumbs": [
      "Distribution Families",
      "Continuous uniform"
    ]
  },
  {
    "objectID": "distributions/continuous-uniform.html#basic-properties",
    "href": "distributions/continuous-uniform.html#basic-properties",
    "title": "Continuous uniform distribution",
    "section": "Basic properties",
    "text": "Basic properties\n\n\n\n\n\n\n\nNotation\n\\(X\\sim\\text{Unif}(a,\\,b)\\)\n\n\nRange\n\\([a,\\,b]\\)\n\n\nParameter space\n\\(-\\infty &lt; a &lt; b &lt; \\infty\\)\n\n\nPDF\n\\(f(x)=\\begin{cases}\\frac{1}{b-a} & a\\leq x\\leq b\\\\0&\\text{else}\\end{cases}\\)\n\n\nCDF\n\\(F(x)=\\begin{cases}0 & x &lt; a\\\\\\frac{x-a}{b-a} & a\\leq x\\leq b\\\\1&b&lt;x\\end{cases}\\)\n\n\nExpectation\n\\(\\frac{a+b}{2}\\)\n\n\nVariance\n\\((b-a)^2/12\\)\n\n\n\nFun facts:\n\nUnif(0, 1) is called the standard uniform distribution.",
    "crumbs": [
      "Distribution Families",
      "Continuous uniform"
    ]
  },
  {
    "objectID": "distributions/continuous-uniform.html#r-commands",
    "href": "distributions/continuous-uniform.html#r-commands",
    "title": "Continuous uniform distribution",
    "section": "\nR commands",
    "text": "R commands\nHere is the documentation for the suite of commands that let you work with the normal distribution in R:\n\ndunif(x, min, max) # PDF\npunif(q, min, max) # CDF: P(X &lt;= q)\nqunif(p, min, max) # quantile function\nrunif(n, min, max) # random numbers",
    "crumbs": [
      "Distribution Families",
      "Continuous uniform"
    ]
  },
  {
    "objectID": "distributions/continuous-uniform.html#play-around",
    "href": "distributions/continuous-uniform.html#play-around",
    "title": "Continuous uniform distribution",
    "section": "Play around!",
    "text": "Play around!\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 600\n#| fig-asp: 1\n\nlibrary(shiny)\n\n# Define UI for application that draws a histogram\nui &lt;- fluidPage(\n  \n  # Application title\n  titlePanel(\"Uniform distribution CDF and PDF\"),\n  \n  # Sidebar with a slider input for number of bins \n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"a\",\n                  \"Lower bound (a):\",\n                  min = 0,\n                  max = 2,\n                  value = 1,\n                  step = 0.1),\n      sliderInput(\"b\",\n                  \"Upper bound (b):\",\n                  min = 3,\n                  max = 5,\n                  value = 4,\n                  step = 0.1)\n    ),\n    \n    # Show a plot of the generated distribution\n    mainPanel(\n      plotOutput(\"distPlot\")\n    )\n  )\n)\n\n# Define server logic required to draw a histogram\nserver &lt;- function(input, output) {\n  \n  output$distPlot &lt;- renderPlot({\n    \n    a &lt;- input$a\n    b &lt;- input$b\n\n    par(mfrow = c(2, 1), mar = c(4, 4, 2, 2))\n    \n    curve(punif(x, min = a, max = b), from = -1, to = 6, n = 1000,\n          ylab = \"\", col = \"red\", xlim = c(0, 5))\n    curve(0*x, from = -1, to = a, n = 1000, \n          ylim = c(0, 1), ylab = \"\", col = \"red\", xlim = c(0, 5))\n    curve(dunif(x, min = a, max = b), from = a, to = b, n = 1000, \n          ylim = c(0, 1), ylab = \"\", col = \"red\", add = TRUE)\n    curve(0*x, from = b, to = 6, n = 1000, \n          ylim = c(0, 1), ylab = \"\", col = \"red\", add = TRUE)\n  })\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)",
    "crumbs": [
      "Distribution Families",
      "Continuous uniform"
    ]
  },
  {
    "objectID": "distributions/continuous-uniform.html#derivations",
    "href": "distributions/continuous-uniform.html#derivations",
    "title": "Continuous uniform distribution",
    "section": "Derivations",
    "text": "Derivations\n\n\n\n\n\n\nThe pdf integrates to 1\n\n\n\n\n\n\\[\n\\begin{aligned}\n    \\int_{-\\infty}^\\infty f_X(x)\\,\\text{d} x\n    &=\n    \\int_{-\\infty}^a f_X(x)\\,\\text{d} x\n    +\n    \\int_{a}^b f_X(x)\\,\\text{d} x\n    +\n    \\int_{b}^\\infty f_X(x)\\,\\text{d} x\n    \\\\\n    &=\n    {\\int_{-\\infty}^a 0\\,\\text{d} x}\n    +\n    \\int_{a}^b \\frac{1}{b-a}\\,\\text{d} x\n    +\n    {\\int_{b}^\\infty 0\\,\\text{d} x}\n    \\\\\n    &=\n    \\frac{1}{b-a}\\int_a^b1\\,\\text{d} x\n    \\\\\n    &=\n    \\frac{1}{b-a}\n    [x]_a^b\n    \\\\\n    &=\n    \\frac{1}{b-a}(b-a)\n    \\\\\n    &=\n    1\n    .\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\nThe CDF\n\n\n\n\n\nThe cdf is \\(F_X(x)=P(X\\leq x)=\\int_{-\\infty}^x f_X(t)\\,\\text{d} t\\). If \\(x&lt; a\\), then\n\\[\nF_X(x)=\\int_{-\\infty}^xf_X(t)\\,\\text{d} t=\\int_{-\\infty}^x0\\,\\text{d} t=0.\n\\]\nIf \\(a\\leq x\\leq b\\), then\n\\[\n\\begin{aligned}\n    F_X(x)\n    &=\n    \\int_{-\\infty}^xf_X(t)\\,\\text{d} t\n    \\\\\n    &=\n    \\int_{-\\infty}^af_X(t)\\,\\text{d} t\n    +\n    \\int_{a}^xf_X(t)\\,\\text{d} t\n    \\\\\n    &=\n    {\\int_{-\\infty}^a0\\,\\text{d} t}\n    +\n    \\int_{a}^x\\frac{1}{b-a}\\,\\text{d} t\n    \\\\\n    &=\n    \\frac{1}{b-a}[t]_a^x\n    \\\\\n    &=\n    \\frac{x-a}{b-a}\n    .\n\\end{aligned}\n\\]\nIf \\(b &lt; x\\), then\n\\[\n\\begin{aligned}\n    F_X(x)\n    &=\n    \\int_{-\\infty}^xf_X(t)\\,\\text{d} t\n    \\\\\n    &=\n    \\int_{-\\infty}^af_X(t)\\,\\text{d} t\n    +\n    \\int_{a}^bf_X(t)\\,\\text{d} t\n    +\n    \\int_{b}^xf_X(t)\\,\\text{d} t\n    \\\\\n        &=\n    {\\int_{-\\infty}^a0\\,\\text{d} t}\n    +\n    \\int_{a}^b\\frac{1}{b-a}\\,\\text{d} t\n    +\n    {\\int_{b}^x0\\,\\text{d} t}\n    \\\\\n    &=1.\n\\end{aligned}\n\\]\nSo the cdf of \\(X\\sim\\text{Unif}(a,\\, b)\\) is\n\\[\n    F_X(x)\n    =\n    \\begin{cases}\n        0 & x&lt;a \\\\\n        \\frac{x-a}{b-a} & a\\leq x\\leq b \\\\\n        1 & b&lt;x.\n    \\end{cases}\n\\]\n\n\n\n\n\n\n\n\n\nThe mean\n\n\n\n\n\nThe expected value is the midpoint of the interval:\n\\[\n\\begin{align*}\n    E(X)\n    &=\n    \\int_{-\\infty}^\\infty\n    xf_X(x)\\,\\text{d} x\n    \\\\\n    &=\n    \\int_{-\\infty}^a\n    xf_X(x)\\,\\text{d} x\n    +\n    \\int_{a}^b\n    xf_X(x)\\,\\text{d} x\n    +\n    \\int_{b}^\\infty\n    xf_X(x)\\,\\text{d} x\n    \\\\\n        &=\n    {\\int_{-\\infty}^a\n    x\\cdot0\\,\\text{d} x}\n    +\n    \\int_{a}^b\n    x\\frac{1}{b-a}\\,\\text{d} x\n    +\n    {\\int_{b}^\\infty\n    x\\cdot0\\,\\text{d} x}\n    \\\\\n    &=\n    0+\\frac{1}{b-a}\\int_a^b x\\,\\text{d} x+0\n    \\\\\n    &=\n    \\frac{1}{b-a}\\left[\\frac{1}{2}x^2\\right]_a^b\n    \\\\\n    &=\n    \\frac{1}{b-a}\\left[\\frac{1}{2}b^2-\\frac{1}{2}a^2\\right]\n    \\\\\n        &=\n    \\frac{1}{2(b-a)}\\left[b^2-a^2\\right]\n    \\\\\n        &=\n    \\frac{1}{2{(b-a)}}{(b-a)}(b+a)\n    \\\\\n    &=\\frac{b+a}{2}.\n\\end{align*}\n\\]\nThat’s the proper derivation, but without the math, we could have reasoned our way to this answer by noting that the range is bounded, so the mean is guaranteed to exist, and the distribution is symmetric, so the mean matches the point of symmetry.",
    "crumbs": [
      "Distribution Families",
      "Continuous uniform"
    ]
  },
  {
    "objectID": "distributions/continuous-beta.html",
    "href": "distributions/continuous-beta.html",
    "title": "Beta distribution",
    "section": "",
    "text": "The beta distribution plays many roles in probability and statistics. One of them is as the conjugate prior for the Bernoulli and geometric distributions. These distributions have one parameter \\(p\\), the probability of success for some binary trial. You can pick a particular beta distribution to represent your beliefs about where the unknown \\(p\\) is likely to lie.",
    "crumbs": [
      "Distribution Families",
      "Beta"
    ]
  },
  {
    "objectID": "distributions/continuous-beta.html#basic-properties",
    "href": "distributions/continuous-beta.html#basic-properties",
    "title": "Beta distribution",
    "section": "Basic properties",
    "text": "Basic properties\nThe beta has bounded range, so all moments are finite. Sadly, we do not have closed-form expressions for the cdf or the mgf.\n\n\n\n\n\n\n\nNotation\n\\(X\\sim\\text{Beta}(\\alpha,\\,\\beta)\\)\n\n\nRange\n\\((0,\\,1)\\)\n\n\nParameter space\n\\(\\alpha,\\,\\beta&gt;0\\)\n\n\nPDF\n\\(f(x)=\\begin{cases}\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1} &0&lt;x&lt;1\\\\0 & \\text{else}.\\end{cases}\\)\n\n\nExpectation\n\\(\\frac{\\alpha}{\\alpha+\\beta}\\)\n\n\nVariance\n\\(\\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}\\)\n\n\n\nEvery probability density is an integral identity in disguise, so we have that:\n\\[\n\\int_0^1\nx^{\\alpha-1}(1-x)^{\\beta-1}\n\\,\\text{d}x\n=\n\\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}\n.\n\\]",
    "crumbs": [
      "Distribution Families",
      "Beta"
    ]
  },
  {
    "objectID": "distributions/continuous-beta.html#r-commands",
    "href": "distributions/continuous-beta.html#r-commands",
    "title": "Beta distribution",
    "section": "\nR commands",
    "text": "R commands\nHere is the documentation for the suite of commands that let you work with the beta distribution in R:\n\ndbeta(x, shape1, shape2) # PDF\npbeta(q, shape1, shape2) # CDF: P(X &lt;= q)\nqbeta(p, shape1, shape2) # quantile function (inverse CDF)\nrbeta(n, shape1, shape2) # random numbers",
    "crumbs": [
      "Distribution Families",
      "Beta"
    ]
  },
  {
    "objectID": "distributions/continuous-beta.html#play-around",
    "href": "distributions/continuous-beta.html#play-around",
    "title": "Beta distribution",
    "section": "Play around!",
    "text": "Play around!\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  titlePanel(\"Beta Distribution Viewer\"),\n  \n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"alpha\", \"Alpha (shape1):\",\n                  min = 0.5, max = 10, value = 2, step = 0.1),\n      sliderInput(\"beta\", \"Beta (shape2):\",\n                  min = 0.5, max = 10, value = 2, step = 0.1)\n    ),\n    \n    mainPanel(\n      plotOutput(\"betaPlot\", height = \"600px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n  \n  output$betaPlot &lt;- renderPlot({\n    a &lt;- input$alpha\n    b &lt;- input$beta\n    x &lt;- seq(0, 1, length.out = 500)\n    \n    par(mfrow = c(2,1), mar = c(4,4,2,1))\n    \n    ## --- CDF ---\n    plot(x, pbeta(x, a, b), type = \"l\", lwd = 2,\n         xlab = \"\", ylab = \"CDF\", main = \"Beta CDF\",\n         ylim = c(0,1))\n    \n    ## --- PDF ---\n    plot(x, dbeta(x, a, b), type = \"l\", lwd = 2,\n         xlab = \"x\", ylab = \"PDF\", main = \"Beta PDF\",\n         ylim = c(0,7))   # fixed y-axis range\n  })\n}\n\nshinyApp(ui, server)",
    "crumbs": [
      "Distribution Families",
      "Beta"
    ]
  },
  {
    "objectID": "distributions/continuous-beta.html#derivations",
    "href": "distributions/continuous-beta.html#derivations",
    "title": "Beta distribution",
    "section": "Derivations",
    "text": "Derivations\n\n\n\n\n\n\nMean\n\n\n\n\n\n\\[\n\\begin{aligned}\nE(X)\n&=\n\\int_0^1\nx\nf(x)\n\\,\\text{d}x\n\\\\\n&=\n\\int_0^1\nx\n\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}\n\\,\\text{d}x\n\\\\\n&=\n\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\n\\int_0^1\n\\underbrace{x^{\\alpha+1-1}(1-x)^{\\beta-1}}_{\\text{kernel of Beta}(\\alpha+1,\\,\\beta)}\n\\,\\text{d}x\n\\\\\n&=\n\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\frac{\\Gamma(\\alpha+1)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta+1)}\n\\\\\n&=\n\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\frac{\\alpha\\Gamma(\\alpha)\\Gamma(\\beta)}{(\\alpha+\\beta)\\Gamma(\\alpha+\\beta)}\n\\\\\n&=\n\\frac{\\alpha}{\\alpha+\\beta}\n.\n\\end{aligned}\n\\]\nThis is classic “massage and squint.” We do a kernel-trick with the beta density, and we recall the properties of the gamma function from Problem Set 0.",
    "crumbs": [
      "Distribution Families",
      "Beta"
    ]
  },
  {
    "objectID": "distributions/discrete-hypergeometric.html",
    "href": "distributions/discrete-hypergeometric.html",
    "title": "Hypergeometric distribution",
    "section": "",
    "text": "Imagine we have a finite population containing \\(m\\) “success” cases and \\(n\\) “failure” cases, for a total population size of \\(m+n\\). If we sample \\(k\\) cases from the population without replacement, then a hypergeometric random variable counts the number of sampled cases that are a success.\nYou first met this distribution on Problem Set 4 when studying contested elections.",
    "crumbs": [
      "Distribution Families",
      "Hypergeometric"
    ]
  },
  {
    "objectID": "distributions/discrete-hypergeometric.html#basic-properties",
    "href": "distributions/discrete-hypergeometric.html#basic-properties",
    "title": "Hypergeometric distribution",
    "section": "Basic properties",
    "text": "Basic properties\n\n\nNotation\n\\(X\\sim\\text{HG}(m, n, k)\\)\n\n\nRange\n\\(\\{0,\\,1,\\,2,\\,3,\\,...,\\,k-1,\\,k\\}\\)\n\n\nPMF\n\\(P(X = x) = \\binom{m}{x} \\binom{n}{k-x}/\\binom{m+n}{k}\\)\n\n\nExpectation\n\\(km/(m+n)\\)",
    "crumbs": [
      "Distribution Families",
      "Hypergeometric"
    ]
  },
  {
    "objectID": "distributions/discrete-hypergeometric.html#r-commands",
    "href": "distributions/discrete-hypergeometric.html#r-commands",
    "title": "Hypergeometric distribution",
    "section": "\nR commands",
    "text": "R commands\nHere is the documentation for the suite of commands that let you work with the hypergeometric distribution in R:\n\ndhyper(x, m, n, k)     # PMF: P(X = x)\nphyper(q, m, n, k)     # CDF: P(X &lt;= q)\nqhyper(p, m, n, k)     # quantile function (inverse CDF)\nrhyper(ndraw, m, n, k) # random numbers",
    "crumbs": [
      "Distribution Families",
      "Hypergeometric"
    ]
  },
  {
    "objectID": "distributions/continuous-binormal.html",
    "href": "distributions/continuous-binormal.html",
    "title": "Bivariate normal distribution",
    "section": "",
    "text": "\\[\n    \\begin{aligned}\n        f_{XY}(x, y)\n        =\n        &\\frac{1}{2\\pi\\sigma_x\\sigma_y\\sqrt{1-\\rho^2}}\n        \\\\\n        &\n        \\times\n        \\exp\\left(-\\frac{1}{2(1-\\rho^2)}\\left[\n        \\left(\\frac{x-\\mu_x}{\\sigma_x}\\right)^2\n        -\n        2\\rho\n        \\left(\\frac{x-\\mu_x}{\\sigma_x}\\right)\n        \\left(\\frac{y-\\mu_y}{\\sigma_y}\\right)\n        +\n        \\left(\\frac{y-\\mu_y}{\\sigma_y}\\right)^2\n        \\right]\\right),\n        \\\\\n        &\n        \\\\\n        &\n        (x, y)\\in\\mathbb{R}^2,\n    \\end{aligned}\n\\]\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\n\nfxy = function(x, y, mu, Sig, sd1, sd2, rho) {\n  \n  if(missing(mu)) mu=c(0,0)\n  \n  if(!missing(Sig)) {\n    sd1 = sqrt(Sig[1,1])\n    sd2 = sqrt(Sig[2,2])\n    if(Sig[1,2] != Sig[2,1]) {\n      print(\"Covariance matrix is not symmetric... Returning .\")\n      return(NULL)\n    }\n    rho = Sig[1,2]/(sd1*sd2)\n  }\n  else if(missing(rho) || missing(sd1) || missing(sd2)) {\n    sd1 = sd2 = 1\n    rho = 0\n  }\n  \n  Q = (x-mu[1])^2/sd1^2 + (y-mu[2])^2/sd2^2 -\n    2*rho*(x-mu[1])*(y-mu[2])/(sd1*sd2)\n  \n  1/(2*pi*sd1*sd2*sqrt(1-rho^2))*exp(-Q/(2*(1-rho^2)))\n}\n\n\n## Calls persp() with preferred arguments\npersp.plot = function(x, y, z, main=\"Bivariate Normal Density\",\n                      theta=30, phi=25, r=50, d=.1, expand=0.5, ltheta=90, lphi=180,\n                      shade=0.5, ticktype=\"simple\", nticks=5, col=\"lightgreen\", zlab=\"\", ...) {\n  \n  persp(x, y, z, main=main,\n        theta=theta, phi=phi, r=r, d=d, expand=expand, ltheta=ltheta,\n        lphi=lphi, shade=shade, ticktype=ticktype, nticks=nticks,\n        col=col, zlab=zlab, ...)\n}\n\n\n## Creates covariance matrix from sd.x, sd.y, and rho\ncalc.Sig = function(sd.x, sd.y, rho) {\n  \n  sig.xy = rho*sd.x*sd.y\n  matrix(c(sd.x^2, sig.xy, sig.xy, sd.y^2), nrow=2)\n}\n\n\n## Returns bivariate normal density for specified x-y grid\ndmvnorm = function(x, y, mu, Sig) {\n  \n  if(missing(mu)) mu = c(0,0)\n  if(missing(Sig)) Sig = diag(2)\n  \n  outer(x, y, fxy, mu, Sig)\n}\n\n\n## This is only the kernel of the bivariate Normal density\n## x is a 2x1 vector\nf = function(x, y, mu=c(0,0), sd.x=1, sd.y=1, rho=0) {\n  \n  #t(X-mu)%*%solve(Sig)%*%(X-mu)\n  mu.x = mu[1]\n  mu.y = mu[2]\n  A = (x-mu.x)^2/sd.x^2 + (y-mu.y)^2/sd.y^2\n  B = 2*rho/(sd.x*sd.y)*(x-mu.x)*(y-mu.y)\n  return((A-B)/(1-rho^2))\n}\n\n\n### End: Function definitions ###\n\n\n\n\nlibrary(shiny)\n\n# Define UI for application that draws a histogram\nui &lt;- fluidPage(\n  \n  # Application title\n  titlePanel(\"The bivariate normal density\"),\n  \n  # Sidebar with a slider input for number of bins \n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"sd.x\",\n                  \"X standard deviation:\",\n                  min = 0,\n                  max = 1,\n                  value = 0.5,\n                  step = 0.01),\n      sliderInput(\"sd.y\",\n                  \"Y standard deviation:\",\n                  min = 0,\n                  max = 1,\n                  value = 0.5,\n                  step = 0.01),\n      sliderInput(\"rho\",\n                  \"Correlation:\",\n                  min = -1,\n                  max = 1,\n                  value = 0,\n                  step = 0.01)\n    ),\n    \n    # Show a plot of the generated distribution\n    mainPanel(\n      plotOutput(\"distPlot\")\n    )\n  )\n)\n\n# Define server logic required to draw a histogram\nserver &lt;- function(input, output) {\n  \n  output$distPlot &lt;- renderPlot({\n    \n    ## Define sequence of parameter\n    \n    p.seq = c(.8, .9, .95, .99)\n    cont.lev = qchisq(p.seq, 2)\n    cont.lab = c(\"80%\", \"90%\", \"95%\", \"99%\")\n    \n    \n    ## Plotting starts here\n    \n    par(cex.lab=2)\n    par(cex.axis=1.75)\n    par(las=1)\n    par(mfrow=c(1,2))  # 1.5:1 aspect ratio\n    \n    sd.x = input$sd.x\n    sd.y = input$sd.y\n    rho = input$rho\n    N = 100\n    x = y = seq(-3.2,3.2,le=N)  # create x-y grid of size NxN\n    mu = c(0,0)\n    \n    \n    z = dmvnorm(x, y, mu, calc.Sig(sd.x, sd.y, rho))\n    persp.plot(x, y, z, main=\"\", col=\"lightblue\", border=NA, cex.lab=1.5,\n               axes=F)\n    \n    ## Contour Plot\n    z = outer(x, y, f, mu, sd.x, sd.y, rho)\n    contour(x, y, z, levels=cont.lev, cex.axis=1.4, labels=cont.lab,\n            xlab=\"x\", ylab=\"y\", cex.lab=1.5)\n    abline(v=0, h=0, lty=3, col=\"darkgrey\")\n    \n  })\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)"
  }
]